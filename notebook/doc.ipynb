{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df85acf",
   "metadata": {},
   "source": [
    "### Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e00f2c76",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc9baa",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dc2aa21",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "dir_loader=DirectoryLoader(\n",
    "    \"../docs/pdfs\",\n",
    "    loader_cls=PyMuPDFLoader\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d390b14",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 0}, page_content='International Journal of Digital Earth\\nISSN: 1753-8947 (Print) 1753-8955 (Online) Journal homepage: www.tandfonline.com/journals/tjde20\\nA systematic review and comparative analysis of\\ndeep learning models for Twitter/X-based traﬃc\\nevent detection\\nDanya Qutaishat & Songnian Li\\nTo cite this article: Danya Qutaishat & Songnian Li (2026) A systematic review and comparative\\nanalysis of deep learning models for Twitter/X-based traﬃc event detection, International\\nJournal of Digital Earth, 19:1, 2604977, DOI: 10.1080/17538947.2025.2604977\\nTo link to this article:  https://doi.org/10.1080/17538947.2025.2604977\\n© 2025 The Author(s). Published by Informa\\nUK Limited, trading as Taylor & Francis\\nGroup.\\nView supplementary material \\nPublished online: 29 Dec 2025.\\nSubmit your article to this journal \\nArticle views: 145\\nView related articles \\nView Crossmark data\\nFull Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=tjde20'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 1}, page_content=\"INTERNATIONAL JOURNAL OF DIGITAL EARTH \\n2026, VOL. 19, NO. 1, 2604977 \\nhttps://doi.org/10.1080/17538947.2025.2604977\\nREVIEW ARTICLE                                                                          \\nA systematic review and comparative analysis of deep learning models for \\nTwitter/X-based traffic event detection\\nDanya Qutaishat and Songnian Li\\nDepartment of Civil Engineering, Toronto Metropolitan University, Toronto, Canada\\nABSTRACT\\nTraffic anomalies caused by accidents, sports events, and lane closures are spatiotemporal \\nevents that reduce free-flow speed, increase vehicular queues, and impair human mobility. \\nEarly detection may provide better route planning before traffic gets worse. Recent and \\nongoing research, as well as a review of transportation literature, have revealed three \\nessential topics: big data, data mining and representation, and Deep Learning (DL). \\nFurthermore, traffic studies have adopted DL to extract hidden features that efficiently \\ninfer human activities and interactions and detect the underlying relationships to generate \\nuseful fine-grained information. This paper reviews current research that adopts state-of-the- \\nart DL in detecting traffic events from big data, specifically Twitter/X data. In addition, it \\ninvestigates the detailed pipeline for developing a DL-based model using data from Twitter/X \\nfor traffic event detection (TED). The review is a timely addition that clarifies the roadmap of \\ndetecting traffic events from big social media data, which benefits transportation and DL \\ncommunity researchers.\\nARTICLE HISTORY \\nReceived 24 February 2025 \\nAccepted 12 December 2025 \\nKEYWORDS\\nDeep learning; feature \\nlearning; word embedding; \\nmodel selection; Twitter/X\\n1 Introduction\\nTraffic Event Detection (TED) holds significant promise in assisting road users. It helps choose optimal paths, \\nreduce travel time, mitigate traffic congestion, minimise fuel consumption, and reduce environmental pollution \\n(Nejjari, Benhlima, and Bah 2016; Kim et al. 2023; Lee et al. 2023; Gannina Kumar et al. 2024; Qutaishat and Li \\n2025a). Moreover, it help reduce traffic accidents that lead to injuries, fatalities, and property damage, which \\nresult in substantial social and economic costs (Es Swidi et al. 2023; Gannina Kumar et al. 2024). Additionally, \\ngenerating multiple scenarios equips traffic management departments with a tool to suggest timely and practical \\nplans to improve traffic conditions. This enables prompt responses by emergency services and supports traffic \\nrerouting to enhance traffic management (Nejjari, Benhlima, and Bah 2016).\\nTransportation agencies' most significant challenge is acquiring real-time, large-scale, and up-to-date \\nobservational data (Anda, Erath, and Fourie 2017; Liu et al. 2020; Saeedi et al. 2020). Traditional data sources \\nconsist of structured data collected using physical condition monitoring devices deployed in the field or \\nsensing devices installed in moving vehicles (Hall, Shi, and Atala 1993; Sethi et al. 1995; Samant and Adeli \\n2000). Although incorporating conventional data for detecting traffic events provides accurate information \\nregarding their location and time, several challenges have been identified. First, studies are built on the \\nassumption of data reliability; however, incident detection has proven to be difficult due to detector failure, \\nsensor malfunctions, and communication errors in large-scale areas. Second, external factors impact traffic \\noperations, reducing the effectiveness of traffic metrics in detecting traffic incidents. Third, physical sensors \\nor detectors require regular maintenance and cover small-scale areas such as intersections or short road \\nsegments. This limitation restricts the capture of traffic patterns and non-recurring events across entire urban \\nareas (Münz, Sa, and Georg 2007; Zhang et al. 2018).\\nSupplemental data for this article can be accessed online at https://doi.org/10.1080/17538947.2025.2604977. \\nThis article has been corrected with minor changes. These changes do not impact the academic content of the article.\\n© 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. \\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (http://creativecommons.org/licenses/by-nc/4 \\n.0/), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited. The terms on which \\nthis article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) or with their consent.\\nCONTACT Songnian Li \\nsnli@torontomu.ca\\nDepartment of Civil Engineering, Toronto Metropolitan University, Toronto, Canada\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='Nowadays, social media data makes a significant contribution to traffic studies, including human \\nactivity patterns or travel behaviours (Hasan and Ukkusuri 2014; Hasnat and Hasan 2018), traffic flow \\nforecasting (Lin et al. 2015; Ni, He, and Gao 2016; Cottrill et al. 2017), transportation management and \\nplanning (Cottrill et al. 2017), travel mode extraction (Maghrebi, Abbasi, and Waller 2016), and destina\\xad\\ntion choice modelling (Huang, Gallegos, and Lerman 2017; Molloy and Moeckel 2017; Hasnat and Hasan \\n2018; Hasnat et al. 2019). However, concerns are rising due to the lack of tools and techniques to unlock \\nthe power of data and extract valuable knowledge from this kind of massive, complex, and diverse big data.\\nTraditional machine learning (ML) models have been recognised as cornerstones of Twitter/X-based TED. \\nThey laid the foundation for utilising structured textual data and feature engineering to classify and predict traffic- \\nrelated events. The most commonly investigated models are Support Vector Machine (SVM) (Noori and Mehra \\n2020; Afyouni, Aghbari, and Razack 2022; Dinesh, Kuhaneswaran, and Ravikumar 2023), Random Forest \\n(Alomari, Mehmood, and Katib 2019; Jiang and Deng 2020; ElSahly and Abdelfatah 2023), and Naive Bayes \\n(Alomari, Mehmood, and Katib 2019; Nirbhaya and Suadaa 2023). However, several challenges arise from issues \\nrelated to data quality, model performance, and computational efficiency. Selection bias arises from non- \\nrepresentative Twitter/X data, leading to skewed outcomes and reduced generalisability (Liu et al. 2024). Class \\nimbalance negatively impacts detection accuracy due to the higher number of non-traffic tweets compared to \\nrelevant ones (Liu et al. 2024). Linguistic challenges, including informal language, expressions, and negative \\nphrases, contribute to frequent detection errors (Dhiman and Toshniwal 2020; Liu et al. 2024). Additionally, short \\ntweet lengths limit contextual understanding, reducing model effectiveness (Dhiman and Toshniwal 2020). High \\ncomputational costs are another issue, as processing large-scale Twitter/X data with complex algorithms demands \\nsignificant resources (Dhiman and Toshniwal 2020). Lastly, feature extraction issues arise from automated social \\naccounts mimicking human behaviour, making traffic event detection more difficult (Sethurajan and K 2023).\\nIn the context of TED using Tweets, Deep Learning (DL) models have overcome ML in multiple aspects. The \\nfirst aspect is the ability of DL models to excel in automatically extracting relevant features from unstructured text \\ndata, eliminating the need for manual feature engineering that is labour-intensive and prone to human error \\n(Hussain 2024; Qutaishat and Li 2025a). Second, techniques like transformers enable these models to understand \\nthe context of informal language, expressions, and abbreviations commonly found in tweets (Neruda and \\nWinarko 2021). They also process sequential and multimodal data, such as text, time, and location, using \\narchitectures like Recurrent Neural Networks (RNN) and multi-input neural networks, which provide a more \\ncomprehensive analysis (Alifi and Supangkat 2018; Zhang et al. 2018). Third, DL scales efficiently and can handle \\nlarge, real-time datasets while adapting real-time patterns and language changes on social media. Finally, its ability \\nto detect complex, non-linear patterns leads to significantly higher accuracy in identifying traffic events, making it \\na superior choice for real-time, dynamic environments (Kisters and Bauer 2023 ; Li, Dou, and Zhou 2023; \\nNirbhaya and Suadaa 2023; Qutaishat and Li 2025a, Suat-Rojas, Gutierrez-Osorio, and Pedraza 2022; Yang 2022; \\nQutaishat and Li 2025b).\\nThis paper reviews the literature on DL-based TED using social media data, focusing on Twitter/X. Twitter/X \\nhas emerged as a valuable platform for supporting the detection and modelling of traffic events and deserves \\nclose attention in this context. Several reviews have explored the topic of event detection using social media data, \\nwith some studies focusing on traditional machine learning techniques and statistical methods for analysing \\nTwitter/X data to identify traffic-related events (Garg and Kumar 2016; Xu, Li, and Wen 2018; Liu et al. 2024). \\nThese studies lack emphasis on DL approaches, which have proven to be more effective in handling \\nunstructured text, capturing complex relationships, and integrating multimodal data sources. Other studies \\nhave focused on investigating event detection on Twitter/X, without specifically addressing traffic events or \\ndetailing the techniques used, such as (Saeed et al. 2019; Atefeh and Khreich 2013). There has been a lack of \\nsystematic reviews that provide comprehensive insights into the research framework for traffic event detection \\nusing Twitter/X data, specifically in the context of DL techniques.\\nThe main contributions of this paper are as follows:\\n1. A comprehensive systematic review that consolidates and critically analyses published studies related to \\nTwitter/X-based TED using DL techniques.  \\n2. A detailed description of the general Twitter/X-based TED workflow, including preprocessing steps and \\nmodel development. \\n2\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 3}, page_content='3. An analysis of feature learning and models, evaluating their strengths and weaknesses, and the influence \\nof parameters on DL performance. \\n4. Identification of the challenges and future research directions, offering essential guidance in transpor\\xad\\ntation engineering and related domains.\\n2 Review methodology\\nThis research followed the PRISMA 2020 guidelines for systematic reviews (Page et al. 2021; Page and \\nMcKenzie, 2020 ). Figure 1 presents the PRISMA flow diagram, which shows the number of studies \\nidentified, screened, excluded, and ultimately included in the review.\\n2.1 Eligibility criteria\\nStudies were assessed based on the following inclusion and exclusion criteria:\\n2.1.1 Inclusion criteria\\nStudies were included if they:\\n• Applied DL models such as CNNs, RNNs, or LSTMs for TED.  \\n• Used Twitter/X as a standalone source or integrated it with other data.  \\n• Were published in English, from 2010 onward, reflecting DL advancements from that time.  \\n• Provided detailed methodologies, including DL architecture, preprocessing, and evaluation metrics (e.g. \\naccuracy, F1-score). \\n• Were published in peer-reviewed journals or reputable conferences in relevant fields, such as transpor\\xad\\ntation engineering or computer science.                           \\nFigure 1. The PRISMA flow diagram of the article selection process. Note: Although the main flow shows 30 studies from \\ndatabase searches, additional studies were identified via citation/web methods, bringing the total to 44.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n3'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 4}, page_content='• Cover both recurring (e.g. rush hour) and non-recurring (e.g. accidents) traffic events.\\n2.1.2 Exclusion criteria\\nStudies were excluded if they:\\n• Used observational or non-social media data for TED.  \\n• Depended on traditional ML instead of DL.  \\n• Focused on events unrelated to traffic (e.g. natural disasters, crime, health)  \\n• Lacked sufficient methodological detail or empirical evaluation.  \\n• Were theoretical, non-English, or published before 2010.\\n2.2 Information sources and search strategy\\nFor the scope of this review, the search was narrowed down to journal articles and conference proceedings. \\nFive scholarly databases were searched: Engineering Village, Scopus, Web of Science, IEEE, and Summon \\n2.0. Citation searches help trace prior, derivative, or related works. ResearchRabbit, a citation-based \\nliterature mapping tool, facilitated this process in conjunction with Google Scholar. The timeframe, set \\nfrom 2010 onward, reflects the rise of DL, driven by the revolution in computational power and the \\navailability of large datasets. Earlier studies predominantly used ML or statistical techniques.\\nBoolean Keyword combinations used included:\\n(traffic event detection AND deep learning) AND (social media OR Twitter/X OR crowdsourcing).\\nInitial database search yielded:\\n• Engineering Village: 57  \\n• IEEE Xplore: 19  \\n• Scopus: 16  \\n• Web of Science: 13  \\n• Summon 2.0: 20\\nAn additional 1,123 studies were identified via citation and web-based searching.\\n2.3 Screening and selection process\\nA double-screening strategy was applied (Nama et al. 2019) in which two reviewers independently \\nscreened records. First, EndNote X9 was used to import references, remove duplicates, and sort studies \\nby publication year, title, and author to structure the screening sequence. Each reviewer then indepen\\xad\\ndently assessed titles and abstracts; a random subset was cross-checked to ensure consistency. Rayyan \\nfacilitated collaborative full‐text review and inclusion/exclusion decisions.\\nTo ensure inter-rater reliability, a random subset of studies was jointly reviewed, and reviewer \\nagreement was monitored. Although Cohen’s kappa coefficient was not formally calculated, consistency \\nwas verified through regular calibration and consensus discussions. Any discrepancies between reviewers \\nwere resolved through consensus discussion, and a third reviewer was not needed.\\n2.4 Final inclusion and data extraction\\nAfter removing duplicates and applying inclusion/exclusion criteria, a total of 44 studies were included, \\nwith 30 from database searches and 14 from other methods, as shown in Figure 1.\\nWe extracted the following data from each included study:\\n• Title, year, publication type, and author list  \\n• Country or region of focus \\n4\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 5}, page_content='• DL models used  \\n• Data representation methods  \\n• Performance metrics (e.g. accuracy, precision, F1-score)\\n3 Results and discussion\\n3.1 Characteristics of reviewed studies\\nFigure 2 presents a word cloud of key terms. ‘Deep Learning’, ‘Social Networking’, and ‘Twitter/X’, which \\nwere clearly stated in the article search, appear with high prominence in the word cloud. The CNN and \\nLSTM models are the most frequently used, alongside the terms related to semantic feature representation, \\nsuch as word embeddings and information management, which are essential for Twitter/X data mining.\\nStudies focusing on keyword generation emphasise the term ‘Traffic’ in various contexts, such as ‘Traffic \\nCongestion’, ‘Traffic Incident’,  and ‘Traffic information’,  indicating a primary focus on unexpected or \\nnon-recurring traffic events. For model evaluation, terms like ‘accuracy’  and ‘Accuracy assessment’  were \\ncommonly used, suggesting that the studies emphasise textual feature representation, model selection, and \\nframework construction over evaluation measures. Given the novelty of social media data analysis in traffic \\nresearch, diverse keywords like ‘Detection’,   ‘Event Detection’,  and ‘TED’  were employed.\\nFigure 3 is a world map showing the geographic distribution of study sites. India leads in publications, \\nfollowed by the UK and the US, which together account for approximately 45% of the reviewed studies. \\nIndonesia, China, and Pakistan contributed 25%. While India-centric data (8/44 studies) demonstrates feasibility \\nin local contexts, reliance on region-specific corpora may restrict generalisability. Future research should expand \\nvalidation efforts in underrepresented regions such as Africa and South America to ensure global applicability. \\nFigure 2. Word cloud of the prominent keywords in 44 studies.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n5'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 6}, page_content=\"Based on Figure 4, 60.98% of studies were published between 2021 and 2023, with publications doubling in 2023, \\nreflecting a surge in research interest during that period.\\nThe dominant subject areas were computer science (40.4%), engineering (25.8%), and mathematics \\n(12.4%). India's dominance is attributed to its large social media user base, extensive volume of Twitter/X \\ndata, and robust AI/ML capabilities, which are supported by collaborations among academia, industry, and \\ngovernment. The UK and US benefit from top-tier research institutions, advanced infrastructure, and \\nrobust funding for AI, ML, and DL studies in transportation research (Sahni and Raja 2018).\\n3.2 Deep-learning Twitter/X Data analysis for traffic events detection preprocessing workflow\\nA detailed workflow for deep-learning-based Twitter/X data analysis in TED, as shown in Figure 5, is \\nsummarised from the reviewed studies. However, variations exist in the techniques and algorithms as \\nresearchers explore different models, methods, and architectures. At its core is the data crawling stage, \\nwhere tweets are collected via REST or Streaming APIs using filters like keywords and geolocation. Data \\nFigure 3. Map of the world based on No. of publications related to Twitter/X TED using DL.\\nFigure 4. The trend of published studies on Twitter/X TED using DL (2017–2024) peaked in 2022–2023.\\n6\\nD. QUTAISHAT AND S. LI\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 7}, page_content='quality is enhanced during preprocessing, while feature representation prepares the data for the DL model. \\nA suitable model is selected, followed by training and evaluation to improve and assess performance. This \\nframework is adaptable to various social media platforms, including Facebook, Instagram, Twitter/X, and \\nSina Weibo.\\n3.2.1 Traffic tweets crawling on twitter/X\\nBased on the included studies, traffic-related data from Twitter/X was historically collected using two \\nprimary methods: REST APIs (programmatic Representational State Transfer) and Streaming APIs. These \\nAPIs allowed researchers to define a centroid (latitude, longitude), a radius, and a set of keywords using \\noperators, including AND, OR, and EXCLUDE, in their format (Ali et al. 2017; Ali et al. 2019).\\nEach API serves distinct purposes and offers unique advantages for researchers and developers. The \\nREST API was typically used for historical data collection, allowing users to query based on specific \\nkeywords, locations (centroid + radius), and timeframes. It supported up to 3,200 tweets per request, with \\na 15-minute rate limit of 350 requests and was suited for historical analysis or offline model training (Gu, \\nQian, and Chen 2016; Xu, Li, and Wen 2018; Ali et al. 2019). In contrast, the Streaming API offers real- \\ntime tweet collection as they are posted, making it ideal for live TED and traffic monitoring (Doguc and \\nAhmet 2023). However, it only returned a limited sample of tweets and lacked the flexibility of advanced \\nquery customisation.\\nAs of 2024, access to Twitter/X’s APIs has undergone a significant transformation. Following corporate \\nownership changes and a strategic shift toward monetisation, Twitter/X now severely restricts free-tier \\naccess. Real-time streaming and full historical search functionalities are available only through premium or \\nenterprise-level subscriptions. The cost of full access, which may reach up to $42,000/month, has rendered \\nlarge-scale data collection impractical for many academic researchers (Murtfeldt et al. 2024).\\nThis change has introduced a new challenge to reproducibility, limiting the ability to replicate earlier \\nstudies. Many previously effective crawling methods (e.g. Tweepy, REST API scripts, TWINT) have \\nbecome unreliable or non-functional under the updated API policies. In response, some researchers \\nhave shifted to alternative methods, such as web scraping; however, these approaches raise both ethical and \\ntechnical concerns (Poudel and Weninger 2024).\\nIn light of restricted Twitter/X API access as of 2024, researchers are encouraged to utilise existing \\npublicly available Twitter/X datasets to support reproducible research. For instance, the CrisisLexT6 \\ndataset includes annotated tweets from natural disasters, some of which are transportation-related \\n(Imran et al. 2015).\\nThe T4SA dataset contains over 4.5 million sentiment-labelled tweets and is accessible via GitHub \\n[https://github.com/codiceSpaghetti/T4SA-2.0]. Table 1 summarises widely available datasets, their \\ndomains, access types, and representative studies in which they have been used.\\nTable 2 provides a comparative overview of past TED studies, highlighting the data crawling methods \\nused and their viability under present-day Twitter/X API policies. It is essential to note that while some \\nresearchers have collected upwards of 1 million tweets (e.g. Alomari, Mehmood, and Katib 2019), such \\nvolumes are no longer realistically attainable without institutional resources or paid access. Jonnalagadda \\nand Hashemi (2021) collected approximately 10,000 tweets, of which 5,000 were used for developing the \\ndeep learning model. Dabiri and Heaslip (2019) retrieved around 50,000 tweets, with 17,437 manually \\nFigure 5. Generalised workflow for DL-based social media TED. It includes data crawling from social media platforms \\n(e.g. Twitter/X, Facebook), preprocessing, feature representation, DL model selection, training and validation, and final \\nmodel evaluation for accurate event detection.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n7'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 8}, page_content=\"identified as traffic-related tweets. Alomari, Mehmood, and Katib (2019) gathered over 1 million tweets, \\nbut only an estimated 5,000 were categorised as relevant to traffic events; the remainder reflected broader \\nsocial or environmental contexts. Neruda and Winarko (2021) attempted to supplement data using \\nTWINT, which is now non-operational due to changes in X’s frontend structure and API restrictions.\\nThe variability in Twitter/X data affects model development in multiple ways. While large datasets \\nenhance statistical strength, they often come with noisy labels and higher preprocessing demands (Tsou, \\nZhang, and Jung 2017; Effrosynidis, Sylaios, and Arampatzis 2024). In contrast, smaller datasets are cleaner \\nbut less generalisable and more prone to overfitting, especially when class imbalance exists, as seen in cases \\nwhere there are fewer tweets related to traffic incidents (Liu et al. 2024). Additionally, geotagged Tweets are \\nrare (found in only about 1–2% of tweets), limiting spatial analysis unless complemented by inferred or \\nexternal location data (Tsou, Zhang, and Jung 2017).\\nTable 1. Sample dataset comparison table.       \\nDataset name\\nDomain\\nLanguage\\nAccess type\\nUsed by\\nCrisisLexT6\\nCrisis/Traffic\\nEnglish\\nPublic via website\\n(Olteanu, Vieweg, and Castillo  2015; Alam, Ofli, and Imran  2018)\\nTwevent\\nEvent detection\\nEnglish\\nGitHub\\n(Li et al.  2012)\\nQCRI datasets\\nCrisis/Traffic\\nMultilingual\\nGitHub\\n(Alam, Ofli, and Imran  2018)\\nGeoCOV19\\nMobility\\nEnglish\\nPublic via portal\\n(Qazi, Imran, and Ofli  2020)\\nCustom (Dabiri)\\nTraffic-specific\\nEnglish\\nGitHub\\n(Dabiri and Heaslip  2019)\\nTable 2. Summary of studies highlighting social media platforms, crawling techniques, tweet collection volume, and data \\nrepresentation methods.         \\nStudy\\nPlatform\\nData crawling  \\nmethod\\nNo. of Tweets\\nFeature learning  \\nmethod\\nNotes on current feasibility\\n(Ali et al.  2021)\\nTwitter/X \\nFacebook\\nstandard APIs\\n60000 tweets \\n5000 Facebook \\nposts\\nWord2vec \\nFastText\\nStandard API access is \\ndeprecated or highly \\nlimited on Twitter/X as \\nof 2024.\\n(Dabiri and Heaslip  2019)\\nTwitter/X\\nn/a\\n50,000 tweets\\nWord2vec \\nFastText\\nMethod unspecified. \\nAssumed pre-2022 \\nfeasibility\\n(Alomari, Mehmood, and \\nKatib  2019)\\nREST API\\n1 million\\nTF-IDF\\nLarge-scale tweet \\ncollection using REST API \\nno longer viable \\n(requires enterprise \\naccess).\\n(Ali et al.  2019)\\nTwitter/X\\nREST and \\nStreaming \\nAPIs\\n30,000 tweets\\nString2word \\nGlove2vec \\nLexicon Features \\nDoc2vec\\nStreaming API deprecated \\nunder v2. Results not \\nreproducible with free- \\ntier access.\\n(Chen et al.  2018)\\nSina Weibo\\nSina Weibo \\ncrawler\\n11,000\\nContinuous Bag of \\nWords (CBOW)\\nNon-Twitter/X platform. \\nMethod not affected.\\n(Lu et al.  2018)\\nNews articles \\nand Weibo \\nposts,\\nNetwork of \\nsocial sensors\\n1.15 million texts\\nWord2Vec \\nCBOW\\nNon-Twitter/X platform.\\n(Zhang et al.  2018)\\nTwitter/X\\nStreaming API\\n3 million tweets\\nA systematic feature \\nselection process\\nHigh-volume collection no \\nlonger feasible under \\ncurrent Twitter/X API \\nlimits.\\n(Fatichah et al.  2020)\\nTwitter/X and \\nImages\\nTwitter/X API\\n10000 tweets \\n1000 images\\nWord Embedding\\nAPI usage requires \\nelevated or paid access \\ntiers.\\n(Jonnalagadda and \\nHashemi  2021)\\nTwitter/X\\nn/a\\n10,000 tweets\\nWord2vec\\nMethod unspecified. \\nPresumed historical \\nfeasibility.\\n(Ambastha and \\nDesarkar  2020)\\nTwitter/X\\nStreaming API\\n1887 tweets\\nTF-IDF \\nWord2Vec\\nStreaming API deprecated.\\n(Puangnak and \\nRachsiriwatcharabul  2022)\\nTwitter/X\\nn/a.\\n3363 tweets\\nWord Embedding Word \\nIndexing\\nNo crawling method \\nprovided.\\n(Neruda and Winarko  2021)\\nTwitter/X\\nTweepy \\nTWINT\\n6319 tweets\\nBidirectional Encoder \\nRepresentations from \\nTransformers (BERT)\\nTWINT is no longer \\nfunctional due to X's API \\nand frontend \\nobfuscation changes.\\n(Almassar and Girsang  2022)\\nTwitter/X\\nTwitter/X API\\n4,087 tweets\\nWord2vec \\nFastText\\nTweet API access is limited \\nunder new pricing tiers.\\n8\\nD. QUTAISHAT AND S. LI\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 9}, page_content='In summary, while Twitter/X data remains valuable for TED, practical access is now significantly \\nconstrained. Researchers must adapt by utilising limited-access APIs, employing ethical web scraping, or \\nleveraging multimodal datasets to compensate for the limitations imposed by the post-API era (Blakey \\n2024; Poudel and Weninger 2024).\\n3.2.2 Twitter/X data preprocessing\\nPreprocessing of raw Twitter/X data is a critical step in TED, as it addresses the platform’s non-standard \\nlanguage and reduces ambiguity, abbreviations, and uncertainty (Ramadhani and Goo 2017; Ali et al. 2019; \\nKarthik et al. 2023; Rezaeinia, Ghodsi, and Rouhollah 2017; Safitri et al. 2024). A key challenge is the \\nscarcity of traffic-related tweets relative to general posts, which further necessitates efficient preprocessing \\nto mitigate linguistic ambiguity and noise (Fatichah et al. 2020; Afyouni, Aghbari, and Razack 2022; Li, \\nDou, and Zhou 2023).\\nThe research addresses three primary goals: noise reduction, improved traffic event extraction, and enhanced \\ndataset generalisability (Garg and Kumar 2016; Ramadhani and Goo 2017; Xu, Li, and Wen 2018; Zhang et al. \\n2018; Ali et al. 2019; Azhar et al. 2022). However, challenges persist in handling the evolving nature of informal \\nlanguage, slang, emojis, sarcasm, and ambiguity in tweets (Fatichah et al. 2020). Additionally, privacy concerns \\nnecessitate adherence to regulations, including data anonymization and obtaining user consent (Ramadhani and \\nGoo 2017; Karthik et al. 2023). Integrating sentiment analysis and contextual information may help address \\nthese linguistic challenges, while privacy concerns require appropriate anonymization and consent procedures. \\nFigure 6 illustrates the preprocessing workflow for Twitter/X data for TED.\\n3.2.3 Semantic feature learning: from traditional vectors to semantic embeddings\\nA critical challenge in applying DL to traffic event detection TED lies in converting raw text into \\nmeaningful, vectorized representations suitable for algorithmic processing. Text representation plays a \\nfoundational role, influencing model accuracy and robustness, particularly when dealing with noisy and \\ninformal Twitter/X data (Zhang et al. 2018; Dabiri and Heaslip 2019; Neruda and Winarko 2021).\\nText representation in TED has evolved across three major phases:\\na. frequency-based models including Bag-of-Words (BoW) and Term Frequency-Inverse Document \\nFrequency (TF-IDF).  \\nb. Word embeddings such as Word2Vec and GloVe.  \\nc. contextual embeddings using transformer-based models such as BERT (discussed further in \\nSection 3.2.4).\\na. Traditional Frequency-Based Models: BoW and TF-IDF\\nEarly approaches to representing textual data in TED systems relied on BoW and TF-IDF. The BoW \\napproach converts unstructured tweets into fixed-size numerical vectors based on word occurrence, \\nignoring context and word order (Dabiri and Heaslip 2019). Each word is assigned a unique index, and \\ntweets are represented as N-dimensional vectors, where N is the vocabulary size. TF-IDF refines this by \\nweighting terms based on their frequency across multiple documents, which emphasises more informative \\nwords (Rajaraman and Ullman 2011).\\nSeveral studies have employed these methods in analysing traffic-related tweets. D’Andrea et al. (2015) \\nused IDF-based feature selection to classify Italian tweets, while Alomari et al. (2021) developed a Spark- \\nbased feature extraction pipeline for Arabic tweets. However, both BoW and TF-IDF suffer from key \\nlimitations: they ignore word order, semantic similarity, and contextual meaning. This reduces their \\neffectiveness in handling the informal, slang-heavy, and abbreviation-rich nature of social media data \\n(Deho et al. 2018; Rudkowsky et al. 2018; Dabiri and Heaslip 2019).\\nAs deep learning models require dense, semantically rich input vectors, the limitations of BoW and TF- \\nIDF have led to a shift toward word embedding techniques that can capture syntactic and semantic \\nrelationships more effectively. This evolution reflects the broader transition from classical vectorisation to \\nneural embedding in modern TED pipelines.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n9'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 10}, page_content='Figure 6. Twitter/X data preprocessing steps applied to a traffic-related tweet, including text cleaning, normalisation, \\ntokenization, and lemmatization for model-ready input.\\n10\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 11}, page_content=\"b. Transition to Word Embeddings\\nWord embeddings extend beyond traditional vectorisation by capturing contextual relationships \\nbetween terms (Ali et al. 2019; Dabiri and Heaslip 2019). Widely applied across natural language \\nprocessing, computer science, artificial intelligence, machine learning, and computational linguistics, \\nthese methods map words into low-dimensional vector spaces that encode syntactic, semantic, and \\ndistributional meanings (Noori and Mehra 2020; Sampath and Supriya 2023). Common word embedding \\ntechniques include String2Vec, Word2Vec, Doc2Vec, GloVe, and FastText, with Word2Vec and FastText \\nbeing the most frequently used in Twitter/X-based TED. In summary, while Twitter/X data remains \\nvaluable for TED, practical access is now significantly constrained. Researchers must adapt by utilising \\nlimited-access APIs, employing ethical web scraping, or leveraging multimodal datasets to compensate for \\nthe limitations imposed in the post-API era (Blakey 2024; Poudel and Weninger 2024). Table 2 sum\\xad\\nmarises the feature learning methods utilised in this domain.\\nWord2Vec, introduced by Google in 2013, is an unsupervised DL technique that learns word represen\\xad\\ntation by capturing semantic relations, synonyms, and analogies through analysis of large text corpora like \\nTwitter/X (Bilgin and Şentürk 2017; Mikolov, Quoc V, and Ilya 2013; Mikolov and Sutskever, 2013). \\nAlthough it generalises well, it struggles with Out-of-Vocabulary words (Bilgin and Şentürk 2017).\\nFastText, developed by Facebook's AI Research lab, represents words as sub-word n-grams. It is \\neffective in handling typos, abbreviations, and informal language commonly found in Twitter/X traffic \\nreports. FastText enhances NER and DL models by capturing morphological similarities (Mannes 2017).\\nGloVe (Global Vectors for Word Representation) differs from Word2Vec in that it leverages global co- \\noccurrence statistics. While it performs well on similar tasks, it struggles with words that have multiple \\nmeanings (Pennington, Socher, and Manning 2014; Abad et al. 2016).\\nDoc2Vec, an extension of Word2Vec, generates embeddings for full documents, enabling phrase and \\nparagraph-level analysis (Bilgin and Şentürk 2017; Kamkarhaghighi and Makrehchi 2017). Figure 7\\npresents the efficiency and comparative performance of word embedding methods in TED.\\nc. Performance and Trade-Offs in TED Applications\\nSeveral studies have demonstrated the effectiveness of word embeddings in Twitter/X-based TED, often \\noutperforming traditional BoW and TF-IDF models. Dabiri and Heaslip (2019) found that integrating \\nCNN with Word2Vec achieved better accuracy and F-score. Similarly, Lu et al. (2018) developed a \\nWord2Vec-based approach that outperformed CBOW in terms of traffic incident detection accuracy.\\nAli et al. (2019) evaluated String2Word, Word2Vec, Doc2Vec, and GloVe for transportation sentiment \\nanalysis, identifying key terms such as ‘Crash,’ ‘Accident,’ ‘Traffic,’ ‘Speed,’ and ‘Event.’ While \\nFigure 7. Comparison of word embedding models for traffic text, showing their strengths in context capture, noise \\nhandling, and classification. The bottom panel compares typo tolerance, context understanding, data efficiency, speed, \\nand classification performance.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n11\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 12}, page_content='String2Word had good accuracy, its high dimensionality increased computational time and reduced \\nprecision. Doc2Vec achieved 74% accuracy using logistic regression and 73% with deep learning, out\\xad\\nperforming both Word2Vec and GloVe. Finally, GloVe, trained on a transportation corpus, reached 66% \\naccuracy in deep learning tasks, excelling in feature recognition but underperforming in accuracy \\ncompared to Word2Vec and Doc2Vec. Almassar and Girsang (2022) tested FastText, achieving 86.33% \\naccuracy and a 96.61 F-score in congestion detection. Fatichah et al. (2020) combined FastText with CNN, \\nLSTM, and C-LSTM for handling out-of-vocabulary terms for incident detection. Ambastha and Desarkar \\n(2020) compared TF-IDF and Word2Vec, using SVM, Naïve Bayes, CNN, and LSTM models. Azhar et al. \\n(2022) proposed an integrated model combining word embeddings with numeric traffic and sentiment \\nlayers to enhance TED performance.\\nDespite their advantages, advanced embeddings can pose computational challenges in real-time \\nsystems. Lightweight models, such as Word2Vec and FastText, are widely preferred due to their fast \\ninference and low memory usage, making them ideal for large-scale tweet streams. In contrast, Doc2Vec \\nprovides richer semantic representations but requires greater computational resources, including GPU \\nacceleration and longer training times. Embeddings exceeding 300 dimensions can slow processing \\nconsiderably in high-throughput environments. Thus, choosing an embedding requires balancing seman\\xad\\ntic accuracy against processing efficiency, particularly in latency-sensitive deployments (Rudkowsky et al. \\n2018; Ali et al. 2019; Gu et al. 2021).\\nTo improve conceptual clarity while considering practical limitations, we compare embeddings based \\non both performance and suitability for different use cases. FastText, due to sub-word modelling, is ideal \\nfor short and informal text (Bonandrini and Gatti 2024). Conversely, semantically rich models like \\nDoc2Vec or GloVe are better suited to batch processing and archival analysis, where computational \\nlatency is less critical (Pita and Pappa 2018).\\nWord embeddings offer clear benefits but also introduce challenges. One major issue is optimising the \\nembedding dimension, typically set between 100 and 300, which affects both accuracy and computational \\ncost (Asudani et al. 2023). Embeddings also struggle with informal and evolving social media language, \\nsuch as slang and abbreviations. Additionally, relying on pre-trained embeddings may limit adaptability, \\nthus requiring domain-specific fine-tuning (Raunak 2017; Wilson et al. 2020; Torregrossa et al. 2021; \\nAsudani et al. 2023). Table 3 summarises the trade-offs among various embedding methods in terms of \\nsemantic richness and computational demands.\\nFurthermore, high-dimensional embeddings increase computational complexity, which affects scalabil\\xad\\nity in real-time TED applications (Deho et al. 2018; Rudkowsky et al. 2018; Gu et al. 2021). In summary, \\nthe selection of an appropriate embedding technique depends on dataset characteristics, computational \\nconstraints, and task-specific requirements (Gu et al. 2021). These considerations are especially important \\nwhen deploying models in real-time systems, where the trade-off between embedding accuracy and \\ncomputational efficiency becomes critical.\\n3.2.4 DL models for TED\\nTo build a solid foundation, it is essential to contextualise the development of TED models from ML to DL \\nand, more recently, to transformer-based architectures. This shift reflects advances in data availability, \\nalgorithmic design, and computing power.\\nTable 4 presents the evolution of TED methodologies from traditional ML approaches to transformer- \\nbased models. During the ML era, algorithms such as SVM, Decision Trees, and Random Forests formed \\nthe foundation of early TED systems. These models were typically combined with TF-IDF or Bag-of- \\nTable 3. The trade-offs between embedding methods in terms of semantic richness and computational efficiency.         \\nEmbedding \\nmethod\\nSematic richness\\nTraining time\\nInference \\nspeed\\nComputational cost\\nDimensionality\\nBest use case\\nWord2Vec\\nModerate\\nFast\\nFast\\nLow\\n100-300\\nReal-time systems, short texts\\nFastText\\nModerate–High\\nVery Fast\\nVery Fast\\nVery Low\\n100-300\\nReal-time, Out-of-Vocabulary \\nhandling\\nDoc2Vec\\nHigh\\nSlow\\nModerate\\nMedium–High\\n+ 300\\nDocument-level classification\\nGloVe\\nModerate\\nModerate\\nFast\\nMedium\\n100-300\\nSentiment analysis, pre- \\ntrained use\\n12\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 13}, page_content='Words (BoW) for feature representation. Prior studies have shown that such approaches lack context \\nsensitivity and require extensive manual feature engineering (Atefeh and Khreich 2013; D’Andrea \\net al. 2015).\\nA major shift occurred in the deep learning era, with models such as CNNs, LSTMs, and GRUs \\nemerging as more effective alternatives. These architectures learn hierarchical and temporal patterns \\ndirectly from raw text, enhancing performance on unstructured tweet data (Dabiri and Heaslip 2019; \\nAlmassar and Girsang 2022).\\nFinally, the transformer era introduced models like BERT, which offer bidirectional contextual under\\xad\\nstanding, effectively handle multilingual and noisy data, and support multimodal data integration. These \\nmodels, however, are computationally intensive (Neruda and Winarko 2021; Nirbhaya and Suadaa 2023).\\nThe effectiveness of DL models in TED depends on the quantity and quality of training data, as well as \\nthe complexity of the tasks. DL Models like CNN, LSTM, and BERT require large, labelled datasets, \\nwhereas simpler ML models can perform reasonably well on smaller datasets. Model selection is also \\ninfluenced by the application. Complex models tend to improve accuracy, while simpler models offer faster \\ninference. Integrating features such as emotion, weather, and geo-location data can further enhance TED \\nperformance. However, no single DL model guarantees optimal results. The choice depends on dataset \\ncharacteristics, feature engineering, and computational constraints (Azhar et al. 2022; Yang 2022). This \\nsection reviews the DL models adopted for TED in the selected studies. Figure 8 illustrates their capabilities \\nfor Twitter/X-based TED.\\n3.2.4.1 Multi-Layer Perceptron (MLP)\\nMLP is a feed-forward neural network comprising an input layer, \\na hidden layer, and an output layer. Jonnalagadda and Hashemi (2021) compared MLP with SVM, Bi- \\nLSTM, and CNN, finding that MLP underperformed due to its inability to capture sequential \\ndependencies. Similarly, Puangnak and Rachsiriwatcharabul (2022) showed that CNN and \\nCNN + LSTM outperformed MLP, underscoring the importance of sequence modelling. MLP remains \\nuseful but is highly dependent on data quality and feature integration.\\n3.2.4.2 Convolutional Neural Networks (CNNs)\\nOriginally developed for image recognition, CNNs \\nhave been effectively adapted for natural language processing by applying one-dimensional \\nconvolutions to text embeddings. They excel at identifying local patterns, such as n-grams, making \\nthem suitable for keyword-based incident detection. Integrated with pretrained embeddings like \\nFastText, CNNs perform well even on noisy text. Hybrid models, such as CNN + LSTM, combine \\nspatial pattern recognition with temporal learning, thereby improving overall accuracy (Alifi and \\nSupangkat 2018; Dabiri and Heaslip 2019; Liu et al. 2020; Neruda and Winarko 2021; Jain et al. 2023; \\nQutaishat and Li 2025b). Almassar and Girsang (2022) found that CNN + FastText outperformed \\nCNN + Word2Vec and SVM. Dabiri and Heaslip (2019) found that CNN outperformed both LSTM \\nand CNN + LSTM. CNN + LSTM hybrids further enhance performance by combining spatial and \\ntemporal features. Chen et al. (2018) confirmed this in Sina Weibo traffic analysis, where LSTM–CNN \\nachieved the top F1 score.\\n3.2.4.3 Recurrent neural networks (RNNs) and long short-term memory (LSTM)\\nRNNs process \\nsequential data but struggle with long-term dependencies due to the vanishing gradient problem. LSTM \\naddresses this issue. Azhar et al. (2022) reported that LSTM achieved 94.2% accuracy in road accident \\ndetection, followed by GRU of 91.6% and RNN of 39.7%. Yang (2022) combined stacked autoencoders \\nwith LSTM, achieving 98.25% accuracy, which outperforms CNN + LSTM at 96.36%.\\nTable 4. Evolution of TED methodologies from ML to transformer-based models, highlighting models, data \\nrepresentation, and modelling capabilities.       \\nMethodology era\\nModels\\nData representation\\nStrengths\\nWeaknesses\\nML\\nSVM, Decision Trees, Random Forests\\nTF-IDF, BoW\\nSimple, interpretable\\nNo semantic/contextual \\nunderstanding\\nDL\\nCNN, LSTM, GRU\\nWord Embedding\\nLearns hierarchical \\nfeatures\\nRequires more data & \\ncomputing\\nTransformers\\nBERT, Multimodal Bi-transformers (MMBT), \\nVision-and-Language Transformer (ViLT)\\nMultimodal inputs\\nContextual & cross-modal \\nreasoning\\nResource intensive\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n13'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 14}, page_content='3.2.4.4 Bidirectional LSTM (Bi-LSTM)\\nBi-LSTM improves upon LSTM by capturing bidirectional \\ndependencies. Puangnak and Rachsiriwatcharabul (2022) reported an accuracy of 93.53% in incident \\ndetection using Bi-LSTM, although the accuracy for severity classification was lower, reaching 77.92%. Alifi \\nand Supangkat (2018) developed a hybrid model combining Bi-LSTM and CNN, achieving an F-score of 78.9%.\\n3.2.4.5 CNN-LSTM(C-LSTM)\\nC-LSTM integrates CNNs for spatial feature extraction with LSTMs for \\ntemporal learning. Fatichah et al. (2020) adapt C-LSTM for incident detection, achieving 99.09% accuracy. \\nCombined with VGG16, it yielded the highest multimodal prediction confidence. Zeng et al. (2019) \\nreported C-LSTM outperformed both CNN at 80.27% and LSTM at 80.96% in traffic classification.\\n3.2.4.6 Generative Adversarial Networks (GANs)\\nGANs consist of a generator and a discriminator \\nworking together to improve data quality. They are used for detecting traffic anomalies and generating \\nsynthetic, yet realistic, data. Liu et al. (2024) proposed a GAN-Transformer for TED, using GANs to \\nbalance datasets while Transformers extracted complex relations. Lin et al. (2020) utilised GANs to \\nmitigate data imbalance, resulting in a 3.2% improvement in accuracy and a 5.65% reduction in false \\nalarms.\\n3.2.4.7 Gated Recurrent Unit (GRU)\\nGRU, developed by Kyunghyun Cho in 2014, is an RNN variant \\nwith update and reset gates to manage information flow. Azhar et al. (2022) demonstrated that GRU \\nachieved an accuracy of 93.7% in detecting accident-related tweets. Suat-Rojas, Gutierrez-Osorio, and \\nPedraza (2022) proposed a GRU-CNN hybrid that outperforms baseline models in accident prediction.\\n3.2.4.8 Bidirectional Encoder Representations from Transformers (BERT)\\nBERT was developed in two \\nmodel sizes: BERTBASE (110 M parameters) and BERTLARGE (340 M parameters). Although studies on \\nBERT for TED are limited, it has proven effective for detecting and analysing traffic incidents in noisy, \\nunstructured social media data (Qutaishat and Li 2025a). Nirbhaya and Suadaa (2023) reported 99.26% \\naccuracy using IndoBERT in Indonesian-language TED, outperforming traditional models. Neruda and \\nWinarko (2021) demonstrated that BERT-CNN outperformed ELMo-CNN and Word2Vec-CNN in terms \\nof performance.\\nFigure 8. DL models for Twitter/X traffic event detection, highlighting their roles in feature extraction, sequential \\nprocessing, and noise robustness.\\n14\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 15}, page_content=\"3.2.5 DL model training and validation\\nTo achieve effective training and model validation, Twitter/X-based TED studies commonly divide \\ndatasets into three subsets: training, validation, and testing (Raschka 2018). The training set is repeatedly \\nused for model learning, allowing it to discover the underlying patterns, while the validation and test sets \\nassess the model's performance and generalisation.\\nThe proportions of these subsets often vary based on dataset size and characteristics. In social media-based \\nTED, where data sparsity and informal language are common, selecting appropriate splits becomes crucial (Jain \\net al. 2022; Savvides and Mäkelä 2023; Qiu 2024). As illustrated in Figure 9 although a general protocol for \\ndataset partitioning exists in DL, many studies fail to adhere to it.\\nSeveral studies report their data split strategies. Alifi and Supangkat (2018) used a 70:15:15 split for training, \\ntesting, and validation. Nirbhaya and Suadaa (2023) employed k-fold cross-validation, dividing the data into 80% \\ntraining, 10% validation, and 10% testing sets. Almassar and Girsang (2022) used 60% for training, 20% for \\nvalidation, and 20% for testing. Puangnak and Rachsiriwatcharabul (2022) explored multiple ratios, including \\n50:50, 60:40, 70:30, 80:20, 90:10, and 95:5, and identified 90:10 and 95:5 as optimal; however, they ultimately \\nadopted 70:30. Nevertheless, their study lacked detail on the validation procedure. Neruda and Winarko (2021) \\nemployed a 64:16:20 split and explicitly noted measures to prevent data leakage through preprocessing and \\nfeature extraction. Table 5 presents the dataset partitioning strategies across reviewed TED studies.\\nWhile many studies report their dataset split ratios, few evaluate how these choices impact model \\nperformance. High training proportions, such as 90:10 or 95:5, may boost training accuracy but risk \\noverfitting due to limited evaluation data. In contrast, balanced splits, such as 70:30 or k-fold cross- \\nvalidation, generally provide more robust estimates of generalisability. For instance, although Puangnak \\nand Rachsiriwatcharabul (2022) reported strong accuracy with 90:10 and 95:5 splits, the absence of a \\ndetailed validation procedure limits interpretability. Figure 10 illustrates how reported performance \\nmeasures vary by split strategy, reinforcing the need for standardised, transparent validation procedures \\nto support fair benchmarking across TED studies.\\n3.2.6 Model performance measures and evaluations\\nBased on Table 5, most studies included in this review used accuracy and F-score to evaluate model \\nperformance. Accuracy is a common starting point due to its simplicity and clear indication of overall correct                       \\nFigure 9. The DL dataset split for model selection and evaluation. Though widely recommended, this protocol is often \\ninconsistently applied in practice.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n15\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 16}, page_content='Table 5. DL-based twitter/X TED studies.           \\nStudy author\\nStudy objective\\nModel\\nDat split (train/ \\nvalidation/test) & \\nvalidation \\nmethod\\nPerformance measures\\nFuture work\\nAccuracy \\n(%)\\nF1-score (%)\\nPrecision (%)\\nRecall \\n(%)\\nAlmassar and \\nGirsang ( 2022)\\nTraffic \\ncongestion \\ndetection\\nCNN + Word2Vec, \\nCNN + FastText \\nSVM\\n60/20/20—Grid \\nSearch Cross- \\nValidation\\n85.79% \\n86.33% \\n67.62%\\n86.11% \\n96.61% \\n68.53%\\n80.59% \\n81.18% \\n63.84%\\n92.45% \\n92.83% \\n73.96%\\n• Improve model accuracy.  \\n• \\nIntegrate data sources.  \\n• \\nEnhance predictive capabilities.  \\n• \\nScale to other regions/languages.  \\n• \\nExplore real-time implementation and \\nperformance.\\nZhang \\net al. ( 2018)\\nTraffic accident \\ndetection\\nDeep Belief Network \\nANN \\nLSTM \\nSVM\\nNot reported—5- \\nFold Cross- \\nValidation\\n85% \\n82% \\n81% \\n79%\\nNA \\nNA \\nNA \\nNA\\n92% \\n81% \\n87% \\n84%\\nNA \\nNA \\nNA \\nNA\\n• \\nInvestigate underreported accidents on \\nTwitter/X.  \\n• \\nIntegrate social media data for better traffic \\njam detection.  \\n• \\nDevelop a common dataset via community \\ncollaboration.\\nPuangnak and \\nRachsiriwatchara\\xad\\nbul ( 2022)\\nRoad traffic \\nincident reports \\nclassification\\nMLP \\nCNN \\nBi-LSTM \\nLSTM + CNN\\n70/30—10-Fold \\nCross-Validation\\n44.4% \\n93.24% \\n77.92% \\n93.44%\\nNA \\nNA \\nNA \\nNA\\nNA \\nNA \\nNA \\nNA\\nNA \\nNA \\nNA \\nNA\\n• \\nAutomate incident detection.  \\n• \\nReduce reliance on manual surveillance.  \\n• \\nBuild accurate DL models for traffic \\nclassification.\\nAzhar et al. ( 2022)\\nRoad accident \\ndetection and \\nprediction\\nGRU \\nRNN \\nLSTM\\n80/20—k-Fold \\nCross-Validation \\n(k not specified)\\n93.7% \\n91.6% \\n94.2%\\n93% \\n90% \\n95%\\n91% \\n90% \\n94%\\nNA \\nNA \\nNA\\n• \\nManage large Twitter/X datasets for \\naccident detection.  \\n• \\nImprove geo-location accuracy.  \\n• \\nManage incomplete datasets, such as \\nweather info.\\n(Neruda and \\nWinarko ( 2021)\\nTED\\nBERT + CNN \\nELMo + CNN \\nWord2Vec + CNN\\n64/16/20—5- \\nFold Cross- \\nValidation\\nNA \\nNA \\nNA\\n90% \\n88% \\n87%\\n90% \\n90% \\n88%\\n92% \\n87% \\n87%\\n• \\nEnhance data labelling for better TED.  \\n• \\nAssess fine-tuning of pre-trained word \\nembeddings on task performance.  \\n• \\nExplore advanced oversampling techniques \\nlike MLSMOTE.  \\n• \\nTest various neural network architectures for \\ndetection improvements.\\nJonnalagadda and \\nHashemi ( 2021)\\nTED\\nRF \\nSVM \\nMLP \\nBLSTM \\nCNN\\n20% \\nValidation—5- \\nFold Cross- \\nValidation\\n67% \\n73.1% \\n78.7% \\n88.1% \\n93.3%\\n67% \\n73.4% \\n78% \\n88% \\n93%\\nNA \\nNA \\nNA \\nNA\\nNA \\nNA \\nNA \\nNA\\n• \\nEnhance TED by integrating additional data \\nsources and refining algorithms.  \\n• \\nImprove accurate identification of traffic \\nevents from social media data.  \\n• \\nRefine algorithms for better accuracy and \\nreliability.  \\n• \\nAddress the vanishing gradient problem \\nRNN to improve long-term dependencies.  \\n• \\nExpand TED beyond binary classification to \\ninclude fine-grained categorisation.\\nFatichah \\net al. ( 2020)\\nIncident type \\nprediction\\nCNN \\nCNN + LSTM\\nNot \\nreported—Hold\\xad\\nout Validation\\n98.95% \\n99.09%\\nNA \\nNA\\nNA \\nNA\\nNA \\nNA\\n• \\nEnhance real-time processing for live \\nmonitoring.  \\n• \\nAssess model robustness in varied \\nenvironments. \\n16\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 17}, page_content=\"Table 5. (Continued) \\nStudy author\\nStudy objective\\nModel\\nDat split (train/ \\nvalidation/test) & \\nvalidation \\nmethod\\nPerformance measures\\nFuture work\\nAccuracy \\n(%)\\nF1-score (%)\\nPrecision (%)\\nRecall \\n(%)\\n• \\nAnalyse data augmentation effects on CNN \\narchitectures.  \\n• \\nIncorporate audio and video for incident \\ndetection.  \\n• \\nExplore advanced DL models like \\ntransformers for classification.\\nDabiri and \\nHeaslip ( 2019)\\nTraffic incident \\ndetection\\nCNN + Word2vec \\nLSTM + Word2vec \\nClSTM + Word2vec \\nCNN + FastText \\nLSTM + FastText \\nClSTM + FastText \\nCNN + Random Word \\nvector \\nLSTM + Random Word \\nvector \\nCLSTM + Random word \\nvector\\nNot reported—5- \\nFold Cross- \\nValidation\\n98.6% \\n98.4% \\n98.5% \\n98.6% \\n98.5% \\n98.6% \\n50.1% \\n50.3% \\n49.9%\\n98.6% \\n98.4% \\n98.5% \\n98.6% \\n98.5% \\n98.6%3939.8% \\n50.2% \\n49.8% \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA\\n• \\nDevelop an efficient geocoder to extract \\ntraffic event locations from tweets.  \\n• \\nImplement a unified Twitter/X-based traffic \\ninformation system across traffic networks.  \\n• \\nEnhance real-time traffic information \\ndissemination to drivers and traffic \\nmanagers.  \\n• \\nExplore integration with transportation \\nagencies and state police for improved \\ntraffic flow management\\nAmbastha and \\nDesarkar ( 2020)\\nTraffic \\ncongestion \\ndetection\\nCNN \\nLSTM \\nULMFiT\\nL-TWITS split (not \\nspecified)—Hol\\xad\\ndout Validation\\nNA \\nNA \\nNA\\n78% \\n82% \\n90%\\n70% \\n76% \\n84%\\n64% \\n74% \\n89%\\n• \\nExplore traffic location detection from tweet \\ncontent.  \\n• \\nExpand the L-TWITS dataset by collecting \\nmore tweets on diverse traffic incidents.  \\n• \\nAddress ULMFiT's prediction failures for \\nsarcastic and irrelevant traffic tweets.\\nAli et al. ( 2021)\\nTraffic Incident \\nDetection and \\nCondition \\nAnalysis\\nOLDA + word2Vec + RNN \\nOLDA + word2Vec + LST\\xad\\nM \\nOLDA + word2Vec + Bi- \\nLSTM \\nOLDA + FastText + RNN \\nOLDA + FastText + LSTM \\nOLDA + FastText + bi- \\nLSTM\\n70/30—Holdout \\nValidation\\n80% \\n85% \\n91% \\n85% \\n92% \\n97%\\n77% \\n84% \\n89% \\n85% \\n92% \\n97%\\n83% \\n83% \\n85% \\n82% \\n94% \\n97%\\n71% \\n86% \\n94% \\n88% \\n92% \\n97%\\n• \\nData Enhancement \\n• \\nIncorporating diverse event types \\n• \\nImproving model performance\\nAlifi and \\nSupangkat ( 2018)\\nTraffic \\ncondition \\nrecognition\\nBi-LSTM + CNN\\n70/15/ \\n15—Holdout \\nValidation\\n78.9%\\n82.1%\\n75%\\n• \\nCombine GANs with other DL models for \\nimproved accuracy. \\n• \\nAdapt the framework for real-time traffic \\ndata processing. \\n• \\nTest on diverse datasets to ensure \\nrobustness. \\n• \\nAddress imbalances in incident types and \\nconditions. \\n• \\nInvestigate CNNs or RNNs to boost GAN \\nperformance.\\n(Continued) \\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n17\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 18}, page_content='Table 5. (Continued) \\nStudy author\\nStudy objective\\nModel\\nDat split (train/ \\nvalidation/test) & \\nvalidation \\nmethod\\nPerformance measures\\nFuture work\\nAccuracy \\n(%)\\nF1-score (%)\\nPrecision (%)\\nRecall \\n(%)\\nLin et al. ( 2020)\\nTraffic incident \\ndetection\\nGAN \\nRF \\nSVM\\nNot \\nreported—10- \\nFold Cross- \\nValidation\\n91.87% \\nNA \\n91.53%\\nNA \\nNA \\nNa\\nNA \\nNA \\nNA\\nNA \\nNA \\nNA\\n• \\nCombine GANs with other DL models for \\nimproved accuracy. \\n• \\nAdapt the framework for real-time traffic \\ndata processing. \\n• \\nTest on diverse datasets to ensure \\nrobustness. \\n• \\nAddress imbalances in incident types and \\nconditions. \\n• \\nInvestigate CNNs or RNNs to boost GAN \\nperformance.\\nNirbhaya and \\nSuadaa ( 2023)\\nTraffic incident \\ndetection\\nSVM \\nNavey Bayes \\nLogistic Regression \\nLSTM \\nIndoBERT\\n80/10/10—5- \\nFold Cross- \\nValidation\\n98.81% \\n93.86% \\n98.68% \\n96.14% \\n99.26%\\n98.54% \\n92.49% \\n98.37% \\n95.16% \\n99.10%\\n98.78% \\n91.91% \\n98.77% \\n95.72% \\n99.20%\\n98.31% \\n93.16% \\n98% \\n94.72% \\n99.02%\\n• \\nEnhanced data collection \\n• \\nAdvanced pre-processing techniques \\n• \\nIncorporating multimodal data\\nNote: CNN = Convolutional Neural Network; LSTM = Long Short-Term Memory; Bi-LSTM = Bidirectional LSTM; SVM = Support Vector Machine; MLP = Multi-Layer Perceptron; GRU = Gated Recurrent Unit; \\nGAN = Generative Adversarial Network; ULMFiT = Universal Language Model Fine-tuning for Text Classification; RF = Random Forest; L-TWITS = Labelled-TWeets for Indian Traffic Scenario; MLSMOTE = Multi-Label \\nSynthetic Minority Over-sampling Technique. NA = Not Available. Some studies did not report full metric sets or specify validation splits. \\n18\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 19}, page_content=\"predictions. However, with imbalanced data, a frequent issue in TED where incident-related tweets are fewer \\nthan non-incident ones, accuracy can be misleading. It often prioritises the majority class while overlooking \\nperformance on minority classes (Johnson and Khoshgoftaar 2019). To mitigate this, the F-score is widely \\nadopted for its balance of precision, which refers to the proportion of correctly identified positives, and recall, \\nwhich refers to the proportion of actual positives correctly captured (Dinga et al. 2019).\\nNonetheless, comparisons across studies reveal inconsistencies. For example, Almassar and Girsang \\n(2022) reported an accuracy of 86.33% using CNN + FastText, but with a lower precision of 81.18%, \\nindicating a potential misclassification of minority-class events. Similarly, Ali et al. (2021) achieved \\naccuracy scores up to 97%, while some F1-scores ranged from 84% to 89%, reflecting the influence of \\nclass imbalance on model reliability.\\nMost studies mention data augmentation or resampling to address this issue, but rarely specify the methods \\nused. For instance, Neruda and Winarko (2021) referred to synthetic oversampling, and Jonnalagadda and \\nHashemi (2021) adapted weighted loss functions to emphasise underrepresented labels. Other studies, such as \\nthose by Azhar et al. (2022), Puangnak and Rachsiriwatcharabul (2022), and Ambastha and Desarkar (2020), \\neither referred to augmentation broadly or did not report a specific strategy.\\nClass imbalance can significantly reduce recall and F1-scores for rare events (Johnson and Khoshgoftaar \\n2019; Walsh and Tardy 2022; Jiang et al. 2025). This lack of transparency in the study's methodology \\nmakes it challenging to assess the comparative effectiveness of imbalance-handling techniques. So future \\nwork should clearly report the techniques used and their impact on minority-class performance. Table 6\\npresents common techniques for handling class imbalance in deep learning models (Johnson and \\nKhoshgoftaar 2019; Walsh and Tardy 2022; Jiang et al. 2025).\\n4 Challenges, open issues, and future directions\\nChallenges and limitations arise at each stage of the DL-based Twitter/X TED workflow, culminating in \\nthe final detection stage. Key issues, open challenges, and future directions are discussed below and \\nsummarised in Table 5.\\nFigure 10. Performance metrics (accuracy, precision, recall, and F1-score) reported across studies using various dataset \\nsplit strategies for TED.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n19\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 20}, page_content='4.1 Data quality and labelling\\nTwitter/X’s dynamic nature produces noisy data with misspellings, abbreviations, and informal language that \\nhinder accurate TED. Ambiguity and brevity in tweets further complicate the extraction of meaningful \\ninformation. DL models require substantial labelled data, yet obtaining sufficient traffic-related tweets remains \\ndifficult. Manual labelling is time-consuming and resource-intensive. Two solutions have been suggested:\\na. Adopting semi-supervised learning to leverage a small volume of labelled data alongside a larger pool of \\nunlabelled data for DL model training.  \\nb. Incorporating external sources such as police reports or traffic authority data to automatically label \\nrelevant tweets.\\n4.2 Imbalanced data distribution\\nTraffic-related events that occur more frequently, such as ‘congestion,’ are given more priority than those \\nthat rarely occur, such as ‘accidents,’ which skew the training process. To mitigate this challenge, \\nresearchers can use data augmentation, which can be used to generate new data from the existing dataset \\nand train DL models. Additionally, assigning higher weights to misclassify rare events during training can \\nbe incorporated to ensure attention is paid when modelling.\\n4.3 Ethics and data governance\\nThe application of social media data for identifying traffic events presents ethical and governance challenges. \\nPrivacy concerns arise when users are unaware that their posts are being analysed, necessitating informed \\nconsent and data anonymization (Mredula and Noyon, 2022). Additionally, algorithms trained on such data \\nmay reflect existing social biases, which can potentially lead to unfair outcomes. Addressing these issues requires \\nthe adoption of fairness-aware models (Chen and Wang 2019; Alomari and Mehmood 2023). Data governance \\nalso plays a key role, involving questions about data ownership and compliance with regulations such as the \\nGeneral Data Protection Regulation, which mandates user consent and control over personal data (Chen, Chen, \\nand Qian 2014; Melhem, Abdi, and Meziane 2024). While technical issues are being managed, the focus should \\nbe on ethical standards and regulatory frameworks. A responsible approach will require collaboration across \\ntechnology, ethics, and policy domains to ensure both effectiveness and respect for user rights.\\n4.4 Multi-modal data integration\\nOngoing research explores the integration of traffic sensor data, speed logs, and accident reports to \\nenhance traffic condition analysis. Multimodal fusion techniques combine inputs from multiple sources, \\nTable 6. Common techniques for handling class imbalance in deep learning models.      \\nTechnique\\nDescription\\nType\\nReported effect\\nRandom oversampling/ \\nundersampling\\nDuplicate minority class samples or downsample \\nthe majority class to achieve a balanced \\ndistribution.\\nData-level\\nImproved recall, sometimes at the \\ncost of precision.\\nClass weighting/cost-sensitive \\nlearning\\nAssign higher loss weights to minority classes \\nduring model training.\\nAlgorithm-level\\nBalanced F1-score, reduced bias \\ntowards the majority class.\\nSMOTE (synthetic minority over- \\nsampling technique)\\nGenerate synthetic minority samples by \\ninterpolating the feature space.\\nData-level\\nIncreased sensitivity to rare classes; \\nreduced under-detection.\\nADASYN/MLSMOTE (advanced \\nSMOTE variants)\\nAdaptive generation of minority samples, \\nfocusing on harder-to-learn examples.\\nData-level\\nImproved minority-class recall, but \\nhigher training complexity.\\nGAN-based data augmentation\\nUse Generative Adversarial Networks to \\nsynthesise realistic minority samples.\\nHybrid\\nBoosted accuracy and F1-scores, \\nespecially in rare-event detection.\\nEnsemble methods\\nCombine classifiers trained on different balanced \\nsubsets.\\nAlgorithm-level\\nImproved robustness, stable \\nperformance across imbalanced \\ndatasets.\\n20\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 21}, page_content='boosting detection accuracy. Recent work also emphasises spatial and temporal modelling to better capture \\nthe evolving nature of traffic events.\\nIn addition to tweets, recent research emphasises the integration of text, audio, video, and IoT sensor \\ndata to enhance TED. These modalities complement Twitter/X’s text-based signals by providing real- \\nworld, multimodal evidence of incidents or congestion. Each source contributes distinct strengths:\\n• Text (tweets) offers real-time, crowd-sourced alerts.  \\n• Audio (sirens, honking) reflects ambient traffic conditions.  \\n• Video (CCTV, dashcams) provides visual confirmation of events.  \\n• IoT sensors (GPS, accelerometers, traffic counters) deliver location-specific, quantitative indicators.\\nTo process these diverse inputs, deep learning models such as late fusion CNN-LSTM architectures and \\nattention-based transformers are employed. These models support cross-modal feature learning, enhancing \\ndetection robustness, particularly in noisy or ambiguous textual environments. Transformer-based frameworks, \\nsuch as Multimodal Bi-transformers (MMBT) and Vision-and-Language Transformer (ViLT), are also promis\\xad\\ning for aligning visual, textual, and sensor-based information within a unified pipeline.\\nA practical implementation is demonstrated in Fatichah et al. (2020), who developed a TED system that \\nintegrates Twitter/X text and images. Their approach used CNNs for image analysis and C-LSTM for text \\nprocessing. By multimodal data integration, the system improved detection accuracy compared to text- \\nonly approaches. This shows the potential of multimodal deep learning for enhancing incident detection \\nby leveraging complementary sources of information available on social media platforms. Figure 11\\nexpands on this framework, illustrating a conceptual pipeline that incorporates additional Twitter/X \\ndata such as audio, video, and IoT sensor data alongside Twitter/X text for enhanced TED.\\n4.5 Deep learning models hardware constraints\\nThe need for extensive hardware resources when deploying deep learning models, especially Transformer-based \\narchitectures, is considered a challenge(Burhanuddin 2023; Lyu et al. 2022). For example, achieving inference \\nlatency below 500 ms requires GPU acceleration with at least 16 GB of VRAM. For medium- to large-scale                        \\nFigure 11. Conceptual framework for multimodal data fusion in traffic event detection (TED), integrating Twitter/X text, \\naudio, video, and IoT sensor data. This figure is an expanded version of the framework proposed in Fatichah et al. ( 2020), \\nillustrating how deep learning models can jointly process heterogeneous data streams to improve incident detection \\naccuracy and robustness.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n21'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 22}, page_content='processing, such as handling more than 10,000 tweets/second, TPU integration or distributed GPU clusters are \\nrecommended to maintain throughput (Cao et al. 2025). A number of memory architecture optimisation \\ntechniques were suggested, such as halving the embedding size, parameter pruning, and quantisation (Saai and \\nVijayakumar 2024). In addition, DeepSpeed-Inference offers a multi-GPU and heterogeneous inference solution \\nthat significantly reduces latency and increases throughput (Yazdani et al. 2022). It enables the inference of \\nmodels up to 25 times larger than those supported by GPU-only solutions.\\n4.6 Large language models (LLMs) for TED\\nLLMs, such as ChatGPT and Claude, are not widely adopted in TED research. However, their capabilities \\nin handling informal, multilingual, and ambiguous language show a promising path in future applications. \\nAdditionally, LLMs support zero- and few-shot learning, empowering classification, summarisation, and \\nsentiment analysis with minimal labelled data (Wandelt et al. 2024). Still, challenges such as high \\ncomputational costs, Limited open access to real-time inference, fine-tuning issues on traffic data, and \\nreproducibility issues due to API-based LLMs impede their adaptability in the TED research (Zhang et al. \\n2024). Hybrid systems combining LLMs for preprocessing with lightweight models for detection may offer \\nan effective path forward. As LLMs evolve, their role in multilingual and multimodal TED systems calls for \\nfurther exploration (Mahmud et al. 2025).\\n4.7 Complexity of DL and the need for explainability and interpretability\\nDL models often function as black boxes, making it essential to improve their interpretability and \\ndebugging for reliable predictions. Researchers are developing AI techniques to understand and refine \\nmodel processes. Common approaches include:\\n• Attention visualisation, which highlights key parts of a tweet sequence during prediction to assess \\nelement significance.  \\n• Counterfactual explanations, or ‘what-if’ analysis, examining how changes in specific tweet elements \\naffect predictions.  \\n• SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), \\ntwo model-agnostic interpretability techniques that help explain predictions by attributing them to \\nindividual features.\\n4.8 The constant change of language and behaviour of social media users\\nIncremental learning techniques can help models continuously learn from new data while supporting \\nprevious knowledge and adapting to changing language patterns on social media. Model adaptation \\ninvolves training on historical tweets with labelled data and applying this knowledge to current tweets \\nwhere labelled data is limited, enabling knowledge transfer for improved performance.\\nFuture research should focus on transfer learning by using pre-trained models on large text corpora to \\naddress the limited labelled Twitter/X data challenge and improve model generalisation and performance. \\nMoreover, Deep fusion techniques can also be explored by integrating multiple DL models with different \\narchitectures or data subsets to enhance accuracy and robustness. By addressing these challenges, \\nresearchers can enhance DL models for Twitter/X traffic event detection, resulting in improved traffic \\nmanagement, reduced congestion, and real-time commuter awareness.\\nAuthor contributions\\nCRediT: Danya Qutaishat conducted the literature review, analysed the data, draughted the manuscript, and revised it \\nbased on feedback. Songnian Li supervised the study, contributed to the conceptual design, critically reviewed the \\nmanuscript, and provided intellectual guidance.\\n22\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 23}, page_content='Disclosure statement\\nNo potential conflict of interest was reported by the author(s).\\nFunding\\nThis work was funded by the Natural Sciences and Engineering Research Council of Canada, grant number RGPIN- \\n2017-05950.\\nData availability statement\\nThe data supporting the findings of this study come from public datasets, which are described in Section 2.2. Data \\nsharing is not applicable to this article as no new data were created or analysed in this study.\\nReferences\\nAbad, A., O. Alfonso, T. António, M. Carmen García, M.H. Carlos D, P. Fernando, B. Fernando and M. Nuno, eds. \\n2016. Advances in Speech and Language Technologies for Iberian Languages. In Lecture Notes in Computer \\nScience. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-49169-1.\\nAfyouni, I., Z. A. Aghbari, and R. A. Razack. 2022. “Multi-Feature, Multi-Modal, and Multi-Source Social Event Detection: \\nA Comprehensive Survey.” Information Fusion 79: 279–308. https://doi.org/10.1016/j.inffus.2021.10.013.\\nAminabadi Yazdani, R., S. Rajbhandari, A.A. Awan, C. Li, Du Li, E. Zheng, O. Ruwase, R. Y. Aminabadi, S. Smith, M. \\nZhang, J. Rasley, and Y. He. 2022. DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at \\nUnprecedented Scale. In SC22: International Conference for High Performance Computing, Networking, Storage \\nand Analysis, 1–15. Dallas, TX, USA: IEEE. https://doi.org/10.1109/SC41404.2022.00051\\nAlam, F., F. Ofli, and M. Imran. 2018. “CrisisMMD: Multimodal Twitter Datasets from Natural Disasters.” Proceedings of \\nthe International AAAI Conference on Web and Social Media 12(1), https://doi.org/10.1609/icwsm.v12i1.14983.\\nAli, F., D. Kwak, P. Khan, S. Islam, K. H. Kim, and K. S. Kwak. 2017. “Fuzzy Ontology-Based Sentiment Analysis of \\nTransportation and City Feature Reviews for Safe Traveling.” Transportation Research Part C: Emerging \\nTechnologies 77: 33–48. https://doi.org/10.1016/j.trc.2017.01.014.\\nAli, F., A. Ali, M. Imran, R. Ali Naqvi, M. H. Siddiqi, and K.-S. Kwak. 2021. “Traffic Accident Detection and \\nCondition Analysis Based on Social Networking Data.” Accident Analysis & Prevention 151: 105973. https:// \\ndoi.org/10.1016/j.aap.2021.105973.\\nAli, F., D. Kwak, P. Khan, S. El-Sappagh, A. Ali, S. Ullah, K. H. Kim, and K.-S. Kwak. 2019. “Transportation Sentiment \\nAnalysis Using Word Embedding and Ontology-Based Topic Modeling.” Knowledge-Based Systems 174: 27–42. \\nhttps://doi.org/10.1016/j.knosys.2019.02.033.\\nAlifi, M. R. and S. H. Supangkat. 2018. Information Extraction of Traffic Condition from Social Media Using \\nBidirectional LSTM-CNN. In 2018 International Seminar on Research of Information Technology and Intelligent \\nSystems (ISRITI), 637–640. https://doi.org/10.1109/ISRTI.2018.8864265\\nAlmassar, R. R., and A. S. Girsang. 2022. “Detection of Traffic Congestion Based on Twitter Using Convolutional \\nNeural Network Model.” IAES International Journal of Artificial Intelligence (IJ-AI) 11 (4): 1448–1459. https:// \\ndoi.org/10.11591/ijai.v11.i4.pp1448-1459.\\nAlomari, E., I. Katib, A. Albeshri, T. Yigitcanlar, and R. Mehmood. 2021. “Iktishaf+: A Big Data Tool with Automatic \\nLabeling for Road Traffic Social Sensing and Event Detection Using Distributed Machine Learning.” Sensors 21(9): \\n2993. https://doi.org/10.3390/s21092993.\\nAlomari, E. A., and R. Mehmood. 2023. “Smart Cities, Smarter Roads: A Review of Leveraging Cutting-Edge \\nTechnologies for Intelligent Event Detection from Social Media.” International Journal of Advanced Computer \\nScience and Applications 14(11): 364–374. https://doi.org/10.14569/ijacsa.2023.014113.\\nAlomari, E., R. Mehmood, and I. Katib. 2019. Road Traffic Event Detection Using Twitter Data, Machine Learning, \\nand Apache Spark. In 2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted \\nComputing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and \\nSmart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), 1888–1895. Leicester, UK: IEEE. \\nhttps://doi.org/10.1109/SmartWorld-UIC-ATC-ScalCom-IOP-SCI.2019.00332\\nAmbastha, P. and M. S. Desarkar. 2020. Incident Detection From Social Media Targeting Indian Traffic Scenario \\nUsing Transfer Learning. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems \\n(ITSC), 1–6. Rhodes, Greece: IEEE. https://doi.org/10.1109/ITSC45102.2020.9294295\\nAnda, C., A. Erath, and P. J. Fourie. 2017. “Transport Modelling in the Age of Big Data.” International Journal of \\nUrban Sciences 21(sup1): 19–42. https://doi.org/10.1080/12265934.2017.1281150.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n23'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content=\"Asudani, D., N. K. Suresh, P. Nagwani, and Singh. 2023. “Impact of Word Embedding Models on Text Analytics in \\nDeep Learning Environment: A Review.” Artificial Intelligence Review 56(9): 10345–10425. https://doi.org/ \\n10.1007/s10462-023-10419-1.\\nAtefeh, F., and W. Khreich. 2013. “A Survey of Techniques for Event Detection in Twitter.” Computational \\nIntelligence 31(1): 132–164. https://doi.org/10.1111/coin.12017.\\nAzhar, A., S. Rubab, M. M. Khan, Y. A. Bangash, M. D. Alshehri, F. Illahi, and A. K. Bashir. 2022. “Detection and \\nPrediction of Traffic Accidents Using Deep Learning Techniques.” Cluster Computing 26(1): 477–493. https:// \\ndoi.org/10.1007/s10586-021-03502-1.\\nBilgin, M. and I. F. Şentürk. 2017. Sentiment Analysis on Twitter Data with Semi-Supervised Doc2Vec. In 2017 \\nInternational Conference on Computer Science and Engineering (UBMK), 661–666. Antalya, Turkey: \\nIEEE.https://doi.org/10.1109/UBMK.2017.8093492\\nBlakey, E. 2024. “The Day Data Transparency Died: How Twitter/X Cut Off Access for Social Research.” Contexts \\n23(2): 30–35. https://doi.org/10.1177/15365042241252125.\\nBonandrini, R., and D. Gatti. 2024. “FastText (Sub)Word Vectors, Reference Module in Social Sciences. https:// \\ndoi.org/10.1016/b978-0-323-95504-1.00032-6.\\nBurhanuddin, M. A. 2023. “Efficient Hardware Acceleration Techniques for Deep Learning on Edge Devices: A \\nComprehensive Performance Analysis.” KHWARIZMIA 2023: 103–112. https://doi.org/10.70470/khwarizmia/2023/010.\\nCao, S., S. Liu, T. Griggs, P. Schafhalter, X. Liu, Y. Sheng, J. E. Gonzalez, M. Zaharia, and I. Stoica. 2025. Moe-Lightning: \\nHigh-Throughput MoE Inference on Memory-Constrained GPUs. In Proceedings of the 30th ACM International \\nConference on Architectural Support for Programming Languages and Operating Systems, 715–730.https://doi.org/ \\n10.1145/3669940.3707267\\nChen, Q. and W. Wang. 2019. Multi-Modal Neural Network for Traffic Event Detection. In 2019 IEEE 2nd \\nInternational Conference on Electronics and Communication Engineering (ICECE), 26–30. Xi'an, China: IEEE. \\nhttps://doi.org/10.1109/ICECE48499.2019.9058508\\nChen, P.-T., F. Chen, and Z. Qian. 2014. Road Traffic Congestion Monitoring in Social Media with Hinge-Loss \\nMarkov Random Fields. In 2014 IEEE International Conference on Data Mining.80–89. Shenzhen, China: IEEE. \\nhttps://doi.org/10.1109/ICDM.2014.139\\nChen, Y., Y. Lv, X. Wang, L. Li, and F.-Y. Wang. 2018. Detecting Traffic Information From Social Media Texts With \\nDeep Learning Approaches. In IEEE Transactions on Intelligent Transportation Systems,20: 3049–3058. \\nIEEE.https://doi.org/10.1109/TITS.2018.2871269\\nCottrill, C., P. Gault, G. Yeboah, J. D. Nelson, J. Anable, and T. Budd. 2017. “Tweeting Transit: An Examination of \\nSocial Media Strategies for Transport Information Management During a Large Event.” Transportation Research \\nPart C: Emerging Technologies 77: 421–432. https://doi.org/10.1016/j.trc.2017.02.008.\\nD’Andrea, E., P. Ducange, B. Lazzerini, and F. Marcelloni. 2015. Real-Time Detection of Traffic From Twitter Stream \\nAnalysis In IEEE Transactions on Intelligent Transportation Systems, 16: 2269–2283. IEEE.https://doi.org/ \\n10.1109/TITS.2015.2404431\\nDabiri, S., and K. Heaslip. 2019. “Developing a Twitter-Based Traffic Event Detection Model Using Deep Learning \\nArchitectures.” Expert Systems with Applications 118: 425–439. https://doi.org/10.1016/j.eswa.2018.10.017.\\nDeho, B.O., A. William Agangiba, L. Felix Aryeh, and A. Jeffery Ansah. Sentiment Analysis With Word Embedding. \\nIn 2018 IEEE 7th International Conference on Adaptive Science & Technology (ICAST), 1–4.\\nDhiman, A., and D. Toshniwal. 2020. “An Approximate Model for Event Detection From Twitter Data.” IEEE Access \\n8: 122168–122184. https://doi.org/10.1109/ACCESS.2020.3007004.\\nDinesh, L., B. Kuhaneswaran, and N. Ravikumar. 2023. A Multifaceted Machine Learning Approach to Understand Road \\nAccident Dynamics Using Twitter Data. In Handbook of Research on Advancements of Contactless Technology and \\nService Innovation in Library and Information Science, 247–267. https://doi.org/10.4018/978-1-6684-7693-2.ch013\\nDinga, R., P. Brenda W. J. H, V. Dick J, S. Lianne, and M. Andre F. 2019. “Beyond Accuracy: Measures for Assessing \\nMachine Learning Models, Pitfalls and Guidelines.” bioRxiv, 743138. https://doi.org/10.1101/743138.\\nDoguc, T. B., and A. A. Ahmet. 2023. “Tweet Toplama, Analiz ve Depolama İçin Platform Tasarımı (TweetCASP).” \\nComputer Science, https://doi.org/10.53070/bbd.1344271.\\nEffrosynidis, D., G. Sylaios, and A. Arampatzis. 2024. “The Effect of Training Data Size on Disaster Classification \\nFrom Twitter.” Information 15(7): 393. https://doi.org/10.3390/info15070393.\\nElSahly, O., and A. Abdelfatah. 2023. “An Incident Detection Model Using Random Forest Classifier.” Smart Cities \\n6(4): 1786–1813. https://doi.org/10.3390/smartcities6040083.\\nEs Swidi, A., S. Ardchir, A. Daif, and M. Azouazi. 2023. “Road Users Detection for Traffic Congestion Classification.” \\nMathematical Modeling and Computing 10(2): 518–523. https://doi.org/10.23939/mmc2023.02.518.\\nFatichah, C., P. D. Sammy Wiyadi, D. Adni Navastara, N. Suciati, and A. Munif. 2020. Incident Detection Based on \\nMultimodal Data From Social Media Using Deep Learning Methods. In 2020 International Conference on ICT for \\nSmart Society (ICISS), 1–6. Bandung, Indonesia: IEEE. https://doi.org/10.1109/ICISS50791.2020.9307555\\nGannina Kumar, A., A. A. Ram, T. M. Jaffarullah, S. M. Reddy, A. S. Subba Reddy, S. Vikas, V. Mathi, and \\nRamalingam. 2024. “A New Approach to Road Incident Detection Leveraging Live Traffic Data: An Empirical \\nInvestigation.” Procedia Computer Science 235: 2288–2296. https://doi.org/10.1016/j.procs.2024.04.217.\\n24\\nD. QUTAISHAT AND S. LI\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='Garg, M., and M. Kumar. 2016. “Review on Event Detection Techniques in Social Multimedia.” Online Information \\nReview 40(3): 347–361. https://doi.org/10.1108/OIR-08-2015-0281.\\nGu, Y., Z. (S. Qian, and F. Chen. 2016. “From Twitter to Detector: Real-Time Traffic Incident Detection Using Social Media \\nData.” Transportation Research Part C: Emerging Technologies 67: 321–342. https://doi.org/10.1016/j.trc.2016.02.011.\\nGu, W., A. Tandon, Y.-Y. Ahn, and F. Radicchi. 2021. “Principled Approach to the Selection of the Embedding \\nDimension of Networks.” Nature Communications 12(1): 3772. https://doi.org/10.1038/s41467-021-23795-5.\\nHall, F. L., Y. Shi, and G. Atala. 1993. “On-Line Testing of the McMaster Incident Detection Algorithm Under \\nRecurrent Congestion.” Transportation Research Record 1394: 1–7.\\nHasan, S., and S. V. Ukkusuri. 2014. “Urban Activity Pattern Classification Using Topic Models From Online Geo-Location \\nData.” Transportation Research Part C: Emerging Technologies 44: 363–381. https://doi.org/10.1016/j.trc.2014.04.003.\\nHasnat, M. M., and S. Hasan. 2018. “Identifying Tourists and Analyzing Spatial Patterns of Their Destinations From \\nLocation-Based Social Media Data.” Transportation Research Part C: Emerging Technologies 96: 38–54. https:// \\ndoi.org/10.1016/j.trc.2018.09.006.\\nHasnat, M., A. Mehedi, N. Faghih-Imani, S. Eluru, and Hasan. 2019. “Destination Choice Modeling Using Location- \\nBased Social Media Data.” Journal of Choice Modelling 31: 22–34. https://doi.org/10.1016/j.jocm.2019.03.002.\\nHuang, A., L. Gallegos, and K. Lerman. 2017. “Travel Analytics: Understanding How Destination Choice and Business \\nClusters Are Connected Based on Social Media Data.” Transportation Research Part C: Emerging Technologies 77: \\n245–256. https://doi.org/10.1016/j.trc.2016.12.019.\\nHussain, N. Y. 2024. “Deep Learning Architectures Enabling Sophisticated Feature Extraction and Representation for \\nComplex Data Analysis.” International Journal of Innovative Science and Research Technology (IJISRT) 9(10): \\n2290–2300. https://doi.org/10.38124/ijisrt/ijisrt24oct1521.\\nImran, M., C. Castillo, F. Diaz, and S. Vieweg. 2015. “Processing Social Media Messages in Mass Emergency.” ACM \\nComputing Surveys 47(4): 1–38. https://doi.org/10.1145/2771588.\\nJain, E., Neeraja, J., Banerjee, B., and Ghosh, P. 2022. A Diagnostic Approach to Assess the Quality of Data Splitting in \\nMachine LearningarXiv preprint arXiv:2206.11721. https://doi.org/10.48550/arXiv.2206.11721\\nJain, S., R. Pankaj, Z. Sharma, and Fatima. 2023. Traffic Rule Violation and Accident Detection Using CNN. In \\nInternational Conference on Innovative Computing and Communications, 867–878.https://doi.org/10.1007/978- \\n981-99-4071-4_66\\nJiang, H., and H. Deng. 2020. “Traffic Incident Detection Method Based on Factor Analysis and Weighted Random \\nForest.” IEEE Access 8: 168394–168404. https://doi.org/10.1109/ACCESS.2020.3023961.\\nJiang, J., C. Zhang, L. Ke, N. Hayes, Y. Zhu, H. Qiu, B. Zhang, T. Zhou, and G.-W. Wei. 2025. “A Review of Machine \\nLearning Methods for Imbalanced Data Challenges in Chemistry.” Chemical Science 16(18): 7637–7658. https:// \\ndoi.org/10.1039/D5SC00270B.\\nJohnson, J. M., and T. M. Khoshgoftaar. 2019. “Survey on Deep Learning With Class Imbalance.” Journal of Big Data \\n6(1): 27. https://doi.org/10.1186/s40537-019-0192-5.\\nJonnalagadda, J. and M. Hashemi. 2021. A Deep Learning-Based Traffic Event Detection From Social Media. In 2021 \\nIEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI), 1–8. Las \\nVegas, NV, USA: IEEE. https://doi.org/10.1109/IRI51335.2021.00007\\nKamkarhaghighi, M., and M. Makrehchi. 2017. “Content Tree Word Embedding for Document Representation.” \\nExpert Systems with Applications 90: 241–249. https://doi.org/10.1016/j.eswa.2017.08.021.\\nKarthik, B., D. Sai, D. NSSVV, M. A. Bharat, B. T. Alam, M. Sai, S. Rana, S. A. Sharma, and Yadav. 2023. Sentimental \\nTechnique Implementation on Textual Data. In 2023 6th International Conference on Contemporary Computing \\nand Informatics (IC3I). 82–86, Gautam Buddha Nagar, India: IEEE. https://doi.org/10.1109/IC3I59117.2023.10397771\\nKim, Y., S. Song, H. Lee, D. Choi, J. Lim, K. Bok, and J. Yoo. 2023. “Regional Traffic Event Detection Using Data \\nCrowdsourcing.” Applied Sciences 13(16): 9422. https://doi.org/10.3390/app13169422.\\nKisters, P., T. Bauer, W. Posdorfer, and J. Edinger. 2023. Real-Time Traffic Congestion Detection for Driver-Centric \\nApplications. In 2023 IEEE 43rd International Conference on Distributed Computing Systems Workshops \\n(ICDCSW), 163–168. Hong Kong, Hong Kong: IEEE. https://doi.org/10.1109/ICDCSW60045.2023.00029\\nLee, S., S. Lee, J. Noh, J. Kim, and H. Jeong. 2023. “Special Traffic Event Detection: Framework, Dataset Generation, and \\nDeep Neural Network Perspectives.” Sensors (Basel, Switzerland) 23(19): 8129. https://doi.org/10.3390/s23198129.\\nLi, L., Y. Dou, and J. Zhou. 2023. Traffic Accident Detection Based on Multimodal Knowledge Graphs. In 2023 5th \\nInternational Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI),  \\n644–647. \\nHangzhou, China: IEEE.https://doi.org/10.1109/RICAI60863.2023.10489808\\nLi, R., K. H. Lei, R. Khadiwala, and K. C.-C. Chang. 2012. TEDAS: A Twitter-Based Event Detection and Analysis \\nSystem In 2012 IEEE 28th International Conference on Data Engineering (ICDE), 1273–1276. Arlington, VA, \\nUSA: IEEE.https://doi.org/10.1109/ICDE.2012.125\\nLi, Y., L. Li, H. Jing, B. Ran, and D. Sun. 2020. “Examining imbalanced classification algorithms in predicting real- \\ntime traffic crash risk.” Accident Analysis & Prevention 144: 105610. https://doi.org/10.1016/j.aap.2020.105610.\\nLin, L., M. Ni, Q. He, J. Gao, and A. W. Sadek. 2015. “Modeling the Impacts of Inclement Weather on Freeway Traffic \\nSpeed: Exploratory Study With Social Media Data.” Transportation Research Record: Journal of the \\nTransportation Research Board 2482(1): 82–89. https://doi.org/10.3141/2482-11.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n25'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='Lin, Yi., L. Li, H. Jing, B. Ran, and D. Sun. 2020. “Automated Traffic Incident Detection with a Smaller Dataset Based \\non Generative Adversarial Networks.” Accident Analysis & Prevention 144: 105610.  https://doi.org/10.1016/ \\nj.aap.2020.105610.\\nLiu, J., T. Li, P. Xie, S. Du, F. Teng, and X. Yang. 2020. “Urban Big Data Fusion Based on Deep Learning: An \\nOverview.” Information Fusion 53: 123–133. https://doi.org/10.1016/j.inffus.2019.06.016.\\nLiu, H., M. Sheng, Z. Sun, Y. Yao, X.-S. Hua, and H.-T. Shen. 2024. “Learning With Imbalanced Noisy Data by \\nPreventing Bias in Sample Selection.” IEEE Transactions on Multimedia 26: 7426–7437. https://doi.org/10.1109/ \\nTMM.2024.3368910.\\nLiu, Y., X. Shen, Y. Zhang, Z. Wang, Y. Tian, J. Dai, and Y. Cao. 2024. A Systematic Review of Machine Learning \\nApproaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases.arXiv preprint \\narXiv:2403.20293. https://doi.org/10.48550/arXiv.2403.20293\\nLu, H., K. Shi, Y. Zhu, Y. Lv, and Z. Niu. 2018. “Sensing Urban Transportation Events from Multi-Channel Social Signals \\nwith the Word2vec Fusion Model.” Sensors (Basel, Switzerland) 18(12): 4093. https://doi.org/10.3390/s18124093.\\nMikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. “Distributed Representations of Words and \\nPhrases and Their Compositionality.” Advances in Neural Information Processing Systems 26: 3111–3119. Red \\nHook, NY: Curran Associates. \\nLyu, B., H. Yuan, L. Lu, and Y. Zhang. 2022. “Resource-Constrained Neural Architecture Search on Edge Devices.” IEEE \\nTransactions on Network Science and Engineering 9(1): 134–142. https://doi.org/10.1109/TNSE.2021.3054583.\\nMaghrebi, M., A. Abbasi, and S. Waller. 2016. Transportation Application of Social Media: Travel Mode Extraction. \\nIn 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), 1648–1653. Rio de \\nJaneiro, Brazil: IEEE. https://doi.org/10.1109/ITSC.2016.7795779\\nMahmud, D., H. Hajmohamed, S. Almentheri, S. Alqaydi, L. Aldhaheri, R. A. Khalil, and N. Saeed. 2025. “Integrating \\nLLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions.” IEEE Transactions on Intelligent \\nTransportation Systems 26(5): 5674–5709. https://doi.org/10.1109/TITS.2025.3528116.\\nMannes, J. 2017. Facebook’s fastText Library Is Now Optimized for MobileTechCrunch https://TechCrunch.com/201 \\n7/05/02/facebooks-fasttext-library-is-now-optimized-for-mobile/\\nMelhem, W., A. Abdi, and F. Meziane. 2024. “Traffic Detection and Forecasting from Social Media Data Using a Deep \\nLearning-Based Model, Linguistic Knowledge, Large Language Models, and Knowledge Graphs.” Proceedings of the \\n16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge \\nManagement. 235–242. https://doi.org/10.5220/0013066900003838.\\nMikolov, T., Quoc V, L., and Ilya, S. 2013. Exploiting Similarities Among Languages for Machine Translation.” arXiv \\npreprint arXiv:1309.4168. https://doi.org/10.48550/arXiv.1309.4168\\nMolloy, J., and R. Moeckel. 2017. “Improving Destination Choice Modeling Using Location-Based Big Data.” ISPRS \\nInternational Journal of Geo-Information 6(9): 291. https://doi.org/10.3390/ijgi6090291.\\nMredula, M.S., C. Noyon, S.R. Md, M. Imtiaz, and C. You-Ze. 2022. “A Review on the Trends in Event Detection by \\nAnalyzing Social Media Platforms’ Data.” Sensors (Basel, Switzerland) 22(12): 4531. https://doi.org/10.3390/s22124531.\\nMünz, G., L. Sa, and C. Georg. 2007. “Traffic Anomaly Detection Using K-Means Clustering.” GI/ITG Workshop \\nMMBnet 7 (9). https://www.net.in.tum.de/projects/dfg-lupus/files/muenz07k-means.pdf.\\nMurtfeldt, R., Naomi, A., Ihsan, K., and Jevin D, W. 2024. RIP Twitter API: A Eulogy to Its Vast Research \\nContributions.” arXiv preprint arXiv:2404.07340 https://doi.org/10.48550/arXiv.2404.07340\\nNama, N., M. Sampson, N. Barrowman, R. Sandarage, K. Menon, G. Macartney, K. Murto, et al.2019. “Crowdsourcing \\nthe Citation Screening Process for Systematic Reviews: Validation Study.” Journal of Medical Internet Research \\n21(4): e12953. https://doi.org/10.2196/12953.\\nNejjari, F., L. Benhlima, and S. Bah. 2016. Event Traffic Detection Using Heterogeneous Wireless Sensors Network. In \\n2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA), 1–6. https:// \\ndoi.org/10.1109/AICCSA.2016.7945825\\nNeruda, G. A. and E. Winarko. 2021. Traffic Event Detection From Twitter Using a Combination of CNN and BERT. \\nIn 2021 International Conference on Advanced Computer Science and Information Systems (ICACSIS), 1–7. \\nhttps://doi.org/10.1109/ICACSIS53237.2021.9631334\\nNi, M., Q. He, and J. Gao. 2017. “Forecasting the Subway Passenger Flow Under Event Occurrences With Social \\nMedia.” IEEE Transactions on Intelligent Transportation Systems 18(6): 1623–1632. https://doi.org/10.1109/ \\nTITS.2016.2611644.\\nNirbhaya, M. A. W. and L. H. Suadaa. 2023. Traffic Incident Detection in Jakarta on Twitter Texts Using a Multi- \\nLabel Classification Approach. In 2023 International Conference on Computer, Control, Informatics and Its \\nApplications (IC3INA), 290–295. https://doi.org/10.1109/IC3INA60834.2023.10285731\\nNoori, M. A. R., and R. Mehra. 2020. “Traffic Congestion Detection from Twitter Using Word2vec.” In ICT Analysis \\nand Applications, 527–534. https://doi.org/10.1007/978-981-15-8354-4_52.\\nOlteanu, A., S. Vieweg, and C. Castillo. 2015. What to Expect When the Unexpected Happens: Social Media \\nCommunications Across Crises. In Proceedings of the 18th ACM Conference on Computer Supported \\nCooperative Work & Social Computing, 994–1009. https://doi.org/10.1145/2675133.2675242\\n26\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='Page, M. J., J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C. Hoffmann, C. D. Mulrow, L. Shamseer, et al. 2021. “The \\nPRISMA 2020 Statement: An Updated Guideline for Reporting Systematic Reviews.” International Journal of \\nSurgery 88: 105906. https://doi.org/10.1016/j.ijsu.2021.105906.\\nPage, M. J., J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C. Hoffmann, C. D. Mulrow, L. Shamseer, J. M. Tetzlaff, and \\nM. David. 2020. “Updating Guidance for Reporting Systematic Reviews: Development of the PRISMA 2020 \\nStatement.” OSF Preprints, https://doi.org/10.31222/osf.io/jb4dx.\\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe: Global Vectors for Word Representation In Proceedings \\nof the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. https:// \\ndoi.org/10.3115/v1/D14-1162\\nPita, M. and G. L. Pappa. 2018. Strategies for Short Text Representation in the Word Vector Space. In 2018 7th \\nBrazilian Conference on Intelligent Systems (BRACIS). 266–271. https://doi.org/10.1109/BRACIS.2018.00053\\nplatforms’ data.\" 22(12): 4531.\\nPoudel, A., and T. Weninger. 2024. “Navigating the Post-API Dilemma.” Proceedings of the ACM Web Conference \\n2024, 2476–2484. https://doi.org/10.1145/3589334.3645503.\\nPuangnak, K., and N. Rachsiriwatcharabul. 2022. “Collection of Road Traffic Incidents in Bangkok from Twitter Data \\nBased on Deep Learning Algorithm.” ECTI Transactions on Computer and Information Technology (ECTI-CIT \\n16(3): 268–277. https://doi.org/10.37936/ecticit.2022163.248535.\\nQazi, U., M. Imran, and F. Ofli. 2020. “GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19 \\nTweets With Location Information.” SIGSPATIAL Special 12(1): 6–15. https://doi.org/10.1145/3404820.3404823.\\nQiu, J. 2024. “An Analysis of Model Evaluation With Cross-Validation: Techniques, Applications, and Recent \\nAdvances.” Advances in Economics, Management and Political Sciences 99(1): 69–72. https://doi.org/10.54254/ \\n2754-1169/99/2024ox0213.\\nQutaishat, D., and S. Li. 2025a. “A Transformer-Based Multi-Feature Fusion Method for Detecting Traffic Events \\nUsing Twitter Data.” Big Earth Data. 1–33. https://doi.org/10.1080/20964471.2025.2564525. October.\\nQutaishat, D., and S. Li. 2025b. “Multimodal Spatiotemporal Deep Fusion for Highway Traffic Accident Prediction in \\nToronto: A Case Study and Roadmap.” ISPRS International Journal of Geo-Information 14(no. 11): 434. https:// \\ndoi.org/10.3390/ijgi14110434.\\nRajaraman, A. and J. D. Ullman. 2011. Data Mining. In In Mining of Massive Datasets. 1–17. Cambridge University \\nPress.https://doi.org/10.1017/CBO9781139058452.002\\nRamadhani, A. M. and H. S. Goo. 2017. Twitter Sentiment Analysis Using Deep Learning Methods. In 2017 7th \\nInternational Annual Engineering Seminar (InAES),1–4. Yogyakarta, Indonesia: IEEE. https://doi.org/10.1109/ \\nINAES.2017.8068556\\nRaschka, S. 2018. Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. arXiv preprint \\narXiv:1811.12808. https://doi.org/10.48550/arXiv.1811.12808\\nRaunak, V. 2017. Simple and Effective Dimensionality Reduction for Word Embeddings. arXiv preprint \\narXiv:1708.03629. https://doi.org/10.48550/arXiv.1708.03629\\nRezaeinia, S. M., Ghodsi, A., and Rouhollah, R. 2017. Improving the Accuracy of Pre-Trained Word Embeddings for \\nSentiment Analysis.” arXiv preprint arXiv:1711.08609 https://doi.org/10.48550/arXiv.1711.08609\\nRudkowsky, E., M. Haselmayer, M. Wastian, M. Jenny, &A. Emrich, and M. Sedlmair. 2018. “More Than Bags of \\nWords: Sentiment Analysis With Word Embeddings.” Communication Methods and Measures 12(2–3): 140–157. \\nhttps://doi.org/10.1080/19312458.2018.1455817.\\nSaai, K. P., and Vijayakumar, V. 2024. Resource-Efficient Transformer Architecture: Optimizing Memory and Execution \\nTime for Real-Time Applications.” arXiv preprint arXiv:2501.00042 https://doi.org/10.48550/arXiv.2501.00042\\nSaeed, Z., R. A. Abbasi, O. Maqbool, A. Sadaf, I. Razzak, A. Daud, N. R. Aljohani, and G. Xu. 2019. “What’s \\nHappening Around the World? A Survey and Framework on Event Detection Techniques on Twitter.” Journal of \\nGrid Computing 17(2): 279–312. https://doi.org/10.1007/s10723-019-09482-2.\\nSaeedi, R., M. Sankaranarayanasamy, R. Vishwakarma, P. Singh, and R. Vennelakanti. 2020. Towards Modular \\nModeling and Analytic for Multi-Modal Transportation Networks. In 2020 IEEE International Conference on Big \\nData (Big Data), 2426–2432. Atlanta, GA, USA: IEEE. https://doi.org/10.1109/BigData50022.2020.9377806\\nSafitri, A., I. Sekar, S. Wijayanto, and Hadiyoso. 2024. Improving Classification Accuracy with Preprocessing \\nTechniques for Sentiment Analysis. In 2024 International Conference on Data Science and Its Applications \\n(ICoDSA), 487–490. Kuta, Bali, Indonesia: IEEE. https://doi.org/10.1109/ICoDSA62899.2024.10651657\\nSahni, A., and N. Raja. 2018. “Analyzation and Detection of Cyberbullying: A Twitter-Based Indian Case Study.” In \\nData Science and Analytics, 484–497. https://doi.org/10.1007/978-981-10-8527-7_41.\\nSamant, A., and H. Adeli. 2000. “Feature Extraction for Traffic Incident Detection Using Wavelet Transform and \\nLinear Discriminant Analysis.” Computer‐Aided Civil and Infrastructure Engineering 15(4): 241–250. https:// \\ndoi.org/10.1111/0885-9507.00188.\\nSampath, K. K., and M. Supriya. 2023. “Traffic Prediction in Indian Cities from Twitter Data Using Deep Learning \\nand Word Embedding Models.” In Multi-Disciplinary Trends in Artificial Intelligence, 671–682. https://doi.org/ \\n10.1007/978-3-031-36402-0_62.\\nSavvides, R., and J. Mäkelä Kai %J Statistical Analysis Puolamäki, and Data Mining: The ASA Data Science Journal \\n2023. “Model selection with bootstrap validation.” 16 (2): 162–186. https://doi.org/10.1002/sam.11606.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n27'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 28}, page_content='Sethi, V., N. Bhandari, F. S. Koppelman, and J. L. Schofer. 1995. “Arterial Incident Detection Using Fixed Detector \\nand Probe Vehicle Data.” Transportation Research Part C: Emerging Technologies 3(no. 2): 99–112. https:// \\ndoi.org/10.1016/0968-090X(94)00017-Y. April.\\nSethurajan, M. R., and N. K. 2023. “An Adept Approach to Ascertain and Elude Probable Social Bots Attacks on \\nTwitter and Twitch Employing Machine Learning Approach.” MethodsX 11 (December), 102430. https://doi.org/ \\n10.1016/j.mex.2023.102430.\\nSuat-Rojas, N., C. Gutierrez-Osorio, and C. Pedraza. 2022. “Extraction and Analysis of Social Networks Data to Detect \\nTraffic Accidents.” Information 13(no. 1): 26. https://doi.org/10.3390/info13010026.\\nTorregrossa, F., R. Allesiardo, V. Claveau, N. Kooli, and G. Gravier. 2021. “A Survey on Training and Evaluation of \\nWord Embeddings.” International Journal of Data Science and Analytics 11(no. 2): 85–103. https://doi.org/ \\n10.1007/s41060-021-00242-8.\\nTsou, M.-H., Zhang, H., and Jung, C.-T. 2017. Identifying Data Noises, User BiasesJung, Ming-Hsiang, and System \\nErrors in Geo-Tagged Twitter Messages (Tweets).” arXiv preprint arXiv:1712.02433 https://doi.org/10.48550/ \\narXiv.1712.02433\\nWalsh, R., and M. Tardy. 2022. “A Comparison of Techniques for Class Imbalance in Deep Learning Classification of \\nBreast Cancer.” Diagnostics 13(no. 1): 67. https://doi.org/10.3390/diagnostics13010067.\\nWandelt, S., C. Zheng, S. Wang, Y. Liu, and X. Sun. 2024. “Large Language Models for Intelligent Transportation: A Review \\nof the State of the Art and Challenges.” Applied Sciences 14(no. 17): 7455. https://doi.org/10.3390/app14177455.\\nWilson, S., W. Magdy, B. McGillivray, K. Garimella, and G. Tyson. 2020. Urban Dictionary Embeddings for Slang \\nNLP Applications. In Proceedings of the 12th Language Resources and Evaluation Conference, 4764–4773. \\nEuropean Language Resources Association (ELR). https://aclanthology.org/2020.lrec-1.586\\nXu, S., S. Li, and R. Wen. 2018. “Sensing and Detecting Traffic Events Using Geosocial Media Data: A Review.” Computers, \\nEnvironment and Urban Systems 72(November): 146–160. https://doi.org/10.1016/j.compenvurbsys.2018.06.006.\\nYang, S. U. 2022. Anomaly Traffic Detection Based on LSTM. In 2022 IEEE 10th Joint International Information \\nTechnology and Artificial Intelligence Conference (ITAIC), 667–670. Chongqing, China: IEEE, June 17. https:// \\ndoi.org/10.1109/itaic54216.2022.9836629\\nZhang, Z., Q. He, J. Gao, and M. Ni. 2018. “A Deep Learning Approach for Detecting Traffic Accidents from Social Media \\nData.” Transportation Research Part C: Emerging Technologies 86: 580–596. https://doi.org/10.1016/j.trc.2017.11.027.\\nZhang, S., D. Fu, W. Liang, Z. Zhang, B. Yu, P. Cai, and B. Yao. 2024. “TrafficGPT: Viewing, Processing and Interacting with \\nTraffic Foundation Models.” Transport Policy 150: 95–105. https://doi.org/10.1016/j.tranpol.2024.07.004.\\n28\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='7658\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nDGCN-TES: Dynamic GCN-Based Multitask Model\\nWith Temporal Event Sharing for Rumor Detection\\nShuzhen Wan\\n, Guanghao Yang, Fangmin Dong, and Mengyuan Wang\\nAbstract—The rumor detection task aims to identify unofﬁ-\\ncial and unconﬁrmed information that is spreading on social\\nmedia. At any given moment, different users express their\\nopinions, focusing on some propagation events, and the posts\\nthey make gradually form a social network that expands as\\nit grows. Over time, nodes and edges form a dynamic graph\\nthat presents different states at different moments. However,\\nmost existing research focuses more on the text content, social\\ncontext, propagation mode, etc., and they ignore the factors\\nfrom many aspects and do not consider the dynamic rela-\\ntionships implied in the propagation development of social\\nmedia. To analyze these dynamic properties, this article proposes\\na dynamic network-based multitask rumor detection method\\ncalled dynamic GCN-based multitask model with temporal\\nevent sharing for rumor detection (DGCN-TES). This method\\ncan effectively capture the dynamic patterns of relationships\\nin propagation events and change them over time to detect\\nrumors. It is mainly divided into three modules: 1) dynamic-\\ngraph convolutional network (GCN) module, which uses dynamic\\ngraph neural network to construct the propagation graph of\\nrumor events at different times, which can better capture the\\ndynamic spatial features that change over time; 2) content-long\\nshort-term memory (LSTM), which uses the LSTM network as\\na benchmark model and has been improved to better capture\\ntime-series text features over time and for multitask shared\\ninteractions; and 3) temporal event sharing layer is the sharing\\nlayer, which uses time step as the basic unit of sharing, and\\nrealizes the sharing interaction between dynamic structural\\nfeatures and temporal text features between the ﬁrst two modules.\\nWe tested the method on two real-world rumor detection datasets\\nPHEME and WEIBO, and the ﬁnal results show that the method\\nimproved F1-score by more than 2.63% and 3.91% compared to\\nthe other best baselines baseline.\\nIndex Terms—Dynamic graph neural network, multitask, tem-\\nporal event sharing, time step.\\nI. INTRODUCTION\\nI\\nN the ﬁeld of social psychology, rumor refers to the circula-\\ntion of information that has not been ofﬁcially conﬁrmed\\nor news that is deliberately fabricated by human beings [1].\\nRecently, the Internet and social media platforms have been\\nManuscript received 27 August 2023; revised 16 April 2024 and 12 July\\n2024; accepted 31 July 2024. Date of publication 5 September 2024; date\\nof current version 3 December 2024. This work was supported in part by\\nthe National Nature Science Foundation of China under Grant U1703261 and\\nGrant 61871258. (Corresponding author: Shuzhen Wan.)\\nThe authors are with Hubei Key Laboratory of Intelligent Vision-\\nBased Monitoring for Hydroelectric Engineering, Yichang Key Laboratory\\nof Intelligent Medicine, and the College of Computer and Information\\nTechnology, China Three Gorges University, Yichang 443002, Hubei, China\\n(e-mail:\\nwanshuzhen@163.com;\\nguanghao_y@163.com;\\nfmdong@ctgu.\\nedu.cn; Meng_yuan_Wang@163.com).\\nDigital Object Identiﬁer 10.1109/TCSS.2024.3443275\\nthe main channels through which people obtain news, but the\\nlack of censorship brings many opportunities for rumors to\\nspread. For example, news about COVID-191 [2] is spreading\\nrapidly (e.g., some self-media claim that drinking bleach can\\ncure this disease) and is recognized as a rumor by the World\\nHealth Organization.2 This kind of social opinion is very dan-\\ngerous, and some rumors can cause social panic and serious\\nconsequences if they are not identiﬁed in time. Therefore, cor-\\nrectly distinguishing social media rumor events, maintaining\\npublic order of social opinion, strengthening supervision, and\\neffectively screening rumor events are urgent tasks for today’s\\nsocial networks. With the development of artiﬁcial intelligence\\ntechnology, deep learning has been widely applied to the ﬁeld\\nof rumor detection. Recent research [3], [4], [5] shows that by\\nmodeling the mode of spread of rumor events as a spreading tree\\nor spread diagram, similarities in structure can be used to dis-\\ntinguish rumor and nonrumor. For example, by modeling news\\nsamples into graphs and sentence classiﬁcation problems into\\ngraph classiﬁcation problems [6], these methods demonstrate\\nthe point that static spatial structures can be used for rumor\\ndetection. The reason that this idea can detect rumors well is\\nits ability to learn rumor event propagation patterns, which is\\nnot possible with methods related to textual content based [7].\\nHowever, this method of constructing communication patterns\\nis static, and it can only reﬂect the communication patterns of\\nrumor events at the late stage of development or when they have\\nalready ended, whereas the evolution of rumor events in the real\\nworld develops dynamically over time, and this method fails\\nto capture the dynamic characteristics of rumor events in their\\nconstant changes over time.\\nDynamic graph neural networks can be a good solution to the\\nabove problems. Some detection methods based on dynamic\\nideas have been proposed [8], [9], [10], which have better\\nperformance by simulating the propagation process of news\\nevents in the real world and capturing more delicate features\\nthat change over time. A common approach [10] tends to be\\nto construct static graphs of the same propagation event, at\\ndifferent time stages, to form a set of static graph sequences\\nthat are continuous in time. The performance of this approach\\nis limited by the number of time steps, and if the number of time\\nsteps is set too small, a better dynamic effect cannot be realized.\\nTherefore, this article tries to construct a ﬁne-grained dynamic\\n1https://thebulletin.org/2020/01/fake-news-epidemic-coronavirus-breeds-\\nhate-and-disinformation-in-india-and-beyond/\\n2https://m.facebook.com/WHO/photos/fact-drinking-methanol-ethanol-or-\\nbleach-does-not-prevent-or-cure-covid-19-and-c/3040991822612847/\\n2329-924X © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7659\\ngraph neural network based on time steps, whose number of\\ntime steps corresponds to the number of postpropagation of\\nrumor events, to capture a set of transition-dedicated time-\\nbased dynamic structural features, and such dynamic structural\\nfeatures have a certain relationship with the time sequence of\\nrecurrent neural networks in the time dimension.\\nRecurrent neural networks, as a common approach to solving\\nrumor detection, are good at handling sequential data, taking\\ninto account the previous inputs when calculating each time\\nstep, and can capture and utilize the contextual information in\\nsequential data well. Therefore, it has been widely utilized in\\nrecent years. For example, Ma et al. [11] used the recurrent neu-\\nral network (RNN)-based approach to capture contextual infor-\\nmation of related posts in microblog events over time. Ahmad\\net al. [12] constructed bidirectional long short-term memory\\n(LSTM)-RNN networks for rumor prediction by learning social\\nand content-based features. All of these methods treat textual\\ncontent as sequence data, and there are contextual and temporal\\nrelationships between different texts of a sequence, a feature\\nsimilar to the propagation process of a dynamic graph network,\\ni.e., neighboring contextual tweets correspond to nodes where\\nthe dynamic graph is being propagated in neighboring time\\nsteps. If it were possible to achieve a ﬁne-grained dynamic\\ngraph network based on time steps, then there would be a certain\\nrelationship between the sequence of diffusion of preceding and\\nsubsequent nodes in the dynamic graph, and the time sequence\\norder between the upward and downward propagation of texts in\\nthe LSTM network. They would be able to capture the dynamic\\nstructural features of time steps and the temporal sequential\\ntext features, respectively. The above-mentioned detection ap-\\nproaches on sequential data rely only on a single feature such\\nas content, but current social networks are multifaceted and\\nchangeable, and they do not take into account features such\\nas relationships, behaviors, and communication structures in\\nrumor propagation events.\\nFrom the above processing of dynamic graphs as well as\\nLSTM, we are inspired by the fact that it is possible to combine\\nthe advantages of both from a dynamic point of view and\\ncotrain them to learn a representation that combines dynamic\\nstructural features and temporal textual features. Meanwhile,\\nthe multitask learning approach can combine the features of\\nmultiple tasks well, and related works [13], [14], [15], [16] often\\ndeﬁne the LSTM layer as a shared network to combine other\\nnetworks, they all use hardware parameter sharing to interact\\nwith features among other tasks, and although this approach\\nachieves parameter sharing to some extent and can improve\\nmodel performance, rich feature sharing among multiple tasks\\nis neglected. Therefore, we choose the LSTM network as a\\nshared layer to combine dynamic graph neural network and text-\\nbased LSTM network through multitasking, and the advantage\\nis that we take the time step as the basic computational unit, and\\nduring the training, multiple dynamic modules start exchanging\\nand learning each other’s features at the same time step, to better\\nimprove prediction results.\\nIn this article, we simulate the propagation process of news\\nevents in the real world to capture its dynamic features in a\\nmore suitable way, and we design a better sharing way to train\\nthe model to eventually distinguish rumors and nonrumors.\\nThe dynamic graph convolutional network (GCN)-based mul-\\ntitask model with temporal event sharing for rumor detection is\\nproposed. It constructs rumor event propagation graphs corre-\\nsponding to various temporal stages and captures the evolving\\ndynamic structural features of rumor events by dynamic graph\\nneural networks. Simultaneously, the textual content features\\nare captured by LSTM networks. To integrate the two networks,\\nwe designed a novel sharing layer based on temporal steps.\\nFinally, an attention mechanism is employed to update output\\nfeatures for rumor detection.\\nThe contributions of this article are as follows.\\n1) A dynamic network-based multitask detection model\\n(DGCN-TES) is proposed, which integrates dynamic\\ngraph neural network and LSTM in a multitask method,\\nand learns information that combines dynamic structural\\nfeatures and temporal textual features, which effectively\\nimproves the performance of the model.\\n2) We propose a ﬁne-grained dynamic graph neural network\\nbased on time steps, to capture dynamic structural fea-\\ntures of rumor events with more delicate transitions to\\nincrease the efﬁciency of multitask sharing and ultimately\\nimprove the detection results.\\n3) A temporal event-based sharing method (TES) is pro-\\nposed for multitasking interaction. Compared with the\\nprevious interaction methods, TES exerts better sharing\\nperformance, which interacts with the information of\\nmultitasks at each time step and effectively improves the\\nprediction.\\n4) We conducted a series of experiments on two real-world\\ndatasets, and the experimental results demonstrate that\\nour proposed dynamic GCN-based multitask model with\\ntemporal event sharing for rumor detection (DGCN-TES)\\nis effective and outperforms other approaches.\\nII. RELATED WORK\\nIn this section, the work related to the proposed model is\\nreviewed, and this research focuses on the following topics: text\\ncontent-based detection methods, propagation structure-based\\ndetection methods, and multitask-based detection methods.\\nA. Text Content-Based Detection Methods\\nIn the event of rumor spreading, it is often dependent on\\nspeciﬁc forms of social media, such as text, images, and video\\nscreens. These media-speciﬁc forms mainly include textual fea-\\ntures [7] and visual features. This literature mainly discusses\\nthe description of textual content from the linguistic level, such\\nas sentences, vocabulary, and semantics. After these contents\\nare cleaned, deduplicated, and other operations, the potential\\ntext features can be obtained using text embedding techniques\\nto represent sentences as vectors, which are used as inputs to\\nneural networks. For example, Singh et al. [16] proposed an\\nattention-based LSTM network that uses LSTM to model tex-\\ntual content and subsequently uses the text with several different\\nlanguage and user features to distinguish rumors. Ma et al. [11]\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='7660\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nused data from recurrent neural time series for modeling, using\\nan RNN network that can learn the feature of the sequential\\ndata to capture the features of rumor events over time. Ma et al.\\n[17] proposed a generative adversarial network (GAN)-based\\nmodel to obtain a feature representation of fake news, which\\nis based on a GRU-based generator to generate controversial\\ninstances, and in turn, designed an RNN-based discriminator to\\nidentify instances. Cheng et al. [18] mainly proposed a GAN-\\nbased hierarchical framework for text-level rumor detection and\\nprovided solutions for interpretation and gene classiﬁcation.\\nHowever, most content-based detection methods rely only on a\\nsingle feature, or content, and do not take into account the rela-\\ntionships and behaviors in a rumor-spreading event. Therefore,\\nthis article considers the integration of text-related work with\\nother work in the form of multitasking to learn multifaceted\\nfeatures for prediction.\\nB. Propagation Structure-Based Detection Methods\\nIn a large number of deep learning tasks, rumor propagation\\nevents often imply temporal and structural features. The pro-\\ncess of rumor events is not static; it is a historical collection\\nof information that is constantly changing over time, and the\\nstructure of propagation is also constantly changing. There is a\\nslight difference between the propagation pattern of true news\\nand the propagation process of false news: for example, true\\nnews tends to propagate more slowly [19], and the content of\\ntrue news is richer and usually related but not identical content\\ncan appear in multiple posts; while the propagation speed of\\nfalse news tends to be exploded growth in a short time, and the\\npropagation content tends to be a few limited and roughly the\\nsame words and pictures, with a more scanty degree of content.\\nThe richness of the content is relatively scarce. Based on the\\nabove features, rumors can be analyzed from the perspectives of\\ntime and dissemination structure and then distinguished. Some\\napproaches have explored studies based on the structural fea-\\ntures (e.g., propagation features) of social networks [20]. Zhang\\net al. [21] proposed a lightweight propagation path aggregation\\nneural network, where they modeled the propagation structure\\nof each rumor as a set of independent propagation paths for\\nrumor embedding and classiﬁcation. For early detection of ru-\\nmor events, Silva et al. [22] proposed training an autoencoder to\\nlearn the embedding of the entire propagation network, which\\nthey demonstrated would give better results for early rumor\\ndetection. Vaibhav et al. [6] modeled each article in the dataset\\nas a graph and formulated the fake news detection task as a\\ngraph classiﬁcation task, where the nodes of the graph come to\\nrepresent the sentences of the article and the edges between the\\nnodes represent the semantic similarity between the sentences.\\nZhang et al. [23] constructed a propagation graph by tracking\\nthe propagation structure of posts and proposed an algorithm\\nbased on gated graph neural networks to generate a robust rep-\\nresentation for each node for rumor detection. Yang et al. [24]\\nexplored the rumor problem from an adversarial perspective\\nbased on graphs to extract more unique structural features for\\nbetter rumor detection by dynamically generating perturbations\\non heterogeneous social graphs with domain constraints. Bian\\net al. [3] proposed a novel bidirectional graphical model (Bi-\\nGCN) that explores these two features by running on top–down\\nand bottom–up propagation of rumors.\\nThe graph neural network effectively captures the global\\nstructural features learned during the propagation of a rumor\\nevent; however, the propagation graph of many applications\\nchanges over time during the development of real social media.\\nTraditional graph neural networks have limited attention to tem-\\nporal features, and numerous studies on dynamic graph neural\\nnetworks have emerged to be able to capture both temporal and\\nspatial structural features. Instead of learning with a static net-\\nwork, Song et al. [8] proposed a dynamic graph-based detection\\nframework to simulate the event evolution of real-world news.\\nSun et al. [10] investigated a dynamic propagation graph-based\\nnews detection problem, where they utilized a structure-aware\\nmodule and an event-aware module to capture temporal and\\nnetwork structure information, respectively. Huang et al. [25],\\nalthough they did not construct a graph structure to deal with the\\nproblem, proposed a spatiotemporal structured neural network,\\nwhich considers the spatial and temporal structures as a whole,\\nto model the news propagation for rumor detection, taking full\\nadvantage of the temporal features that cannot be captured\\nby traditional networks. A dual dynamic graph convolutional\\nnetwork was investigated in the literature [10], modeling spatial,\\ntemporal, and textual features in a single architecture, passing\\nmultiple fused messages to subsequent network units in a se-\\nquential manner.\\nNumerous studies have shown that dynamic graph neural\\nnetworks possess more powerful capabilities compared to static\\ngraph networks, which not only capture the relational features\\nof the propagated structure but also the dynamic structural\\nfeatures of the temporal sequence by constructing static graphs\\nin the form of different temporal sequences.\\nTherefore, in this article, we choose a dynamic graph neural\\nnetwork as one of the baseline models. We try to construct a\\ntime-step-based ﬁne-grained dynamic graph neural network.\\nC. Multitask-Based Detection Methods\\nMultitask learning refers to joint learning behaviors that can\\nshare information about multiple related but not identical tasks\\n[26]. Auxiliary tasks can support the primary task in learning\\ncertain features that are easily overlooked or difﬁcult to learn\\nby the primary task itself, which makes multitask learning par-\\nticularly important when certain potential, but more valuable,\\nfeatures are not well utilized by the primary task. Collobert and\\nWeston [27] described a single convolutional neural network\\narchitecture that, given a sentence outputting multiple differ-\\nent predictions, shares the parameters of multitasks across the\\nnetwork for joint training. Kochkina et al. [15] constructed a\\nmultitask learning architecture consisting of three tasks dealing\\nwith truthfulness classiﬁcation, stance classiﬁcation, and rumor\\ndetection tasks, and in a concrete implementation, only one\\nset of hidden layers was deﬁned, and the same hidden layers\\nwere used for different tasks, allowing them to achieve hard\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 3}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7661\\nFig. 1.\\nDynamic network-based multitask detection framework.\\nparameter sharing. Wan et al. [14] jointly performed rumor\\ndetection and stance classiﬁcation by using two RNN-based\\nnetwork architectures built with shared layers. Wu et al. [28]\\nexplored shared layers with gating and attention mechanisms\\nthat can selectively capture valuable shared features for fake\\nnews detection and stance classiﬁcation. Bai et al. [29] proposed\\na multitask attention tree neural network (MATNN) speciﬁcally\\ndesigned to provide a structured representation of rumor conver-\\nsations and exploited the attention mechanism to jointly classify\\nstances and detect rumor veracity. Cheng et al. [30] proposed a\\nmultitask variational autoencoder-assisted rumor classiﬁcation\\nsystem consisting of four components: rumor detection, rumor\\ntracking, stance classiﬁcation, and veracity classiﬁcation, where\\na more appropriate classiﬁcation technique and training engine\\nimproves the performance and generalization of the model.\\nHowever, most of the existing multitask learning methods\\nrealize sharing only by deﬁning a set of RNNs as a shared\\nlayer and using hard parameter sharing to interact with other\\nintertask features, which, although it realizes parameter sharing\\nto a certain extent and can improve the model performance, the\\nfeature sharing among multitasks is neglected.\\nTherefore, in this article, based on the multitask sharing\\nlayer, we constructed a shared network that can share multitask\\nfeatures as well as hard parameters simultaneously at the time\\nof training.\\nIII. DGCN-TES MODEL\\nIn this article, we propose a dynamic network-based multi-\\ntask detection model (DGCN-TES), which performs multitask-\\ning interaction for rumor detection in a dynamic perspective. It\\nis mainly divided into four modules: dynamic-GCN, content-\\nLSTM, temporal event sharing layer (hereinafter referred to\\nas TES), and fusion and classiﬁcation module, as shown in\\nFig. 1. First, the propagation graphs of rumor events at different\\nmoments are constructed, and they are modeled by using the\\ndynamic graph convolutional network to capture the rumor\\nevents’ dynamic structural features; second, the temporal share\\nlayer is used as a shared network combining dynamic-GCN and\\ncontent-LSTM to interact with features at each identical time\\nstep; and ﬁnally, the attention mechanism is used to focus on\\nthe information that has an important impact on different tasks\\nand between different moments, which is used as the input to\\nthe classiﬁer to make predictions.\\nCompared to other related work [8], [10] on dynamic graphs,\\nalthough they also consider dynamic graph neural networks to\\ndeal with the dynamic correlation patterns of rumor propagation\\nevents, the former does not yet adequately consider temporal\\ngranularity, and the latter neglects the dynamic interaction of\\nmultitasks. In contrast, the computational process of DGCN-\\nTES proposed in this article follows a temporal event-sharing\\nstrategy based on time-step ﬁne granularity, and this design cre-\\nates a different snapshot of the dynamic graph at each moment,\\nwhich not only improves the efﬁciency of multitask sharing\\nbut also makes the dynamic graphs smoother in terms of the\\ntemporal dimensions of the excess.\\nA. Problems and Symbolic Deﬁnitions\\nThe rumor detection task is deﬁned as a binary classiﬁcation\\nproblem. Let ε = ε1, ..., εn be a set of instances of rumor detec-\\ntion events, εi is the ith event, and n is the number of propaga-\\ntion events to be detected in the dataset. Denote the set of posts\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='7662\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nTABLE I\\nIMPORTANT NOTATIONS AND DESCRIPTION\\nNotations\\nDescriptions\\nεi\\nThe ith event\\nεc\\ni\\nThe set of tweets in the ith event\\nγ\\nThe time step of different tweets in the ith event\\nVir, Eir\\nNodes and edges in the ith event at the moment γ\\nGir =< Vir, Eir >\\nPropagation graph for the γ moment in the ith event\\nFig. 2.\\nCoarse-grained dynamic graphs based on time steps.\\nand comment contents as εc\\ni = {si, ci1, ..., cimi−1}, si is the\\nsource tweet, cij is the jth response text, and mi is the number\\nof posts in the event εi. Considering the source tweet as the 0th\\ntweet for presentation simply. Then, εc\\ni = {si, ci1, ..., cimi−1}\\ncan be expressed as εc\\ni = {ci0, ci1, ..., cimi−1}. By expressing\\nthe propagation order of each post in a propagation event εi in\\nterms of time as εt\\ni = {ti0, ti1, ..., timi−1}, with t denoting the\\ntime, the ﬁnal rumor propagation event εi can be expressed as\\nεi = {(ci0, ti0), (ci1, ti1), ..., (cimi−1, timi−1)}.\\nDivide each post εi in a propagation event into γ time steps\\nalong the time dimension, γ is determined by the number of\\nposts mi in each propagation event, γϵ{1, 2, ..., γ}, for a prop-\\nagation event εi different propagation states at different time\\nsteps γ. In the ﬁrst time step εi0 = {(ci0, ti0)}, in the second\\ntime step εi0 = {(ci0, ti0), (ci1, ti1)}, so that the event εi is\\nﬁnally represented at all time steps as εi = {εi0, εi1, ..., εiγ}.\\nFor the same propagation event εi at different time steps,\\ndifferent propagation structure graphs Gir =< Vir, Eir > are\\nconstructed. For easy understanding, the important mathemat-\\nical notations are listed in Table I.\\nB. Dynamic-GCN Module\\n1) Construction of Dynamic Graphs: The idea of construct-\\ning the dynamic graph is inspired by Sun et al. [10], and we\\nconstruct a time-step-based rumor propagation graph, such as\\nthe dynamic-GCN part in Fig. 1. The propagation of rumor\\nevents develops dynamically according to time, and for the\\nsame propagation event, each time step is a network of rumor\\npropagation. With the development of time, the number of these\\nnetworks increases dynamically, eventually forming a dynamic\\npropagation network. According to the rule that the propaga-\\ntion network changes dynamically over time, a corresponding\\npropagation graph is constructed at each time step, where nodes\\ndenote different posts and edges denote the forwarding and\\nreplying relationships between these posts.\\nThis section gives the deﬁnition of a ﬁne-grained dynamic\\ngraph based on time steps: a time step that propagates a ﬁne-\\ngrained dynamic pattern of only one node. This is done as\\nfollows.\\nFig. 3.\\nFine-grained dynamic graphs based on time steps.\\nAs shown in Fig. 2, for the propagation event εi: at the\\nmoment T0, only one source post exists (node 0); at moment\\nT1, two response tweets interact with the source post (nodes 1\\nand 2); at moment T2, only one tweet responds to the source\\npost (node 3); and at moment T3, nodes 4 and 5 respond to\\nnode 1 again simultaneously. A coarse-grained dynamic graph\\nstate, as shown in Fig. 2, was initially obtained based on this\\nrelationship, but some of these moments propagated more than\\none node at the same time, and a ﬁne-grained segmentation is\\nnow required.\\nIn the subsequent computation, in order for the dynamic\\ngraph network to be able to interact with features based on\\ntime steps in the TES, multiple nodes propagated in the same\\nmoment are sorted, resulting in a moment where only one\\nnode’s state is propagated, as shown in Fig. 3. For nodes 1 and\\n2, which are propagated at the same time in the moment of t1,\\nalthough their creation times are the same, they are sorted based\\non the numbering order of each node. Finally, based on the\\nabove node propagation rules and their response and forwarding\\nnode numbers, the respective static graphs are constructed at\\ndifferent time steps, and a ﬁne-grained dynamic graph sequence\\nbased on the time steps is constructed for the dynamic graph\\nconvolution operation in the next section.\\n2) Dynamic-GCN: This section describes in detail the com-\\nputation of ﬁne-grained dynamic graphs based on time steps.\\nFor the propagation event, εi = {εi0, εi1, ..., εiγ} construct\\nsnapshots of the propagation graph at different moments Gir =\\nGi0, ..., Gir. Hp\\nir ∈Rmir∗F is the feature matrix containing all\\nthe nodes of the propagation graph Gir, mir is the number of\\nnodes of the propagation graph at that moment, and F denotes\\nthe features vector of the node. Its adjacency matrix is deﬁned\\nas Air ∈Rmir∗F . As in (1) and (2), the dynamic graph convo-\\nlution layer is deﬁned as\\nHl+1\\nir\\n= σ( ˆD−1\\n2 ˆA ˆD−1\\n2 Hl\\nirW l)\\n(1)\\nwhere ˆD is the matrix, ˆA is the adjacency matrix, Hl\\nir is the\\nfeature matrix of all nodes of the previous layer of the propa-\\ngation graph Gir, W l is the weight matrix, and σ is the nonlinear\\nactivation function.\\nThe operation in (1) is performed on the propagation graph\\nGir = {Gi0, Gi1, ..., Gir} for all time steps to obtain the spatial\\nfeatures of all time steps\\nHir = {Hi0, Hi1, ..., Hir}.\\n(2)\\nAt this point, the propagation structure feature of the same\\npropagation event at each moment has been obtained, form-\\ning a dynamic feature sequence, which is taken as the input\\nto the temporal event sharing layer (described in detail in\\nSection III-D), at the same time, the dynamic structure fea-\\ntures Hir of the last moment is taken as the ﬁnal output of\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7663\\nthe dynamic-GCN module, which is used as the input to the\\nclassiﬁcation network (described in detail in Section III-E).\\nC. Content-LSTM Module\\nIn the previous section, after the ﬁne-grained time-step-based\\ndynamic graph convolution computation, the dynamic struc-\\ntural feature {Hi0, Hi1, ..., Hir}, which is a time series-based\\nembedding of structural features, the difference between neigh-\\nboring time-steps lies in the generation of a certain node (post),\\nwhich are arranged in a strict temporal order. To make this\\ndynamic feature of dynamic graphs related to content-LSTM\\nin the development of the time step to meet the requirement of\\ncombining them based on the time step in TES. In this section,\\nthe node diffusion order in Section III-B is used as the arrange-\\nment order of the textual content of the corresponding rumor\\nevents, and the textual content of the same order is modeled\\nto form a set of time-step-based textual feature sequences. It is\\nused for subsequent computation steps.\\nInspired by Wan et al. [14], we use an LSTM network as a\\nbaseline model. For the propagation event εi, the entire tweets\\ncontained are denoted as Xi_content = {Xi_0, Xi_1, ..., Xi_n} ∈\\nRn∗300, which is used as the input of content-LSTM, as in the\\nfollowing equations:\\nit = σ(WiiXi_content_t + bit + Whih(t−t) + bhi)\\n(3)\\nft = σ(WifXi_content_t + bif + Whfh(t−1) + bhf)\\n(4)\\ngt = tanh(WigXi_content_t + big + Whgh(t−1) + bhg)\\n(5)\\not = σ(WioXi_content_t + bio + Whoh(t−1) + bho)\\n(6)\\nct = ft ∗ct−+ it ∗gt\\n(7)\\nhcontent −t = ot ∗tanh(ct)\\n(8)\\nwhere Xi_content_t is the input feature vector at a given moment,\\nh(t−1) is the hidden state at the previous moment, and W∗and\\nb∗are the corresponding weight matrix and bias.\\nAt this point, the same propagation event at each moment of\\nthe text feature sequence, formed a temporal feature sequence\\nHcontent, as in (9), notated as\\nhcontent = {hcontent−0, hcontent−1, ..., hcontent−t}.\\n(9)\\nNote that the time series feature corresponds one-to-one with\\nthe dynamic structure sequence feature Hi in (2) in terms of the\\norder of occurrence of the propagation events. This time series\\nfeature is taken as the input of the temporal event sharing layer\\n(described in detail in Section III-D); meanwhile, the sequence\\nfeature Hcontent of the last moment is taken as the ﬁnal output of\\ncontent-LSTM, which is used as the input of the classiﬁcation\\nnetwork (described in detail in Section III-E).\\nD. Temporal Event Share Layer\\nIn the above two sections, the dynamic structural feature Hi\\nand the time-series feature Hcontent are obtained based on the\\nrule that the propagation network develops dynamically over\\ntime, as well as the temporal order in which the nodes are\\nFig. 4.\\nStructure of TES.\\ngenerated, respectively. They are related in the development\\nsequence of propagation events; in other words, for a certain\\npropagation event at the same time stage, we capture its tem-\\nporal and spatial feature sequences, and their temporality and\\ndynamics are consistent in the postdevelopment process.\\nIn this section, as shown in the middle part of Fig. 1,\\na temporal event-sharing layer is proposed, which combines\\ndynamic-GCN and content-LSTM with time step as the basic\\ncomputational unit and shares their features at the same time\\nstage interactively and trains them together. Compared to other\\nmultitask learning using hardware parameter sharing [15] as\\nan approach, we additionally implement temporal event shar-\\ning, which improves the sharing frequency and efﬁciency of\\ndynamic-GCN and content-LSTM. The speciﬁc computation\\nsteps are as follows.\\nStep 1: As in Fig. 4, at the moment t0, the feature Hi0−v ∈\\nR1∗F of the node(Node_0) corresponding to the one that\\nis being propagated in the dynamic structural feature Hi0 ∈\\nRnodes∗F in Section III-B is taken, and then the time-series\\nfeature Hcontent−0 in Section III-C is taken. Now, the propaga-\\ntion structure features and content features of the corresponding\\ntweets with the number 0 are taken at the same time.\\nStep 2: Splice them and go through the linear layer mapping\\nas the input of the cell block at the moment t0 in the TES, and\\ngo through the gating calculation to get the output of the ﬁrst\\ntime step, as in the following equations:\\nXshare−0 = Linear(Concat(Hi0, Hcontent−0))\\n(10)\\nit = σ(WiiXshare−0 + bit + Whth(t−1) + bhi)\\n(11)\\nft = σ(WifXshare−0 + bif + Whfh(t−1) + bhf)\\n(12)\\ngt = tanh(WigXshare−0 + big + Whgh(t−1) + bhg) (13)\\not = σ(WioXshare−0 + bio + Whoh(t−1) + bho)\\n(14)\\nct = ft ∗ct−1 + it ∗gt\\n(15)\\nHshare−0 = ot ∗tanh(ct).\\n(16)\\nIn the above equation, Xshare−0 is the input of the temporal\\nevent sharing layer at the moment t0, W∗and b∗are the weight\\nmatrix and bias, and h(t−1) is the hidden layer vector of the\\nprevious moment, which is randomly initialized and generated\\nat the moment t0, and Hshare−0 is the output.\\nAt this point, a similar operation is carried out in the ﬁrst cell\\nblock of the content-LSTM, with the difference that the input\\nis a representation of the text content at the moment t0, and the\\noutput is the time series feature hcontent−0 at the moment t0.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 6}, page_content='7664\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nStep 3: Splice hcontent−0 and Hshare−0 and undergo linear\\nmapping to update the hidden state of content-LSTM at the\\nmoment of t0 as the hidden state of the content-LSTM cell block\\nat the next moment, as in\\nhcontent−0 = Linear(Concat(hcontent−0, Hshare−0)).\\n(17)\\nThe computation of the content-LSTM for the next moment\\nis referred to in (3)–(8).\\nStep 4: Repeat the ﬁrst step to the third step until all the time\\nsteps have been looped to ﬁnally get the output of each moment\\nof TES, as in\\nHshare = {Hshare−0, Hshare−1, ..., Hshare−t}.\\n(18)\\nNote that the TES is continuously interacting with the infor-\\nmation in dynamic-GCN and content-LSTM in chronological\\norder during the computation process, thus realizing temporal\\nevent sharing.\\nE. Attentional Mechanisms and Classiﬁer\\nIn the previous section, the computation process has been\\ncompleted and the feature representation after feature shar-\\ning has been obtained through TES, which are dynamic-GCN,\\ncontent-LSTM, and TES layers: Hir, Hcontent, and Hshare.\\nTo understand which features are more important for pre-\\ndiction across different time series and across different tasks,\\nthis section updates them using a multihead attention mecha-\\nnism [31]. Rumor detection efforts in LSTM networks typically\\nuse the latent feature representation of the ﬁnal time series as\\nthe predicted feature vector [14], since the sequence data are\\ncomputed through a gate mechanism, and the output of the last\\ngate also contains all of the previous gating computed hidden\\nstates. Therefore, we also use the features of the last time series\\nfor computation. As in the right part of Fig. 1, we obtain the\\nconvolutional features Hit of the dynamic graph convolutional\\nnetwork at the ﬁnal moment for subsequent computation. The\\ndetails of the implementation are as follows.\\nIn the ﬁrst step, Hshare−t is spliced with Hcontent−t and Hi−t,\\nrespectively, and mapped by a linear layer to obtain the inputs\\nX1 and X2 of the attention layer as in the following equations:\\nX1 = Linear(Concat(Hcontent−t, Hshare−t))\\n(19)\\nX2 = Linear(Concat(Hi−t, Hshare−t)).\\n(20)\\nIn the second step, X1 and X2 are linearly transformed into\\nthree projections with the same dimensions as itself (query Q;\\nkey K; and value V), which are used as inputs to the attention\\nlayer, as in the following equations:\\nMultiHead(Q, K, V ) = Concat(head1, ..., headn)W o\\n(21)\\nheadi = Attention(XW Q\\ni , XW K\\ni , XW V\\ni ) (22)\\nAttention(O, K, V ) = softmax\\n\\x02QKT\\n√dk\\nV\\n\\x03\\n.\\n(23)\\nMultiHead(Q, K, V ) is the output of the attention layer, Q,\\nK, and V denote XW Q\\ni , XW K\\ni , XW V\\ni , and W o is the weight\\nTABLE II\\nSTATISTICS OF DATASETS\\nStatistic\\nWEIBO\\nPHEME\\nPosts/events\\n4663\\n5473\\nNon −rumors\\n2312\\n1996\\nRumors\\n2351\\n3477\\nAvg posts/event\\n452\\n20\\nMax posts/event\\n44 764\\n346\\nMin posts/event\\n2\\n3\\nmatrix, Wi is the initialization of the different multimatrices,\\ndk is the dimension of K, scaling the output when K is larger.\\nIn the third step, the outputs of the attention layer are spliced\\nwith Hshare−t and Hi−t, and go through a linear mapping, which\\nis used to update them, as in the following equations:\\nHcontent−t = Linear(Concat(Hshare−t, Hcontent−t))\\n(24)\\nHi−t = Linear(Concat(Hshare−t, Hi−t)).\\n(25)\\nIn the fourth step, the updated, ﬁnal event feature represen-\\ntations Hcontent−t,Hi−t for prediction are obtained, and the cor-\\nresponding fully connected and activation layers are designed\\nfor them, respectively, as in the following equations:\\nYc = softmax(WcHcontent−t + bc)\\n(26)\\nYDG = softmax(WDGHi−t + bDG).\\n(27)\\nW∗and b∗are their respective corresponding weights and bias\\nparameters, and ﬁnally, the cross entropy loss function is used\\nas the classiﬁcation loss as in the following equations:\\nLc = −\\n\\x04\\ni\\nYilogYC\\n(28)\\nLDG = −\\n\\x04\\ni\\nYilogYDG.\\n(29)\\nYi is the true label of the ith event. For training, the two loss\\nfunctions are assigned corresponding coefﬁcients and summed\\nto ﬁnd the total loss function for backpropagation as in (30).\\nW1 and W2 represent the parameters of the two tasks\\nLoss = W1 ∗LC + W2 ∗LDG.\\n(30)\\nIV. EXPERIMENTS\\nThis section evaluates the superiority of our method by com-\\nparing the explored method with other methods based on several\\npublicly available datasets. In addition, ablation experiments\\nand other complementary experiments are performed to illus-\\ntrate the effectiveness of the various components of our method.\\nA. Datasets\\nTo investigate the content and propagation patterns of fake\\nnews, the WEIBO dataset [5] and PHEME dataset [15] are\\nselected. After removing some samples, the dataset is shown\\nin Table II, which contains both false and true information\\nfrom both sites. Each event contains several response tweets\\nwith comments and retweets, and all have a label, rumor, or\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 7}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7665\\nnonrumor. For the WEIBO dataset, due to the large average\\nnumber of posts per rumor event, the WEIBO dataset is divided\\ninto training, validation, and testing sets in the ratio of 6:2:2,\\nand for the PHEME dataset in the ratio of 8:1:1, taking into\\naccount the balance of the sample distribution. To compare\\nthe detection results with other models to validate the effec-\\ntiveness of the model, the experiments used four evaluation\\nmetrics, namely, accuracy, precision, recall, and F1 score. The\\nF1 score was used to evaluate the performance of the model.\\nThe calculation formulas are shown in the following equations,\\nrespectively, where TP denotes a true-positive rate, TN denotes\\na true-negative rate, FP denotes a false-positive rate, and FN\\ndenotes a false-negative rate:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN\\n(31)\\nPrecision =\\nTP\\nTP + FP\\n(32)\\nRecall =\\nTP\\nTP + FN\\n(33)\\nF1 = 2 ∗Precision ∗Recall\\nPrecision + Recall .\\n(34)\\nB. Comparison Methods\\nWe compare with the following baselines.\\nDTC [32]: a rumor detection method using Decision Tree\\nclassiﬁer with hand-crafted features.\\nSVM-RBF [33]: an SVM model with RBF kernel using\\nhandcrafted features based on the overall statistics of the posts\\nfor classiﬁcation.\\nSVM-TS [34]: A time-series structure-based SVM classiﬁer\\nthat extracts features by modeling the content, users, and prop-\\nagation patterns of rumors as well as their impact on society.\\nRvNN [5]: a method that uses a tree-structured recursive\\nneural network to extract the high-level representation from\\nboth text contents and propagation structures, which reinforces\\nor weakens the stance of the node by judging whether to respond\\nto the node.\\nTGNF [9]: a temporally evolving graph neural network\\nmodel for fake news detection fusing interactive and temporal\\nfeatures.\\nBi-GCN [3]: a GCN-based rumor detection model utilizing\\nthe bidirectional propagation structure to obtain the propagation\\nand dispersion features.\\nSTS-NN [25]: a spatial–temporal structure neural network to\\ntreat the spatial structure and the temporal structure as a whole\\nto model the message propagation for rumor detection.\\nDDGCN [10]: a dual-dynamic graph convolutional network\\nto model the spatial structure, temporal structure, external\\nknowledge, and text information in one uniﬁed framework.\\nC. Parameter Settings\\nFor all comparison methods, the default settings from the\\ncorresponding articles were used. All hyperparameters are spec-\\niﬁed in Table III. For the methods in this study, the parameters\\nTABLE III\\nMAIN HYPERPARAMETER SETTINGS\\nDatasets\\nHyperparameter\\nPHEME\\nAverage subgraph nodes\\n20\\nMax subgraph nodes\\n346\\nTime steps\\n20\\nHidden Dim\\n128\\nBatch size\\n128\\nWEIBO\\nAverage subgraph nodes\\n40\\nMax subgraph nodes\\n44 764\\nTime steps\\n452\\nHidden Dim\\n128\\nBatch size\\n32\\nTABLE IV\\nRUMOR DETECTION RESULTS ON THE WEIBO DATASET\\nMethod\\nAcc\\nPre\\nRecall\\nF1\\nDTC\\n0.858\\n0.834\\n0.822\\n0.857\\nSVM-RBF*\\n0.899\\n0.938\\n0.846\\n0.889\\nSVM-TS\\n0.885\\n0.950\\n0.932\\n0.938\\nRvNNBU(ACL18)\\n0.908\\n0.912\\n0.897\\n0.905\\nSTS-NN\\n0.913\\n0.902\\n0.898\\n0.900\\nBi-GCN(AAAI20)\\n0.919\\n0.918\\n0.916\\n0.913\\nTGNF(IPM21)*\\n0.968\\n0.974\\n0.960\\n0.967\\nDDGCN(AAAI22)*\\n0.948\\n0.953\\n0.948\\n0.950\\nours\\n0.974\\n0.978\\n0.970\\n0.975\\nNote: The bold entries are the relative best values.\\nTABLE V\\nRUMOR DETECTION RESULTS ON THE PHEME DATASET\\nMethod\\nAcc\\nPre\\nRecall\\nF1\\nDTC\\n0.581\\n0.659\\n0.652\\n0.656\\nSVM-RBF\\n0.602\\n0.875\\n0.431\\n0.577\\nSVM-TS\\n0.651\\n0.642\\n0.686\\n0.663\\nRvNNBU(ACL18)\\n0.789\\n0788\\n0.788\\n0.789\\nSTS-NN\\n0.819\\n0.816\\n0.791\\n0.800\\nBi-GCN(AAAI20)\\n0.847\\n0.840\\n0.834\\n0.835\\nTGNF(IPM21)*\\n0.848\\n0.892\\n0.861\\n0.877\\nDDGCN(AAAI22)*\\n0.855\\n0.846\\n0.841\\n0.844\\nours\\n0.869\\n0.878\\n0.877\\n0.877\\nNote: The bold entries are the relative best values.\\nwere optimized using the Adma algorithm. The learning rate is\\n1e-3, and the batch sizes are 32 and 128. For the embedding\\nrepresentation, this article uses the spaCY open source AIP\\npretraining model TRANSFORMER to process each sentence\\nto get the embedding representation with 300 dimensions.\\nD. Experimental Results and Analyses\\nThe proposed model is compared with the baseline method\\nmentioned in Section IV-B, Tables IV and V show the per-\\nformance of the compared methods, where the ﬁrst group is\\nthe machine learning method and the second group is the\\ndeep learning approach. Fig. 5 also shows more visually the\\ncomparison of the proposed method in this article with other\\nbaselines using F1 scores. We ran ﬁve runs based on the default\\nsettings in the original paper and reported the average results\\nhere, “*” Experimental results were obtained directly from the\\noriginal paper.\\nFrom the above table, the following analysis can be obtained.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='7666\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nFig. 5.\\nComparison of F1-score in different baseline models.\\nThe machine learning methods (DTC, SVM-RBF, and SVM-\\nTS) have the worst performance scalar, none of which is bet-\\nter than the second group. This is due to the fact that the\\ninformation extracted by feature engineering is often a latent\\nrepresentation of the data and does not capture deeper feature\\ninformation, whereas deep learning is able to learn high-level\\nsemantic information about the data. However, a comparison\\nof the ﬁrst group of methods with each other shows that SVM-\\nTS outperforms the other two methods due to the development\\nof a dynamic series of time-structured models that are able\\nto explore the patterns of various social contextual features\\nover time, and feature design that incorporates social contextual\\ninformation.\\nIn the second group, the RvNN that utilizes the propaga-\\ntion tree model performs better than SVM-TS, the accuracy is\\n2.59% and 21.19% higher on WEIBO and PHEME datasets,\\nrespectively, this is due to the fact that it models the rumor\\nevents as a propagation tree, the leaf nodes will inﬂuence and\\nlearn each other’s information; however, the gap on the WEIBO\\ndataset is relatively close, which may be due to the fact that\\nthe WEIBO dataset is caused by the higher average number of\\nposts when the average number of posts of events is higher, the\\npropagation tree constructed is richer, and the structural features\\ncaptured are more obvious. STS-NN proposes a spatiotemporal\\nstructural neural network, which models the spatial and tem-\\nporal structure as a whole for message propagation for rumor\\ndetection and does not construct a graph structure to capture the\\nspatial features, so its accuracy is 0.5% higher than that of the\\nRvNN by 0.4%, 3.8%, respectively, which is between RvNN\\nand Bi-GCN.\\nBi-GCN that utilizes the propagation graph structure has bet-\\nter performance, it outperforms RvNN and STS-NN by 1.2%,\\n7.35%, and 0.65%, 3.41% on the two datasets, respectively,\\nthanks to its use of static graph related algorithms. It learns the\\nembedding representation of graphs from a bidirectional point\\nof view, designing a bidirectional convolutional algorithm to\\nlearn better features. But it does not use dynamic ideas.\\nTABLE VI\\nRESULTS OF COMPARISON AMONG DIFFERENT VARIANTS\\nDatasets\\nMethod\\nacc\\npre\\nrecall\\nF1\\nWEIBO\\nW/o GCN\\n0.906\\n0.902\\n0.902\\n0.902\\nW/o TES\\n0.952\\n0.961\\n0.962\\n0.961\\nW/o dynamic\\n0.961\\n0.961\\n0.969\\n0.965\\nDGCN-TES\\n0.974\\n0.978\\n0.970\\n0.975\\nPHEME\\nW/o GCN\\n0.812\\n0.812\\n0.812\\n0.813\\nW/o TES\\n0.828\\n0.823\\n0.826\\n0.824\\nW/o dynamic\\n0.838\\n0.829\\n0.840\\n0.834\\nDGCN-TES\\n0.869\\n0.878\\n0.877\\n0.877\\nNote: The bold entries are the relative best values.\\nAlgorithms using dynamic ideas will have better results,\\nTGNF models the temporal interaction events of nodes from\\nthe dynamic evolution perspective, and the accuracy is 5.33%\\nand 0.1% higher than Bi-GCN, respectively. DDGCN is 0.82%\\nmore accurate than TGNF on the PHEME dataset due to a kind\\nof dynamic graph convolutional network.\\nOverall, DGCN-TES has the best performance in most of the\\nmetrics, and the ﬁne-grained time-step-based dynamic graph\\nnetwork we designed is able to capture dynamic sequence\\nfeatures with more delicate transitions, get stronger feature\\nrepresentations from interaction training in TES, and utilize\\nthe attention mechanism, which is able to act as ﬁltering and\\naccelerate model convergence, and thus outperforms Bi-GCN\\nby 5.98% and 2.59%, respectively, and DDGCN by 2.74%\\nand 1.63%, respectively, than Bi-GCN using the static graph\\nalgorithm.\\nThe method DGCN-TES proposed in this study achieves\\nbetter performance, thanks to the fact that we model and in-\\nteract with information about the dynamic spatial structure and\\ntemporal text content. Speciﬁcally, there are the following ad-\\nvantages that other methods do not do:\\n1) In this article, we constructed a time-step level, ﬁne-\\ngrained dynamic graph convolutional layer based on the\\ntime-step level, where a snapshot of the dynamic graph\\nis created for each extension of a node. More delicate\\nand ﬁne-grained features are captured in the dynamic\\nrelationship model.\\n2) Using the form of multitasking to combine the sequence\\ndata with the dynamic relationship, in the design, it is\\ndone that the time in the sequence data corresponds to\\nthe time in the dynamic relationship, dynamic-GCN and\\ncontent-LSTM are combined at the time-step level, which\\nmakes both of them more powerful and more effective in\\nsharing interaction.\\nE. Ablation Experiment\\nTo further investigate the impact of the individual parts in this\\nmethod, this section conducts ablation experiments using the\\ncomparison strategy shown in Table VI. The model is compared\\nto the network with each part removed.\\nThe following combinations are deﬁned to experimentally\\ndemonstrate the effect of each of our proposed components.\\n1) W/o TES: When the model does not use TES, the overall\\narchitecture is content-LSTM and dynamic-GCN as well\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7667\\nas classiﬁers. Despite the lack of TES, the overall form\\nof multitasking remains.\\n2) W/o dynamic: When the model does not use DGNN,\\nthe overall architecture is content-LSTM, GCN, TES,\\nAttention mechanism, and Classiﬁer.\\n3) W/o GCN: When the model does not use a graph-\\nconvolutional network, the TES cannot be shared due\\nto the lack of a graph-convolutional layer, leaving the\\nnetwork with only the content-LSTM and the classiﬁer.\\nThe following analysis can be obtained from Table VI.\\nW/o GCN: This effect is the worst, due to the fact that\\nthe network loses objects that can be shared, leaving only the\\ncontent-LSTM layer for detection, and is unable to capture\\nspatial features as well as dynamic features.\\nW/o TES: At this time, the network is still in the mode of\\nmultitasking, but due to the loss of TES, the sharing strategy is\\nchanged, and relying on backpropagation alone for parameter\\nsharing is limited and insufﬁcient to achieve the best perfor-\\nmance, but the accuracy is 5.07% and 1.97% higher w/o GCN\\nstrategy, respectively, which shows the important impact of the\\ntime-step-based TES.\\nW/o dynamic: This strategy is the best in addition to DGCN-\\nTES and the accuracy is higher than w/o TES by 0.94% and\\n1.2%, respectively, mainly because this approach basically re-\\nalizes multitasking combined with temporal event sharing. The\\ntwo tasks are able to capture temporal features, spatial features\\ntogether, and at the same time, because of the participation of\\nthe attention mechanism, the performance is better than other\\nstrategies. However, due to the use of static graphs, it ignores\\nthe dynamics of rumor events in the evolution process.\\nBy comparing with other strategies, DGCN-TES is the op-\\ntimal method for each combined strategy. The model can fully\\ncapture the content features of sequence data, the structural\\nfeatures of rumor propagation events, and the dynamic feature\\nof the propagation process, combining them in a multitask\\nfashion and designing a time-series event-sharing layer so that\\nthe two tasks can better learn from each other through ﬁne-\\ngrained time steps. Finally, the attention mechanism is utilized\\nto focus on the features that have an important effect on pre-\\ndiction between different tasks and different time steps and\\nignore those less important information so as to improve the\\nmodel effect.\\nIn summary, the following conclusion is drawn: the DGCN-\\nTES is the best performer, which proves that the idea of combin-\\ning textual content information in the form of multitasking with\\na dynamic dissemination structure, and then utilizing temporal\\nevent sharing to enhance the interaction between the multitasks,\\nis effective and has important implications.\\nF. Compare and Contrast Different Time Granularity\\nDynamics Graph\\nAt the early stage of the experiment, we had a conjecture:\\nhow does the coarseness of the time-step division granularity\\nand the number of static graph snapshots constructed affect the\\nexperiment? We believe that the ﬁner the granularity and the\\nmore static graph snapshots are constructed, the more delicate\\nFig. 6.\\nDifferent time particle size comparison experiments.\\nand more effective the transition of the generated dynamic\\ngraph features will be, which will lead to a better overall per-\\nformance of the model.\\nIn this section, to investigate how dynamic graphs with differ-\\nent granularities affect network performance, this issue is inves-\\ntigated by actively controlling the time interval for generating\\nsnapshots of static graphs. Experiments were conducted on the\\nPHEME dataset and the WEIBO dataset, following several sets\\nof different time granularity strategies. The results are shown\\nin Fig. 6, where the horizontal axis indicates the number of set\\ntime steps relative to the average event propagation length, and\\nthe vertical axis indicates the accuracy.\\nFrom Fig. 6, it is easy to ﬁnd that when the time interval is\\nsmaller, a static graph snapshot with a smoother transition will\\nbe constructed; and thus, the captured structural features are\\nmore delicate and more orderly. More importantly, the smaller\\nthe interval, the better the interaction between the dynamic\\ngraph network layer and the timing event sharing layer, the\\ncloser the information interaction between multiple tasks, and\\nthe better the performance of the model.\\nCoarser granularity and worst results when the time step is\\nsplit into three (1/3). However, comparing the (1/6) time steps,\\nthe accuracy of the WEIBO and PHEME datasets is improved\\nby 4.71% and 0.48%, respectively, and this gap is not very\\nobvious in the PHEME dataset, which may be due to the fact\\nthat the average number of posts of the propagation events in\\nthe PHEME dataset is less, and the transition difference of the\\ndynamical graph features of the different time steps is closer to\\neach other, which ultimately leads to the results of the different\\ntime granularity is closer, and the accuracy increases steadily\\nas the time granularity increases.\\nTherefore, it was concluded as follows.\\n1) The model performs better when the temporal granularity\\nis ﬁner, the transitions of the captured dynamical graph\\nfeatures are more delicate, and the interactions in the TES\\nare closer.\\n2) The average number of posts of a propagation event will\\ndirectly determine the number of time steps, and when the\\naverage number of posts of a propagation event is small,\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 10}, page_content='7668\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nFig. 7.\\nEarly detection on the PHEME dataset.\\nFig. 8.\\nEarly detection on the WEIBO dataset.\\nthe gap between DGCN-TES at different time granulari-\\nties is closer.\\nG. Early Detection\\nEarly rumor detection is one of the important goals of ru-\\nmor detection, a task aimed at identifying rumor events in the\\nearly stages of their propagation, and this means of assessing\\nthe quality of rumor detection methods is another important\\nreference point.\\nTo construct an early rumor detection task based on DGCN-\\nTES, this experiment on one hand controls the number of nodes\\ngenerated by inputting the rumor propagation graph into the\\ndynamic graph convolutional layer, and on the other hand,\\nintercepts the text sequence data corresponding to this time,\\nand represents the different periods of rumor events by re-\\nconstructing the rumor propagation events. Speciﬁcally, this\\nexperiment deletes the corresponding tweets posted after a ﬁxed\\ntime step to reconstruct the propagation event dataset of the\\nmodel, and after that, in the same way, we take the sequential\\ntext content corresponding to the time period and construct\\nthe dynamic propagation graph, and then evaluate the perfor-\\nmance of the rumor detection model in different periods. As in\\nFigs. 7 and 8, the differences between DGCN-TES and other\\napproaches on the PHEME dataset and WEIBO dataset are\\nshown respectively.\\nIn terms of the dataset, the WEIBO dataset performs better\\nthan the other methods in any period of time, and the accuracy\\nsteadily increases as the early detection phase develops. This\\nmay be due to the fact that the WEIBO dataset propagates events\\nwith a larger average number of posts, the dynamic excess of\\nthe time step is very delicate, and the learned dynamic structural\\nfeatures are able to interact with the temporal features at a\\nhigher frequency. From this perspective, it can explain why\\nthe performance of the PHEME dataset is more similar to the\\nDDGCN method in the early detection phase from 20% to 60%,\\nand then pulls away from 80% to 100%.\\nFrom the results, the proposed method DGCN-TES achieves\\nrelatively high accuracy in the early stage of rumor event prop-\\nagation, in addition, STS-NN is the worst in early detection,\\nthis is because all other networks adopt graph network related\\noperation, which suggests that the graph network is more con-\\nducive to deal with early detection. As for the methods using\\ngraph networks, the best results are achieved by using dynamic\\ngraph networks, and our curves are more similar to DDGCN in\\nthe early stage of rumor propagation and superior to DDGCN in\\nthe subsequent stage of rumor propagation, which is due to the\\nfact that our dynamic temporal granularity is more ﬁne-grained,\\nand the more delicate dynamic graph structure demonstrates a\\nmore superior performance in both early detection and long-\\nterm rumor detection.\\nH. Exploring the Impact of Different Variants of the Model\\nIn the above-mentioned work, this article combines dynamic-\\nGCN and content-LSTM in the form of multitasking, although\\nthe two base components are obtained by modifying the GCN\\nand LSTM networks, the most basic original model is used\\nand no related variant network is used. However, in the ﬁeld\\nof NLP research, several studies have carried out richer and\\nmore effective work using variant networks, e.g., Pandey et al.\\n[35] used an LSTM network based on an attention mechanism\\nto recognize sarcastic statements, and Pandey and Singh [36]\\nproposed a BERT-based bidirectional encoder model that stacks\\nLSTM networks to perform recognition prediction.\\nRelevant research has all shown that the variant model ob-\\ntained by improving on the original baseline model has better\\nperformance, therefore, to better explore how much different\\nvariant models affect DGCN-TES, in this section, this article\\nreplaces the baseline components in DGCN-TES and carries\\nout experiments. Speciﬁcally, two variant network experimental\\nscenarios are carried out in this section as follows.\\n1) Using attention-based LSTM network instead of content-\\nLSTM component in DGCN-TES.\\n2) Using bi-LSTM network instead of TES component in\\nDGCN-TES\\nThe experimental results are shown in Figs. 9 and 10.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 11}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7669\\nFig. 9.\\nComparison of F1-score in different variants of networks on WEIBO\\ndataset.\\nFig. 10.\\nComparison of F1-score in different variants of networks on\\nPHEME dataset.\\nIt is clearly visible through Fig. 9 and Fig. 10 that the\\nexperimental results of using the attention mechanism-based\\nLSTM instead of content-LSTM are better than DGCN-TES,\\nand by comparing the F1 scores on the WEIBO and PHEME\\ndatasets, it can be found that the DGCN-TES (Att-LSTM) ex-\\nceeds the DGCN-TES by 0.21% and 0.23%, which is mainly\\nbecause when dealing with sequential text data, the attention\\nmechanism is able to focus on feature information in the con-\\ntext that is more beneﬁcial for rumor detection, thus learning\\nbetter feature representations, which ultimately enhances the\\noverall performance of the model. The variant network scheme\\nusing Bi-LSTM instead of content-LSTM is more effective,\\nexceeding 0.82% and 0.46% on WEIBO and PHEME datasets,\\nwhich is mainly due to the fact that bi-LSTM uses bidirectional\\nsequential structure, which is not only focuses on sequential\\nfeatures from top to the bottom but also focuses on the effective\\ninformation from the bottom to the top when dealing with the\\ntemporal sequential text information. This way of processing is\\nmore dynamic and also makes the dynamic feature interaction\\nbetween the “previous time step” and “next time step” of the\\ndynamic graph in the dynamic-GCN component more effective,\\nthus showing better performance.\\nMore in-depth exploration using variant networks can further\\nimprove the performance of the model, but the focus of this\\narticle is on exploring dynamic patterns in the rumor propaga-\\ntion process, not in this context (e.g., using the GAT network\\ninstead of the GCN network in dynamic-GCN; and using the\\nBERT instead of content-LSTM), and therefore no more in-\\ndepth experiments are conducted using other variant networks\\nin this section.\\nV. CONCLUSION\\nIn this article, we propose a network named DGCN-TES,\\nwhich models dynamic spatial structure, temporal structure,\\nand textual content information under a uniﬁed architecture.\\nDGCN-TES contains a ﬁne-grained dynamic graph convo-\\nlutional layer based on temporal sequences and a textual\\ncontent-based LSTM layer for processing spatial and content\\ninformation over time. In addition, we designed a temporal\\nevent-sharing module for sharing the interaction of features of\\ndifferent temporal in the two tasks to learn from each other\\nduring the training process. Finally, the attention mechanism\\nis utilized to focus on the features that are beneﬁcial for pre-\\ndiction between different tasks and time series to improve the\\nnetwork’s effectiveness. Our experiments get better feedback\\non two public datasets, and DGCN-TES outperforms other\\nmethods. Finally, we did additional supplementary experiments\\nto explore the effect of different temporal granularities on the\\nperformance of dynamic networks and found that more delicate\\ntemporal granularities are more suitable for dynamic networks.\\nAn early detection study was also conducted, and the DDGCN-\\nTES performance was also superior in comparison with other\\nmethods. An extended research is also done on the effect of\\ndifferent variant networks on the model, and by using different\\nvariant networks for experiments, it is found that a more suitable\\nvariant network can bring better performance improvement.\\nIn the future, we will further explore the properties of rumor-\\nspreading events in dynamic networks, and process the multi-\\nmodal information in rumor-spreading events for better network\\nperformance.\\nREFERENCES\\n[1] N. DiFonzo and P. Bordia, “Rumor psychology: Social and organiza-\\ntional approaches.” Amer. Psychol. Assoc., 2007.\\n[2] J. Cohen and D. Normile, “New SARS-like virus in China triggers\\nalarm,” Amer. Assoc. Adv. Sci., vol. 367, no. 6475, pp. 234–235, 2020.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='7670\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\n[3] T. Bian et al., “Rumor detection on social media with bi-directional\\ngraph convolutional networks,” in Proc. AAAI Conf. Artif. Intell.,\\nvol. 34, no. 1, 2020, pp. 549–556.\\n[4] N. Bai, F. Meng, X. Rui, and Z. Wang, “Rumor detection based on a\\nsource-replies conversation tree convolutional neural net,” Computing,\\nvol. 104, pp. 1–17, Jan. 2022.\\n[5] J. Ma, W. Gao, and K.-F. Wong, “Rumor detection on Twitter with\\ntree-structured recursive neural networks,” in Proc. 56th Ann. Meeting\\nAssociation for Comp. Linguistics, Association for Computational Lin-\\nguistics, 2018, pp. 1980–1989.\\n[6] V. Vaibhav, R. M. Annasamy, and E. Hovy, “Do sentence interactions\\nmatter? Leveraging sentence level representations for fake news classi-\\nﬁcation,” 2019, arXiv:1910.12203.\\n[7] X. Zhou and R. Zafarani, “A survey of fake news: Fundamental theories,\\ndetection methods, and opportunities,” ACM Comput. Surv., vol. 53,\\nno. 5, pp. 1–40, 2020.\\n[8] C. Song, Y. Teng, Y. Zhu, S. Wei, and B. Wu, “Dynamic graph neural\\nnetwork for fake news detection,” Neurocomputing, vol. 505, pp. 362–\\n374, Sep. 2022.\\n[9] C. Song, K. Shu, and B. Wu, “Temporally evolving graph neural network\\nfor fake news detection,” Inf. Process. Manage., vol. 58, no. 6, 2021,\\nArt. no. 102712.\\n[10] M. Sun, X. Zhang, J. Zheng, and G. Ma, “DDGCN: Dual dynamic graph\\nconvolutional networks for rumor detection on social media,” in Proc.\\nAAAI Conf. Artif. Intell., vol. 36, no. 4, 2022, pp. 4611–4619.\\n[11] J. Ma, W. Gao, P. Mitra, S. Kwon, and B. J. Jansen, “Detecting rumors\\nfrom microblogs with recurrent neural networks,” in Proc. 25th Int. Joint\\nConf. Artif. Intell., ser. IJCAI’16, 2016, pp. 3818–3824.\\n[12] T. Ahmad, M. S. Faisal, A. Rizwan, R. Alkanhel, P. W. Khan, and\\nA. Muthanna, “Efﬁcient fake news detection mechanism using enhanced\\ndeep learning model,” Appl. Sci., vol. 12, no. 3, 2022, Art. no. 1743.\\n[13] J. Ma, W. Gao, and K.-F. Wong, “Detect rumor and stance jointly\\nby neural multi-task learning,” in Proc. Companion Web Conf., 2018,\\npp. 585–593.\\n[14] S. Wan, B. Tang, F. Dong, M. Wang, and G. Yang, “A writing style-based\\nmulti-task model with the hierarchical attention for rumor detection,” Int.\\nJ. Mach. Learn. Cybern., vol. 14, no. 11, 2023, pp. 1–16.\\n[15] E. Kochkina, M. Liakata, and A. Zubiaga, “All-in-one: Multi-task\\nlearning for rumour veriﬁcation,” 2018, arXiv:1806.03713.\\n[16] J. P. Singh, A. Kumar, N. P. Rana, and Y. K. Dwivedi, “Attention-\\nbased LSTM network for rumor veracity estimation of tweets,” Inf. Syst.\\nFrontiers, vol. 24, pp. 1–16, Aug. 2020.\\n[17] J. Ma, W. Gao, and K.-F. Wong, “Detect rumors on Twitter by promoting\\ninformation campaigns with generative adversarial learning,” in Proc.\\nWorld Wide Web Conf., 2019, pp. 3049–3055.\\n[18] M. Cheng, Y. Li, S. Nazarian, and P. Bogdan, “From rumor to genetic\\nmutation detection with explanations: A GAN approach,” Sci. Rep.,\\nvol. 11, no. 1, 2021, Art. no. 5861.\\n[19] S. Vosoughi, D. Roy, and S. Aral, “The spread of true and false news\\nonline,” Science, vol. 359, no. 6380, pp. 1146–1151, 2018.\\n[20] F. Jin, E. Dougherty, P. Saraf, Y. Cao, and N. Ramakrishnan, “Epidemio-\\nlogical modeling of news and rumors on Twitter,” in Proc. 7th Workshop\\nSocial Netw. Mining Anal., 2013, pp. 1–9.\\n[21] P. Zhang, H. Ran, C. Jia, X. Li, and X. Han, “A lightweight propagation\\npath aggregating network with neural topic model for rumor detection,”\\nNeurocomputing, vol. 458, pp. 468–477, Oct. 2021.\\n[22] A. Silva, Y. Han, L. Luo, S. Karunasekera, and C. Leckie, “Propaga-\\ntion2Vec: Embedding partial propagation networks for explainable fake\\nnews early detection,” Inf. Process. & Manage., vol. 58, no. 5, 2021,\\nArt. no. 102618.\\n[23] Z. Wu, D. Pi, J. Chen, M. Xie, and J. Cao, “Rumor detection based on\\npropagation graph neural network with attention mechanism,” Expert\\nSyst. Appl., vol. 158, 2020, Art. no. 113595.\\n[24] X. Yang, Y. Lyu, T. Tian, Y. Liu, Y. Liu, and X. Zhang, “Rumor detection\\non social media with graph structured adversarial learning,” in Proc. 29th\\nInt. Conf. Int. Joint Conf. Artif. Intell., 2021, pp. 1417–1423.\\n[25] Q. Huang, C. Zhou, J. Wu, L. Liu, and B. Wang, “Deep spatial–temporal\\nstructure learning for rumor detection on Twitter,” Neural Comput. Appl.,\\nvol. 35, no. 18, pp. 1–11, 2020.\\n[26] Y. Zhang and Q. Yang, “A survey on multi-task learning,” IEEE Trans.\\nKnowl. Data Eng., vol. 34, no. 12, pp. 5586–5609, Dec. 2022.\\n[27] R. Collobert and J. Weston, “A uniﬁed architecture for natural language\\nprocessing: Deep neural networks with multitask learning,” in Proc. 25th\\nInt. Conf. Mach. Learn., 2008, pp. 160–167.\\n[28] L. Wu, Y. Rao, H. Jin, A. Nazir, and L. Sun, “Different absorption from\\nthe same sharing: Sifted multi-task learning for fake news detection,”\\n2019, arXiv:1909.01720.\\n[29] N. Bai, F. Meng, X. Rui, and Z. Wang, “A multi-task attention tree\\nneural net for stance classiﬁcation and rumor veracity detection,” Appl.\\nIntell., vol. 53, no. 9, pp. 10715–10725, 2023.\\n[30] M. Cheng, S. Nazarian, and P. Bogdan, “VRoC: Variational autoencoder-\\naided multi-task rumor classiﬁer based on text,” in Proc. Web Conf.,\\n2020, pp. 2892–2898.\\n[31] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 30, no. 1, pp. 261–272, 2017.\\n[32] C. Castillo, M. Mendoza, and B. Poblete, “Information credibil-\\nity on Twitter,” in Proc. 20th Int. Conf. World Wide Web, 2011,\\npp. 675–684.\\n[33] F. Yang, Y. Liu, X. Yu, and M. Yang, “Automatic detection of rumor on\\nSina Weibo,” in Proc. ACM SIGKDD Workshop Mining Data Semantics,\\n2012, pp. 1–7.\\n[34] J. Ma, W. Gao, Z. Wei, Y. Lu, and K.-F. Wong, “Detect rumors using\\ntime series of social context information on microblogging websites,” in\\nProc. 24th ACM Int. Conf. Inf. Knowl. Manage., 2015, pp. 1751–1754.\\n[35] R. Pandey, A. Kumar, J. P. Singh, and S. Tripathi, “Hybrid attention-\\nbased long short-term memory network for sarcasm identiﬁcation,” Appl.\\nSoft Comput., vol. 106, 2021, Art. no. 107348.\\n[36] R. Pandey and J. P. Singh, “BERT-LSTM model for sarcasm detection\\nin code-mixed social media post,” J. Intell. Inf. Syst., vol. 60, no. 1,\\npp. 235–254, 2023.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='Abstract\\nIn this paper, we present a novel deep Siamese network with a multi-scale hybrid feature extraction architecture, named DSN-STC\\n(Deep Siamese Network for Short Text Clustering), that significantly improves the clustering of short text. A key innovation of our\\napproach is a specialized transformation mechanism that maps pre-trained word embeddings into cluster-aware text\\nrepresentations. In this new latent space, the proposed model minimizes the overall overlapping between clusters while improving\\nthe cohesion within each cluster. This results in considerable improvements in clustering performance. Since short texts inherently\\ncontain both sequential context and localized patterns within their limited context, in this paper a hybrid approach is used by\\ncombining both recurrent layers and multi-scale convolutional neural networks to maximize the extractable feature sets from their\\nlimited context. This architecture allows us to capture the sequential features and local dependencies by recurrent layer and\\nconvolutional layers respectively which leads to generating a more accurate and rich representation for each short text. To evaluate\\nour architecture and because our main focus is on clustering Persian short text, several experiments are conducted in which the\\nresults show that the DSN-STC outperforms other approaches in clustering accuracy (ACC) and normalized mutual information\\n(NMI) metrics. Also to further test the proposed architecture’s generalizability and adaptability in other languages, DSN-STC is\\nevaluated on 2 English benchmark datasets where it consistently outperformed previous approaches in both metrics. These results\\nhighlight the model’s ability to learn robust and cluster-aware feature representations that are highly useful for effective short text\\nclustering.\\nCitation: Molaei M, Feizi-Derakhshi M-R, Balafar M-A, Tanha J (2026) DSN-STC: Leveraging Siamese networks for\\noptimized short text clustering. PLoS One 21(1): e0335709. https://doi.org/10.1371/journal.pone.0335709\\nEditor: Qionghao Huang, Zhejiang Normal University, CHINA\\nReceived: June 12, 2025; Accepted: October 14, 2025; Published: January 2, 2026\\nCopyright: © 2026 Molaei et al. This is an open access article distributed under the terms of the Creative Commons\\nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author\\nand source are credited.\\nData Availability: Data Availability Statement: The dataset generated during the current study is available in the GitHub\\nrepository, https://github.com/m-molaei/DSN-STC.\\nFunding: The author(s) received no specific funding for this work.\\nCompeting interests: The authors have declared that no competing interests exist.\\nIntroduction\\nThe exponential growth of textual documents available on the Internet in recent years has significantly increased, especially with\\nthe advent of social media platforms. With mobile devices and Internet technologies advancing quickly, users have become\\nmotivated to search for information, communicate with their peers, and share opinions and thoughts on social media platforms like\\nTwitter, Instagram, Facebook and search engines such as Google. The sheer amount of short text generated daily from these\\nplatforms leads to a large volume of overall unstructured data.\\nShort texts are characterized by their brevity, often lacking sufficient context, making knowledge extraction challenging. Despite\\ntheir brevity, short texts are rich in information and play a crucial role in numerous applications, including information retrieval,\\nsentiment analysis, and topic detection. However, their unstructured nature presents significant challenges for data analysis,\\nparticularly in clustering, where the goal is to automatically identify valuable patterns within large collections of text [1].\\nAmong the various data-mining techniques, clustering stands out as an essential method for analyzing short text corpora.\\nClustering short texts can be highly beneficial across diverse fields like information retrieval. For example, clustering can group\\nsimilar short texts in an unsupervised manner, facilitating tasks such as topic detection in short news texts. This automation reduces\\nthe need for human intervention, saving time, cost, and resources, which are typically required for manual labeling. However,\\nclustering short texts is particularly challenging due to their chaotic nature, often containing noise, slang, emojis, misspellings,\\nabbreviations, and grammatical errors [1]. Additionally, the short length of these texts exacerbates issues related to data sparsity,\\nlimited context, and high-dimensional representation. Standard clustering techniques, such as k-means [2] or DBSCAN [3], often\\nstruggle to accurately group short texts because these methods rely heavily on measuring similarity or distance between data\\npoints and depend on accurate text representations [4]. When applied directly to short text corpora, traditional techniques tend to\\nDSN-STC: Leveraging Siamese networks for optimized short\\ntext clustering\\nPublished: January 2, 2026\\nhttps://doi.org/10.1371/journal.pone.0335709\\nMahdi Molaei, Mohammad-Reza Feizi-Derakhshi \\n, Mohammad-Ali Balafar, Jafar Tanha\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n1/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='perform poorly, as the sparse and high-dimensional feature vectors generated by standard text representation methods like term\\nfrequency-inverse document frequency (TF-IDF) or bag of words (BoW) [5] are less effective in capturing meaningful distances\\nbetween data points [6].\\nTo address these challenges, dimensionality reduction is often employed as an essential step in the short text clustering (STC)\\nprocess. One notable advancement in this area was the introduction of Deep Embedded Clustering (DEC) [7], which utilized an\\nautoencoder (AE) network for dimensionality reduction before applying k-means clustering. This approach marked a significant step\\nforward in tackling the high-dimensionality problem inherent in short text representation. However, despite these advancements,\\nsignificant challenges persist. One notable issue is that the dimensionality reduction achieved through AE networks can\\ninadvertently increase overlap between clusters, potentially degrading the overall clustering performance.\\nRecognizing the limitations of existing single‐branch and purely unsupervised schemes, this paper introduces a sophisticated\\nSiamese‐based architecture that jointly tackles both dimensionality reduction and cluster separability within a unified training\\nframework. Siamese networks, first introduced in 1994, are typically used for tasks involving similarity detection [8]. These networks\\nconsist of two identical subnetworks with shared parameters, which are trained simultaneously to generate outputs for two input\\ninstances. The network then updates its weights based on whether the two inputs belong to the same class, aiming to reduce the\\ndistance between embeddings of similar texts and increase the separation between those of different classes by a margin m. To\\nadvance this paradigm, our proposed DSN-STC integrates a hybrid feature extraction framework that jointly combines two\\ncomplementary branches: a recurrent neural network to model long-range contextual dependencies and a multi-scale convolutional\\nmodule to capture local n-gram patterns across varying granularities. The extractable features from the limited context of short text\\nare maximized by this architecture.\\nBeyond its architectural innovations, DSN-STC learns a mapping from word embeddings into text representations \\n that\\nprojects high-dimensional input vectors into a dense, cluster-aware latent space. The contrastive loss not only ensures tight intra-\\ncluster cohesion and wide inter-cluster gaps, but also implicitly encourages the network to emphasize features with strong cluster-\\nlabel correlations. Theoretically, this encourages the network to identify and amplify key lexical or syntactic features such as\\ndomain-specific keywords or phrase structures, that carry the highest mutual information with respect to cluster labels. In this way,\\nDSN‐STC not only reduces representational dimensionality but also systematically mitigates cluster overlap thereby resolving two\\nof the principal challenges identified in prior short-text clustering research.\\nIn addition, we observed that texts with fewer than 10 tokens, after preprocessing, often lack sufficient information to be encoded\\ninto meaningful representations for clustering purposes. As a result, the dataset is filtered to include only texts with a length\\nbetween 10 and 30 tokens. This range was chosen to focus on short texts while still retaining enough content for effective\\nclustering. By filtering out extremely short texts, it is ensured that our method could produce more meaningful representations,\\nleading to more accurate clustering results. Furthermore, although this study concentrates on Persian short texts, the DSN-STC\\nwas also evaluated on English corpora and found to yield similarly strong gains. This cross‐linguistic evaluation confirms that our\\narchitecture is not tied to a single language’s characteristics but generalizes robustly across different linguistic settings.\\nOur extensive evaluations demonstrate that the proposed approach significantly outperforms previous methods, achieving superior\\nclustering performance.\\nThe main contributions of this paper are:\\n1. Contrastive Siamese Pre-trained Word Embeddings Transformer for Learning Cluster-Aware Text Representations: A novel Siamese‐network\\narchitecture has been formulated to learn dense, cluster‐aware representations for short texts. By jointly optimizing a contrastive loss, the model provably\\nminimizes intra‐cluster cohesion while maximizing inter‐cluster margins in the new latent space. This theoretical design ensures that high‐dimensional, sparse\\ntext inputs are mapped into a lower‐dimensional latent space where cluster overlap is rigorously controlled, leading to more separable and robust clusters.\\nEvaluation results confirm that the learned margin satisfies bounds on cluster cohesion and separation, extending prior work on contrastive representation\\nlearning.\\n2. Multi-Scale Recurrent–Convolutional Fusion for Rich Text Representations: A novel hybrid architecture is presented that fuses bidirectional sequential\\nfeatures with multi-scale n-gram dependencies through parallel processing paths. In this design, one path employs recurrent units to model long-range\\ncontextual interactions, while the other leverages convolutional filters of varying kernel sizes to extract localized n-gram patterns at multiple granularities. This\\nhigher-dimensional basis enables the model to implicitly identify and amplify those token sequences or phrase structures that carry maximal mutual\\ninformation and can reflect the subject of text better. In addition these rich representations are further refined via contrastive training, the resulting the new\\nlatent space becomes both dense and highly discriminative. Empirically, this synergy between long-range dependency modeling and localized pattern\\ndetection yields a more informative representation space and leads to significant improvements in clustering accuracy.\\n3. Focus on Effective Text Length for Clustering: An optimal token‐count window of 10–30 was determined and applied to ensure that each document\\ncontains enough semantic information while excluding outliers, excessively short texts lacking context or overly long passages that may introduce noise. Texts\\noutside this range were removed, resulting in a corpus of concise yet contextually rich inputs. This filtering strategy produced more informative representation\\nand yielded consistent gains in clustering accuracy and cluster coherence which is shown in experiments.\\nThe remainder of this paper is organized as follows. The Related Work section surveys prior work in short-text clustering, grouping\\nmethods into several categories and highlights the gaps that our DSN-STC addresses. The Methods section details the proposed\\nDSN-STC architecture, including its Siamese network design, multi-scale hybrid feature extraction architecture, and contrastive-\\nloss training procedure. The Experiments section describes our experimental setup including datasets, preprocessing,\\nhyperparameters (Table 1), evaluation metrics, and present results across empirical evaluations, ablation studies, statistical\\nvalidation, and cross-linguistic tests. The Discussion section discusses key findings, strengths, and limitations of DSN-STC. Finally,\\nthe Conclusions and Future Works section concludes the paper and outlines directions for future work.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n2/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='Table 1. Hyperparameter settings for DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t001\\nRelated work\\nThe field of short‐text clustering has evolved rapidly, encompassing a variety of deep‐learning architectures and representation\\nstrategies. This section reviews four key categories of prior work, including deep embedded clustering, contrastive and Siamese\\napproaches, transformer‐based clustering, and hybrid recurrent–convolutional models, and then highlights how DSN-STC\\nadvances beyond each.\\nDeep Embedded Clustering (DEC) was one of the pioneering approaches to introduce a deep learning-based method in the field of\\nclustering [7]. DEC uses an autoencoder (AE) network to extract feature representations, which are crucial for overcoming the\\nlimitations of traditional clustering techniques that struggle with high-dimensional and sparse data, such as short texts. In this\\napproach, feature representation learning and clustering are performed jointly by combining the autoencoder with the k-means\\nalgorithm. This approach demonstrated superior performance, outperforming previous methods in both image and textual data\\nclustering tasks. Following the introduction of Deep Embedded Clustering (DEC), many classic methods have been proposed for\\nthe advancement of the deep learning-based text clustering field. Examples include Improved Deep Embedded Clustering (IDEC)\\n[9], Short Text Clustering with SIF Embeddings (STC) [10], Deep Clustering Network (DCN) [11], and DEC with Data Augmentation\\n(DEC-DA) [12], each of which introduce their own methodology for improving the original method, focusing on different components\\nof the clustering method. These approaches share a common thread of jointly learning feature representations and cluster\\nassignments, but they differ in their specific implementations and each of these methods aimed to address specific challenges in\\nclustering tasks, such as better handling of data sparsity, improving feature representation, or enhancing the separation between\\nclusters. These models consistently demonstrated improved performance across various datasets by introducing more\\nsophisticated techniques for joint learning of feature representations and cluster assignments. They showed notable success in\\nclustering both image and textual data, further advancing the field of deep learning-based clustering methods. Self-Taught\\nConvolutional Neural Networks for Short Text Clustering also addresses the sparsity of short‐text by first compressing raw features\\ninto binary codes, then training a CNN to fit those codes while learning semantic representations, and finally applying K-means on\\nthe learned features [4]. This framework demonstrated that unsupervised CNNs, guided by auxiliary codes, can effectively capture\\nboth local n-gram patterns and global semantic structure in brief texts.\\nIn the realm of deep learning-based clustering, a notable advancement came with the introduction of a novel approach that\\nleverages deep neural networks to simultaneously learn feature representations and suitable embeddings [13]. This method builds\\nupon the foundation laid by earlier techniques like DEC [7]. At its core, the approach utilizes an autoencoder for dimensionality\\nreduction, followed by a specialized representation network connected to the encoder’s output. The key innovation lies in its\\nobjective to maximize inter-cluster distances by minimizing cross-entropy between pairwise similarity distributions in the\\nautoencoder’s latent space and the representation network’s embedding space. This strategy effectively encourages maximum\\nseparation between clusters, addressing a common challenge in clustering tasks. Evaluated on both textual and image datasets,\\nthe method demonstrated significant improvements over its predecessors, marking a step forward in the field of unsupervised\\nlearning and clustering. The paper [14] tackles the challenges of clustering sparse and high-dimensional short texts. The authors\\npropose two methods using unsupervised autoencoders to enhance text representation. The first, Structural Text Network Graph\\nAutoencoder (STN-GAE), combines text network structure with pre-trained features using graph convolutional networks. The\\nsecond, Soft Cluster Assignment Autoencoder (SCA-AE), adds a soft clustering constraint in the latent space to improve clustering-\\naware representations. Experiments on seven datasets show significant improvements over traditional models, with the SCA-AE\\nachieving up to 14% better accuracy compared to BERT. In [15] Guan et al. introduce a novel framework that overcomes the\\nlimitations of traditional text clustering methods. Recognizing the shortcomings of bag-of-words models in handling high\\ndimensionality, sparsity, and sequential information, the authors propose a deep feature-based approach. The DFTC framework\\nleverages pre-trained text encoders to capture rich semantic representations, reducing the reliance on supervised learning often\\nassociated with deep learning-based clustering. Empirical evaluations demonstrate DFTC’s superior performance over traditional\\nmethods and state-of-the-art models like BERT across diverse datasets. To enhance interpretability, the paper presents the Text\\nClustering Results Explanation (TCRE) model, which provides insights into the semantics of the formed clusters. This contribution\\nsignificantly advances the field of text data analysis by offering both improved clustering accuracy and meaningful explanations.\\nDing and Mei, in [16] proposed a framework that incorporates semantic fusion into the BiLSTM-CNN architecture, which improves\\nshort text classification via a number of local and contextual features. Using the Skip-gram model to embed words, local features\\nare captured through the CNN, while global context is handled by BiLSTM, which improves upon the limitations of more classical\\nmodels. Multiple tests conducted on several datasets demonstrate method’s superior performance relative to other classification\\nmodels and present a robust method that reclines upon a new method of feature extraction, which enhances performance when\\nclassifying short text. The paper [17] presents a novel method for clustering short texts, particularly from social media. The authors\\naddress challenges like data sparsity and non-standard language by integrating BERT, which captures contextual semantics, with\\nthe Biterm Topic Model (BTM) to analyze word co-occurrences and extract topics. Using the DBSCAN algorithm, the proposed\\napproach demonstrates high clustering accuracy and improved text processing quality. This research offers a valuable contribution\\nto short text analysis, especially in environments with sparse and variable data. Paper [18], tackle the issues of feature sparsity and\\nsemantic ambiguity often found in short texts. They introduce the DCAN model, which combines convolutional neural networks\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n3/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='(CNN) with an attention mechanism and dynamic routing to improve the extraction and fusion of features. The model first uses CNN\\nto capture features at various levels of granularity, enriching the text’s semantic representation. It then applies an attention\\nmechanism to prioritize relevant features based on context, followed by dynamic routing to optimize information flow between\\nlayers. Tested on datasets like AG News and SST-2, DCAN demonstrated superior accuracy over existing methods, offering a\\nvaluable advancement in short text classification and natural language processing. Further advancements have focused on\\nimproving the robustness of these models by explicitly addressing noise and outliers in embedding-based clustering using a\\ncombination of Frobenius-based reconstruction with sparsity-promoting or elastic penalties and by incorporating graph-based\\nregularizes to preserve local geometry [19,20]. These approaches improve robustness to Laplacian/outlier noise and enhance the\\ntopology of learned representations, which can be beneficial for sparse or noisy short-text corpora.\\nOther methods have explored hybrid deep–probabilistic models, such as combining autoencoder representations with Gaussian\\nMixture Models (GMMs) [21]. The authors design a framework that enables the joint optimization of both data representations and\\nGMM parameters, which helps achieve more compact clusters and better separation between them. The process starts with an\\nautoencoder extracting features from unlabeled data, which are then modeled by a GMM. What sets this method apart is its\\nadaptive mechanism, where the GMM parameters are continuously refined based on the learned features, allowing the model to\\nalign more accurately with the true data structure. This twofold optimization not only captures the distribution more precisely but\\nalso enhances the clustering by ensuring the Gaussian components distinctly represent separate clusters. Tests on eight datasets\\nshow that the method surpasses several existing advanced clustering techniques, making it particularly effective for unsupervised\\nlearning. This contribution offers a robust solution that combines deep learning with probabilistic modeling to advance clustering\\ntechniques.\\nSiamese networks have ability to learn meaningful representations by analyzing the relationships between document pairs. These\\nnetworks are particularly effective at improving clustering performance, especially when dealing with high-dimensional, sparse data.\\nBy focusing on the similarities and differences between texts, Siamese architectures allow for more nuanced and context-aware\\nclustering results. In the paper [22], authors propose a deep Siamese neural network that addresses the shortcomings of traditional\\nhigh-dimensional text representation methods. This model learns low-dimensional document embeddings by focusing on semantic\\nsimilarities between documents, which improves text classification tasks. Using two sub-networks based on multi-layer perceptrons\\n(MLPs), the network is trained to boost similarity scores for documents within the same category and reduce them for those in\\ndifferent categories. Tested on the BBC news dataset, the method significantly outperforms conventional approaches, offering\\nimportant insights into improving text categorization with advanced neural architectures. The paper [23] proposed a model based on\\nthe Siamese Neural Network (SNN) which is used for calculating semantic similarities between different languages and domains.\\nThey improved classic SNNs using the ReLU activation function and lexical feature sets, leading to improvements in measuring\\nsemantics between short text pairs. The proposed method is evaluated on both English and Portuguese datasets. The results\\noutperformed baseline models and showed proposed method’s effectiveness in cross-lingual and cross-domain text similarity tasks.\\nThis research offers a valuable approach to improving semantic similarity with minimal training data. A novel methodology to boost\\ntextual clustering by integrating semi-supervised learning approaches is proposed in [24]. The authors use pairwise constraints to\\nguide the clustering process. These constraints specify whether pairs of documents should be grouped together (‘must-link’) or kept\\napart (‘cannot-link’). Pairwise clustering performance improved significantly. The method introduced is based on Convolutional\\nSiamese Network (CSN) that can learn a low-dimensional representation of the documents, which captures semantic similarities\\nbetween the documents. The low-dimensional representations are learned and optimized with the pairwise constraints, leading to\\nimprovement in the quality of the clusters. After the representations are learned, a K-Means algorithm is used to cluster the\\ndocuments. The authors demonstrate performance on 8 data sets, and the proposed method shows consistent improved\\nperformance against alternative clustering methods, i.e., MPC-KMeans [25] and standard K-Means preferences. This paradigm has\\nbeen most powerfully realized in recent years through the adoption of transformer architectures. Models such as Sentence-BERT\\n(SBERT) have set a new standard by fine-tuning pre-trained language models on sentence-pair objectives, thereby producing\\nembeddings that capture nuanced semantic relationships with unprecedented accuracy [26].\\nMany architectures for text clustering have been proposed, but relatively few have focused specifically on short texts. A\\nfundamental challenge in these approaches is the generation of cluster-aware, low-dimensional representations that exactly\\nencode the salient features of the original text while improving effective clustering. Although existing methods successfully integrate\\nrepresentation learning with clustering, they frequently employ single-branch networks or rely on unsupervised coding schemes that\\nmay inadequately capture cluster structure in short-text contexts. Transformer-based clustering solutions (e.g., BERT\\u2009+\\u2009K-means)\\nprovide strong baselines but incur substantial computational overhead. In contrast, the proposed method, DSN-STC, is constructed\\nas a semi-supervised, multi-scale hybrid Siamese network combining recurrent and convolutional feature extractors which is trained\\nwith a contrastive loss to map initial embeddings into a cluster-aware space where representations are both lower in dimensionality\\nand more distinctly separated. Consequently, inter-cluster distances are maximized and intra-cluster distances minimized which\\ncauses minimizing clusters’ overlap and more accurate clustering outcomes. This design has the potential to transform large-scale\\nshort-text analysis by producing embeddings that are inherently optimized for clustering tasks.\\nProblem definition\\nLet \\n be an input pair, where \\n and \\n are the word embeddings of short text that are generated from the pre-trained word\\nembedding model. Both \\n and \\n are fed into a shared neural network , parameterized by θ, and then two feature vectors \\nand \\n are generated at the output:\\n(1)\\nFor contrastive supervision we constructed an exhaustive pairwise training set using the available class labels. Concretely, given \\nsentences \\n with ground-truth labels \\n, we built the binary affinity matrix \\n with:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n4/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 4}, page_content='(2)\\nThis procedure generates a complete training set of \\n labeled pairs, where every ordered pair \\n receives a\\ncorresponding label \\n. We use these labeled pairs as the supervised signal for the Siamese contrastive objective: positive pairs\\nare driven to small embedding distance while negative pairs are pushed to lie beyond the margin m. The exhaustive construction\\nmaximizes the available pairwise supervision and provides a dense, stable training signal for contrastive learning in our semi-\\nsupervised setup.\\nInstead of Euclidean distance, we measure similarity via the cosine distance:\\n(3)\\nwhich lies in the interval [0,2] since \\n. In practice (explained later in the experimental setup section) encoder outputs\\nare L2-normalized before computing cosine distance.\\nProposed method\\nThis study did not involve human participants, identifiable human data, or animals; therefore, ethical approval and informed consent\\nwere not required.\\nThe overall architecture of DSN-STC is depicted in Fig 1. Each Siamese branch begins by feeding a sequence of word embeddings\\nfor two input texts. These embeddings are then processed by the hybrid feature extraction architecture, which consists of three\\nparallel modules: a recurrent layer, three convolutional layers with kernel sizes 3, 5, and 7, and a stack of fully connected layers to\\nextract multi-scale, cluster-aware features. Specifically:\\nFig 1. Proposed architecture of DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.g001\\n1. Recurrent Path:\\nA recurrent layer models long-range dependencies and produces hidden states {ℎ}. These states are projected through two dense\\nlayers to yield a dense feature vector \\n.\\n2. Convolutional Path:\\nThree 1D convolutions capture local n-gram patterns at varying granularities. Their outputs are concatenated and passed through a\\ndense layer to form \\n.\\n3. Fusion and Projection:\\nThe vectors r and c are concatenated and mapped by a last dense layer into the final representation \\n. A contrastive-\\nloss head then enforces pulling together similar pairs and pushing apart others by margin m.\\nTheoretically, this design learns a mapping from word embeddings into text representations \\n into a cluster‐aware latent\\nspace, where representations are both lower in dimension and more separable. Each sentence contains both local and long-range\\ndependencies among its words. For example, consider this sentence: “Alice, who had traveled the world, cherishes her childhood\\nmemories.” Capturing and extracting local patterns (e.g., “childhood memories” in this example) requires convolutional layers, while\\nmodeling the relation between “Alice” and “cherishes” demands long-range context which can be extracted by the recurrent layer.\\nThis combination of convolutional and recurrent features equips the network with a multi-faceted latent space in which the\\ncontrastive loss can more effectively discern and reinforce cluster structure. Furthermore, by emphasizing dimensions that capture\\nboth salient local patterns and long-range relational context, intra-cluster cohesion is strengthened, and inter-cluster separation is\\nenlarged. Consequently, these representations become inherently optimized for downstream clustering tasks. Detailed\\nconfigurations for each module follow in the subsections below.\\nThe Siamese network\\nt\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n5/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 5}, page_content='A Siamese neural network consists of two identical subnetworks that share the same weights, architecture, and parameters. These\\nsubnetworks process two inputs in parallel, which are then compared to determine their similarity or distance. Then the outputs,\\nwhich are two final representations of input short text, are evaluated using the contrastive loss function that measures the distance\\nbetween these two representations \\n and tries to maximize or minimize the existing distance between them based on their\\nlabels that show they are in the same cluster or not. The contrastive loss is defined as follows in (4):\\n(4)\\nwhere \\n is the cosine distance (Eq 3) measured between the embeddings, \\n is a binary label which is taken from affinity matrix\\n, m is a margin that defines the minimum distance between dissimilar pairs, ensuring that negative pairs are pushed apart by\\ndistance of m.\\nThe overall loss \\n minimized for a training mini-batch B is the mean of the pairwise loss over all |B| pairs in the batch:\\n(5)\\nProposition 1: Margin-Separation Guarantee in Theory\\nAssume encoder outputs are L2-normalized (as done in training). If training reaches perfect convergence on the training set (i.e.,\\nwhen L\\u2009=\\u20090 for all pairs), the model guarantees:\\n1. Cluster Cohesion: Every positive pair \\n is mapped to the same point, resulting in a cosine distance of zero:\\nTherefore, the maximum intra-cluster distance is \\n.\\n2. Cluster Separation: Every negative pair \\n is separated by a margin of at least m:\\nTherefore, the minimum inter-cluster distance is at least m.\\nIn other words, this is not an assumption but a direct result of convergence. With \\n and the minimum negative-pair distance\\nbeing m, we have a guaranteed inter-cluster gap of at least m in cosine distance.\\nProof Sketch:\\nThe proof follows from the definition of the contrastive loss function. At the point of training convergence, the total loss L is zero.\\nSince the loss function is a sum of non-negative terms, the loss contribution for every individual training pair must also be zero. We\\nanalyze the two cases:\\n1. Positive pairs \\nFor a positive pair, the corresponding loss term is \\n. For the total loss to be zero, this term must be zero:\\nThis demonstrates that all positive pairs are mapped to the same point in the embedding space and have a cosine distance of zero.\\n2. Negative pairs \\nFor a negative pair, the loss term is \\n. For this term to be zero, the argument of the squared max\\nfunction must itself be zero:\\nThis equality holds only if the term inside is non-positive, which means \\n. This directly implies:\\nTheoretical Bounds in the Non-Asymptotic Case:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n6/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='Beyond the idealized convergence guarantee presented in Proposition 1, the contrastive loss function offers a deeper theoretical\\nstrength by providing explicit bounds on cluster quality in the practical, non-asymptotic case. While the analysis above describes\\nthe ideal convergence scenario, in practice, the model converges to a state where the loss for any given pair is bounded by a small\\nresidual value, \\n. In this more realistic scenario, the structure of the loss function allows us to derive formal guarantees on the\\nfinal cluster structure:\\nThis analysis demonstrates that the quality of the final clustering, both its compactness and its separation margin, is directly and\\nmathematically tied to the model’s ability to minimize the training loss, providing a strong theoretical justification for our approach.\\nThe primary goal is to learn a latent space where distances directly reflect text‐pair similarity, transforming pre-trained word\\nembeddings into a cluster-aware text representations. Furthermore, by maximizing inter‐cluster distances and minimizing intra‐\\ncluster distances, this space mitigates overlap and enhances clustering performance, especially for short texts with limited context.\\nThis is particularly useful for extracting rich features and generating cluster-aware representations for short text that has limited\\ncontextual information.\\nIn the following sections, we will explain about details of the internal structure of the proposed DSN-STC, which is designed to\\nextract a rich feature pool and generate text representations based on these extracted features and their corresponding clusters.\\nRecurrent layer: Sequential features and long-term dependencies.\\nRecurrent layers are the sort of neural network that can help with sequential data and are therefore more geared towards these\\ncontext-heavy domains of use, such as natural language processing and time series analysis. Unlike traditional neural networks,\\nwhich treat inputs independently of each other, recurrent layers have loop connections that allow thoughts and memories to pass\\nfrom one step to another. This preserved data from input helps the model to remember existing orders between sequences and\\nworks well in NLP fields like language modeling, speech recognition, and sequence prediction. But still, the vanilla recurrent layer\\nhas some problems which the most important of them is that as the sequence gets longer, the model cannot remember previous\\ninformation. To overcome the shortcomings of vanilla recurrent layer, researchers developed more advanced models like Long\\nShort-Term Memory (LSTM) [27] and Gated Recurrent Units (GRUs) [28]. These networks have modified structure that help them\\nremember information for longer sequences and make them better suited for handling complex tasks.\\nIn the proposed Siamese network, a recurrent layer is used to effectively capture the complex long-range dependencies between\\nwords and model sequential features. As discussed earlier, recurrent layers excel at handling sequential data, making them\\nparticularly well-suited for language tasks. By using a recurrent layer, the flow of text is modeled, capturing not only individual word\\nmeanings, but also how they interact and existing relations in sequence. By using a recurrent layer in the proposed network, it is\\nensured that the model can dynamically learn from the sequential flow of the text and improve its ability to represent complex\\npatterns and relationships. In better words, we use a recurrent layer to extract sequential data and generate an informative\\nrepresentation at first as our first set of features in this network. As there are various types of recurrent layers, each was\\nimplemented and evaluated to determine which extracts the richest features for clustering. The evaluation results will be discussed\\nin the next section.\\nConvolutional layers: Local features and N-grams dependencies.\\nConvolutional neural networks have gained significant success in the field of NLP because of their ability to process and analyze\\ntextual data effectively. While they were originally proposed and designed for image processing, they have been adapted for text by\\nconsidering words or n-grams as spatial features, allowing these networks to extract and learn hierarchical features. This adaption\\nenables convolutional layers to extract local features and relationships that lead to have good performance in NLP tasks such as\\ntext classification and categorization. As aforementioned earlier, one of the key advantages of using convolutional layers for\\nprocessing textual data is the ability of these networks to model and extract existing local patterns between n-grams by applying\\nconvolutional filters on them. Extracting these sets of features can help to model the overall meaning of the text. This helps the\\nmodel understand the meaning of the text in a way that traditional methods, which simply count words without considering their\\norder or relationships, cannot [29]. In addition, convolutional layers can handle dimensionality challenges by using Pooling layers to\\nreduce dimensionality in a way that the main and essential features preserves. This is especially useful for NLP tasks that involve\\ninput sequences of varying lengths [30].\\nThree convolutional layers were used as the second component of the hybrid feature extraction architecture. The core of this\\ncomponent consists of three parallel convolutional layers that each of them uses a distinct kernel size of 3, 5, and 7, respectively.\\nThe main purpose of this multi-scale approach is rooted in the linguistic diversity of short texts. Key information can be encoded in\\nphrases of varying lengths, and a single kernel size would inevitably overlook critical patterns. Using linguistic examples from\\nPersian, we can illustrate the necessity of this approach:\\nBound on Cluster Cohesion: For any positive pair \\n, the loss term is \\n. If we assume \\n, it follows that \\n, which\\nprovides an upper bound on the intra-cluster distance:\\n\\uf054\\nBound on Cluster Separation: Similarly, for any negative pair \\n, the loss is \\n. Bounding this loss by  implies that\\n. Since the max term must be non-negative, this simplifies to \\n, which provides a lower bound on the\\ninter-cluster distance:\\n\\uf054\\nA small kernel (size 3) is effective at capturing tight, meaningful collocations that act as a single semantic unit, such as «بورس اوراق بهادار» (‘stock exchange’).\\n\\uf054\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n7/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='These general examples are representative of the n-gram structures found within used dataset [31], where key topics are often\\ndistinguished by such phrases of varying lengths. By using these kernels in parallel, the model can simultaneously detect these\\ndifferent types of features-from atomic entities to complete phrasal events- and create a richer and more robust representation for\\neach short text. After each convolutional layer, a Pooling layer is used. These layers have two main responsibilities:\\n1). Dimensionality reduction that helps to computational cost efficiency\\n2). Translation invariance, enhancing the model’s robustness to variations in word positions. In other words, Pooling layers reduce the spatial dimensions of\\nthe feature maps generated by convolutional layers which causes the model to focus on the most salient features which not considering the minor variations in\\ninput. This characteristic is particularly beneficial in NLP, where the same meaning can be conveyed through different word arrangements [30].\\nIn the last step, the feature vectors generated by each network are concatenated together. This concatenation process leads to\\ngenerating a single comprehensive vector for each input short text, that represent a multi-scale encapsulation of features from\\ndifferent contextual windows.\\nThis feature extracting process is design to complement the recurrent component of the proposed architecture which is described in\\nRecurrent Path subsection. In other words, while the recurrent component is used for extracting sequential dependencies, the\\nconvolutional component focuses on extracting local features and patterns between n-gram. This approach will allow us to create a\\nrich feature set for generating the final representations.\\nFully connected layers: Feature concatenation and dimensionality reduction.\\nThe third component of the proposed hybrid feature extraction architecture consists of several fully connected (FC) layers. These\\nlayers have an important role in efficiently integrating and refining features. The main characteristics of these layers can be\\nexplained in the following points:\\n1). Concatenation of feature vectors: These FC layers serve to fuse the rich feature sets that are extracted in previous components. This fused representation\\nintegrates sequential-based features extracted by recurrent component and local n-gram dependencies extracted by convolutional component.\\n2). Efficient dimensionality reduction: During feature fusion, these layers progressively reduce the dimensionality of the fused vector. This process is performed\\nin a learned manner as opposed to a direct way which can cause the loss of important features. In other words, by learning to fuse and compress features\\nrather than applying a direct reduction, the model can preserve the most salient information while yielding a dense final representation.\\nClustering text\\nUpon completion of training, the Text Embedding Encoder learns a cluster‐aware mapping from word embeddings into text\\nrepresentation and becomes a robust representation generator for input texts. It can then be applied to any short text to produce\\nhigh‐quality, cluster‐discriminative representations. By emphasizing features that distinguish each cluster, the encoder enhances\\nintra‐cluster cohesion and enlarges inter‐cluster separation, thereby improving overall clustering performance. In the subsequent\\nclustering phase, these representations are supplied to a clustering algorithm (which is K-means in our implementation) to partition\\nthe texts into clusters.\\nExperiments\\nThis section presents the empirical evaluation of the proposed DSN-STC model. We first describe the experimental setup\\n(datasets, pre-processing, and evaluation metrics) and the implementation details, then report the results of several experiments\\ndesigned to evaluate model performance, justify architectural choices, and compare DSN-STC against state-of-the-art baselines.\\nExperimental setup\\nAll experiments were implemented in Python using the TensorFlow framework [32]. The DSN-STC model was trained using the\\nhyperparameter settings detailed in Table 1, which were kept consistent across all relevant experiments unless otherwise specified.\\nThe final clustering of the learned embeddings was performed using the K-means algorithm, as implemented in the Scikit-learn\\nlibrary. For evaluation purposes, the number of clusters, K, was set to the number of ground-truth classes in each dataset. While\\nthe standard K-means algorithm minimizes squared Euclidean distance, our model’s contrastive loss function operates on cosine\\ndistance. To ensure theoretical consistency between the training objective and the clustering metric, a critical normalization step\\nwas employed. During training, the cosine similarity calculation itself involves L2-normalizing the feature vectors (ensuring each has\\na Euclidean norm of 1) before comparison. Correspondingly, after the embeddings were generated from the trained model, we\\napplied an explicit L2 normalization step to all embedding vectors (ensuring each vector has a Euclidean norm of 1) before feeding\\nthem to the K-means algorithm. This projects all embeddings onto a unit hypersphere, a space where Euclidean distance becomes\\na monotonic function of cosine distance. Consequently, the rank-ordering of distances is preserved, and minimizing one metric is\\nequivalent to minimizing the other, thus formally aligning the clustering process with the learning objective.\\nDataset\\nAs our main goal in this paper is short text clustering in the Persian language we used the Sep_TD_Tel01 dataset [31] which is a\\ncomprehensive Persian text collected from Telegram. This dataset was collected without specific restrictions such as keyword\\nfiltering that makes it a suitable sample of natural data stream from social media. This dataset has been used in other studies on\\nA medium kernel (size 5) can encompass a more complete event or short phrasal topic, like «افزایش قیمت سکه در بازار» (‘increase in coin price in the market’).\\n\\uf054\\nA larger kernel (size 7) is necessary to capture longer-range dependencies within a single clause, where the key relationship spans several words, such as\\nin «بانکی را اعالم کرد بانک مرکزی نرخ بهره بین» (‘The Central Bank announced the interbank interest rate’).\\n\\uf054\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n8/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 8}, page_content='the Persian language including NER [33], event detection [34], text clustering, and topic detection [35,36] which shows its usage\\nand importance in Persian as a low-resource language.\\nIn this dataset, for collecting data, a message collector system was developed at ComInSys (Computerized Intelligence Systems)\\nlab that gathers text from all channels and groups. Approximately 23% of the messages were assigned topic labels, originally\\ncovering 75 distinct clusters which these data were used. After applying constraints that limited texts to 10–30 tokens, the number\\nof clusters was reduced to 44. The Sep_TD_Tel01 dataset was randomly split into train, validation, and test (the ratio is reported at\\nTable 1) using random_seed\\u2009=\\u200973. This seed controlled both data shuffling and model initialization for all experiments. In addition,\\nmore detailed information about the dataset can be found in Table 2.\\nTable 2. Summary of Dataset [31].\\nhttps://doi.org/10.1371/journal.pone.0335709.t002\\nData pre-processing.\\nFor pre-processing text in the dataset, as they contain a variety of noise like typos, emojis, URLs, and other artifacts, several pre-\\nprocessing techniques [37] were used to remove these unnecessary elements from text and clean them while preserving\\ninformative content. The specific steps included text normalization, such as converting any embedded English/Latin characters to\\nlowercase, and noise removal, where URLs, HTML tags, user mentions, and emojis were removed. To preserve the original signal,\\nno stop-word removal or spell correction was performed. Also for tokenization, we used a custom tokenizer developed at the\\nComInSys lab, which is specifically designed for the morphological nuances of the Persian language. This pre-processing helps to\\npreserve informative content. As explained earlier, token-length constraints are applied to text that leads to deleting those that have\\nless than 10 or more than 30 tokens. Figs 2 and 3 show the number of tweets and their respective after preprocessing and\\nnormalization steps. After pre-processing, the resulting clean texts served as the basis for generating the different input\\nrepresentations used in this study. For the experiments utilizing TF-IDF, vectors were generated using a vectorizer configured with\\nsublinear term frequency scaling (sublinear_tf\\u2009=\\u2009True) and a vocabulary limited to the top 300 most frequent features (max_features\\u2009\\n=\\u2009300). Min-max normalization is then performed on these embeddings to reduce the variance. This normalization helps accelerate\\ntraining and improves convergence by scaling the data to a common range. The resulting embeddings are fed into the model as\\ninput.\\nFig 2. Tweet counts per cluster before pre-processing.\\nhttps://doi.org/10.1371/journal.pone.0335709.g002\\nFig 3. Tweet counts per cluster after pre-processing.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n9/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 9}, page_content='https://doi.org/10.1371/journal.pone.0335709.g003\\nEvaluation metrics\\nTwo metrics are used to evaluate the proposed method and compare it with other approaches in the text clustering field:\\nUnsupervised Clustering Accuracy (ACC) and Normalized Mutual Information (NMI).\\n1. Unsupervised clustering accuracy (ACC): This metric measures the correspondence between assigned clusters and ground-truth cluster labels. The ACC\\nformally is defined as follows:\\n(6)\\nWhere:\\n2. Normalized mutual information (NMI): For label set T and cluster set C, NMI is defined as:\\n(7)\\nWhere:\\nExperimental results\\nExperiment1: Performance of different recurrent layers in the DSN-STC.\\nIn this experiment, we aim to determine the most effective layer for the recurrent component of the proposed architecture. To this\\nend, different types of recurrent layers including Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional\\nLSTM (Bi-LSTM), and Bidirectional GRU (Bi-GRU) were implemented and to determine which of these layers can perform better in\\nour short text clustering context. The comparative results are presented in Table 3:\\nTable 3. Comparative Results of Various Recurrent Layers in the Recurrent Component of the DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t003\\nThe results presented in Table 3 indicate that the Bi-LSTM layer is the most suitable choice for our recurrent component. By\\nprocessing each input sequence in both forward and reverse directions, the Bi-LSTM effectively doubles the contextual window\\navailable at every time step, enabling the hidden state to incorporate information from both preceding and succeeding tokens. From\\na theoretical standpoint, this bidirectionality enhances the representational capacity of the recurrent subspace: under the framework\\nof sequence modeling, combining forward and backward state vectors increases the expressive power of the network, permitting it\\nto approximate a broader class of sequence‐to‐representation functions. Furthermore, the gated architecture of the LSTM mitigates\\nvanishing‐gradient issues, ensuring that long‐range dependencies (which is critical for disambiguating short texts) are retained in\\nthe learned state dynamics. These properties collectively yield richer, more nuanced feature sets, as necessary to resolve the\\ninherent brevity and lexical ambiguity of short-text inputs. Accordingly, we adopt the Bi-LSTM as the recurrent module in our final\\nDSN-STC configuration.\\nExperiment2: Effect of contrastive‐loss margin.\\nIn Experiment 2, the margin hyperparameter m in Eq 2 was varied from 0.0 to 2.0 in steps of 0.1. For each margin value, DSN-STC\\nwas trained for 200 epochs with early stopping on validation loss (patience\\u2009=\\u200910). The resulting ACC and NMI were computed on\\nboth training and test splits and the final results are presented in Table 4.\\nN is the total number of data points\\n\\uf054\\ny  is the true cluster label of the i-th data point.\\n\\uf054\\ni\\n is the assigned cluster to the i-th data point\\n\\uf054\\nδ(x,y) is an indicator function equaling 1 if x\\u2009=\\u2009y and 0 otherwise\\n\\uf054\\n illustrates a permutation function that maps each assigned cluster label \\n to the equivalent true cluster label using the Hungarian algorithm [38].\\n\\uf054\\nMI(K, P) is the mutual information among K and P\\n\\uf054\\nE(K) and E(P) is the entropies of K and P respectively.\\n\\uf054\\n is used for normalizing the MI(K, P) to be in range [0, 1]\\n\\uf054\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n10/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 10}, page_content='Table 4. Comparative Results of Different Margin Levels in Contrastive Loss Function for Training the DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t004\\nA clear peak in performance was found when the margin m was set to 1.7–1.8 (train ACC\\u2009=\\u20090.7681; test ACC\\u2009=\\u20090.7669; train NMI\\u2009=\\u2009\\n0.9208; test NMI\\u2009=\\u20090.9207). Performance degrades substantially for margins at the extremes: when m\\u2009≤\\u20090.5, clusters remain\\ninsufficiently separated, yielding low ACC and NMI (under-separation), and when m\\u2009≥\\u20091.9, the enforced separation becomes too\\nstrict, causing gradients to vanish and performance to drop (over-separation). Margins in the intermediate range 0.6\\u2009≤\\u2009m\\u2009≤\\u20091.6\\nproduce only moderate gains, indicating partial cluster separation. In the contrastive loss framework [39], setting the margin m too\\nsmall results in insufficient separation between dissimilar pairs, as only distances within m incur a penalty. Conversely, an\\nexcessively large margin causes most dissimilar pairs to lie beyond m, ceasing to provide gradient updates for optimization.\\nTherefore for all other experiments, we used m\\u2009=\\u20091.7. In addition, Fig 4 plots the effect of the contrastive margin m on clustering\\nperformance (ACC and NMI) and complements Table 4 by visualizing the margin sweep. It highlights the region around m\\u2009=\\u20091.7\\nwhere both ACC and NMI peak and supports our selection of m\\u2009=\\u20091.7 for subsequent experiments.\\nFig 4. Evaluation of ACC and NMI as functions of the contrastive‐loss margin.\\nhttps://doi.org/10.1371/journal.pone.0335709.g004\\nExperiment3: Comparative clustering performance on the persian Sep_TD_Tel01 dataset.\\nTo establish a robust set of state-of-the-art baselines for our comparative analysis, we selected several leading Sentence\\nTransformer (SBERT) models. Given our focus on the Persian language, we specifically chose widely-adopted multilingual variants\\nto ensure broad language coverage and provide a challenging benchmark. All models were sourced from the HuggingFace\\nrepository which is a comprehensive resource for standardized NLP models. The specific architectures, underlying pre-trained\\nlanguage models, and output embedding dimensions for each of the evaluated SBERT variants are detailed in Table 5.\\nTable 5. Details of sentence transformer models.\\nhttps://doi.org/10.1371/journal.pone.0335709.t005\\nA comprehensive evaluation of the proposed DSN-STC model was conducted on the Persian Sep_TD_Tel01 dataset. Following\\ntoken-length filtering during preprocessing, DSN-STC (with margin\\u2009=\\u20091.7) was compared against both standard and new clustering\\ntechniques. For the standard baselines, each was evaluated using four word embedding schemes (TF-IDF, GloVe [40], FastText\\n[41], and ParsBert [42]), while recent deep-clustering algorithms were assessed exclusively using ParsBert embeddings, which are\\npre-trained on Persian text. Clustering performance was quantified via ACC and NMI and evaluated both before and after token-\\nlength filtering to show the effect of the proposed constraint. The results of these comparisons are reported in Table 6.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n11/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 11}, page_content='Table 6. Clustering Performance (ACC, NMI) of Various Implemented Methods on the Sep_TD_Tel01 Dataset, Before and After Token-Length\\nFiltering.\\nhttps://doi.org/10.1371/journal.pone.0335709.t006\\nAs shown in Table 6, imposing the token‐length constraint yielded substantial improvements across all methods, with average\\nabsolute gains of approximately 0.23 in ACC and 0.22 in NMI. One of our key objectives from this experiment is to see if this\\nconstraint can improve the clustering performance or not. Although DSN-STC\\u2009+\\u2009ParsBert already outperformed competing\\napproaches even before filtering, this experiment confirms our first hypothesis: by excluding outlier texts that lack adequate context,\\nclustering coherence is enhanced and overlaps are reduced, leading to marked performance gains for every methods tested.\\nFurthermore, our second hypothesis involved enabling the model to learn a transformation of pre-trained word embeddings into\\ncluster-aware text representation, such that representations in this new latent space can be clustered more easily and accurately\\nthan in the original space. Indeed, after applying our Siamese constraint, DSN-STC\\u2009+\\u2009ParsBert achieved the best scores (ACC\\u2009=\\u2009\\n0.7669; NMI\\u2009=\\u20090.9207). In other words, by optimizing representations for both cluster membership and pairwise similarity,\\nembeddings are mapped into a cluster-aware latent space where detection is significantly more accurate.\\nIn addition to further contextualize the performance of DSN-STC, we established a strong state-of-the-art baseline by clustering\\nsentence embeddings generated from several pre-trained multilingual SBERT models. The best-performing variant, use-cmlm-\\nmultilingual\\u2009+\\u2009K-means, achieved a test Accuracy of 0.60149 and a test NMI of 0.81583 (Table 6). This comparison is highly\\ninstructive and reveals a dual advantage of our proposed architecture. First, while SBERT embeddings are powerful, they are\\ndesigned for general-purpose semantic representation and are not inherently optimized for the specific cluster structure of a\\ndownstream task. Second, and critically for a low-resource language, these multilingual models exhibit a known weakness on\\nmorphologically rich languages like Persian. Their shared BPE vocabularies frequently split Persian morphemes across subword\\nboundaries, which can obscure root forms and impede the learning of coherent representations for nuanced constructs, a challenge\\nnoted in prior work [45]. In contrast, DSN-STC is architected to address both deficits. The substantial performance gap between the\\nSBERT baseline and our DSN-STC\\u2009+\\u2009ParsBert model empirically demonstrates the primary contribution of our work. It confirms that\\nour architecture’s advancement stems not only from its contrastive objective successfully learning a cluster-aware latent space, but\\nalso from its ability to leverage a language-specialized encoder (ParsBert) effectively and overcome the tokenization and\\nrepresentational challenges that general-purpose multilingual models face.\\nThe effectiveness of the token-length constraint and the overall superiority of our model are demonstrated in the results from\\nExperiment 3, which are visualized in Figs 5 and 6. These figures display paired bar charts of ACC and NMI for each method,\\nillustrating the results before and after applying the constraint. Consistent improvements in both metrics were observed across all\\nembedding techniques following filtering constraint. Notably, DSN-STC\\u2009+\\u2009ParsBert achieved the largest improvements, with ACC\\nincreasing from 0.4604 to 0.7669 and NMI from 0.6581 to 0.9207. Also, these figures also allow for a direct comparison against\\nstrong transformer-based baselines. Even the best-performing SBERT model (use-cmlm-multilingual) achieved a final test accuracy\\nof only 0.60149, substantially underperforming all variants of our DSN-STC model.\\nFig 5. Comparison of ACC Metric Before and After Applying Constraints Across Different Methods.\\nhttps://doi.org/10.1371/journal.pone.0335709.g005\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n12/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 12}, page_content='Fig 6. Comparison of NMI Metric Before and After Applying Constraints Across Different Methods.\\nhttps://doi.org/10.1371/journal.pone.0335709.g006\\nImportantly, DSN-STC outperformed competing methods within each embedding technique, underscoring its robustness in learning\\neffective mapping from word embeddings into text representations. However, the intrinsic quality of those embeddings still\\ninfluences the final latent space: higher-quality original embeddings yield more powerful cluster-aware representations. The charts\\nprovide a concise visual summary of each method’s comparative performance before and after applying filtering constraint.\\nMoreover, DSN-STC already led the field even before filtering, confirming that (1) token-length constraints enhance clustering by\\nremoving uninformative texts and (2) the DSN-STC architecture’s ability to learn cluster-aware representation facilitates more\\naccurate cluster detection.\\nExperiment4: Cross-linguistic generalizability.\\nAlthough our main focus of this study is to propose a clustering architecture for the Persian language, we also evaluate the\\nrobustness of the proposed method on two English benchmark datasets that are commonly used in clustering approaches,\\nincluding 20newsgroups [46] and AGnews. This experiment allows us to test and evaluate the generalizability and adaptability of\\nour approach in other languages and prepare good comparative results with other models. Table 7 summarizes these results:\\nTable 7. Comparison of DSN-STC Performance (ACC, NMI) Against Various Reported Methods on Standard English Benchmark Datasets.\\nhttps://doi.org/10.1371/journal.pone.0335709.t007\\nAs shown in Table 7, the proposed method consistently outperforms other clustering approaches on English datasets. This not only\\nhighlights the strength and flexibility of the proposed method but also shows the potential of it to improve clustering in other\\nlanguage contexts. The improvements gained can confirm this experiment’s hypothesis that our method can perform better on\\nclustering text not only in Persian but also in other languages. The reason for this is the ability of our method to extract and create\\nrich feature sets from the limited context of the text, which leads to constructing richer cluster-aware representations for text.\\nExperiment5: Component ablation study.\\nTo understand the individual contributions of each architectural component within DSN-STC, we conducted an ablation study that\\nsystematically removes one component at a time and measures its impact on clustering performance. Table 8 reports each variant\\nof DSN-STC’s ACC and NMI alongside their absolute decreases (ΔACC, ΔNMI) relative to the full DSN-STC model. In this ablation\\nstudy we sought to answer the following research questions:\\nTable 8. Impact of Component Ablation on DSN-STC Performance (Sep_TD_Tel01).\\nhttps://doi.org/10.1371/journal.pone.0335709.t008\\n1. RQ1: Which feature-extraction branch (recurrent vs. convolutional) contributes more to overall clustering performance?\\n2. RQ2: How critical is the multi-scale nature of the convolutional branch (kernel sizes [3, 5, 7] vs. a single size)?\\n3. RQ3: To what extent does the contrastive-loss objective improve cluster separation compared to a standard binary-cross-entropy loss?\\nAs it can be seen in Table 8, every removal causes a clear drop in performance:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n13/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='1. RQ1 finding: Removing the recurrent branch yields ΔACC\\u2009=\\u2009–0.1642 and ΔNMI\\u2009=\\u2009–0.1614, while removing the Conv1D branch yields ΔACC\\u2009=\\u2009–0.1556 and\\nΔNMI\\u2009=\\u2009–0.1504. Both pathways contribute nearly equally, indicating that sequence modeling and local pattern extraction each provide necessary information.\\nThe slightly larger impact of dropping the recurrent branch suggests that long-range dependencies carry marginally more weight in distinguishing clusters of\\nshort text.\\n2. RQ2 finding: Restricting the convolutional branch to a single kernel size incurs the largest drop (ΔACC\\u2009=\\u2009–0.2017; ΔNMI\\u2009=\\u2009–0.1784). This result empirically\\nconfirms our architectural motivation. Multi-scale convolutions capture patterns at varying n-gram windows, from short phrases to longer collocations.\\nRemoving this diversity forces the model to overlook certain granularities of meaning; for instance, a model with only a small kernel might identify key entities\\nbut fail to capture the broader phrasal context that defines a cluster’s topic, as explained by the linguistic examples in the Methods section. This shows that\\ncapturing features across multiple scales is crucial for representing short texts effectively. In other words, giving the model the ability to capture local\\ndependencies in different windows allows it to extract much more discriminative features that can find and highlight the important phrases, helping the model\\nto better model the subject of each text and generate final representations more accurately.\\n3. RQ3 finding: Replacing contrastive loss with binary cross-entropy still degrades performance (ΔACC\\u2009=\\u2009–0.0644; ΔNMI\\u2009=\\u2009–0.0807). While binary cross-entropy\\nstill enforces some degree of separation, its relatively smaller ΔACC compared to the other ablations indicates that architectural components (recurrent\\u2009+\\u2009multi-\\nscale conv) can partially structure the embedding space well. However, the contrastive objective’s explicit push–pull mechanism remains essential for\\nmaximizing inter-cluster margins and minimizing intra-cluster variance, delivering the highest cluster cohesion.\\nThese results confirm that each component of DSN-STC is vital for producing high-quality, cluster-aware representations.\\nExperiment6: Hyperparameter sensitivity analysis.\\nTo further validate our architectural choices and assess the model’s stability, we conducted a sensitivity analysis on three key\\nhyperparameters: the number of units in the recurrent layer, the combination of kernel sizes in the multi-scale convolutional branch,\\nand the number of filters in each convolutional layer. In these experiments, we varied one hyperparameter at a time while keeping\\nall others fixed to their optimal values as listed in Table 6. The comprehensive results of this analysis are presented in Table 9.\\nTable 9. Sensitivity Analysis of Key Hyperparameters for DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t009\\nThe results provide several key insights into the model’s behavior. First, for the Recurrent Units, we observe a clear performance\\npeak at our chosen value of 200. While 100 units also perform well, increasing the capacity to 300 units leads to a notable\\ndegradation in test performance (from 0.7669 to 0.739 ACC), suggesting the onset of overfitting and confirming that 200 units\\nprovides a robust balance between representational power and generalization.\\nSecond, the analysis of Kernel Sizes empirically validates our multi-scale design hypothesis. The [3, 5, 7] configuration significantly\\noutperforms all other variants. The underperformance of configurations focused on exclusively smaller ([2, 3, 4]) or larger ([5, 7, 9])\\nn-grams indicates that the model must capture patterns across a balanced spectrum of linguistic scales, from tight collocations to\\nlonger phrases, to effectively represent the short texts in our dataset.\\nFinally, the sensitivity to the number of Conv1D filters highlights the importance of model capacity. Using 32 filters provides\\ninsufficient representational power, leading to lower performance. Conversely, increasing the filter count to 128 causes a sharp drop\\nin both Test Accuracy and NMI. This is a classic indicator of overfitting, where the overly complex convolutional branch begins to\\nmodel noise rather than generalizable features, validating our choice of 64 filters as optimal.\\nOverall, this sensitivity analysis demonstrates that the hyperparameters chosen for DSN-STC are not arbitrary but are located\\nwithin a stable and high-performing region of the parameter space, confirming the robustness of our model’s architecture.\\nExperiment7: Computational cost analysis.\\nTo evaluate the practical viability and scalability of our proposed model, we conducted an analysis of its computational cost,\\nfocusing on training time. For a fair comparison, all experiments were conducted on a Google Colab instance equipped with a 15\\nGB NVIDIA T4 GPU and 12 GB of RAM. We measured the training time for our full DSN-STC model and compared it against its\\nablated variants as well as key transformer-based baselines. The results, presented in Table 10, provide two key insights that\\naddress both the internal cost-benefit of our hybrid design and its external comparison to baselines.\\nTable 10. Training time and performance comparison for DSN-STC and key baselines.\\nhttps://doi.org/10.1371/journal.pone.0335709.t010\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n14/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='First, the analysis of our model’s components is highly informative. The recurrent-only branch (270.39s) is substantially more\\ncomputationally expensive than the convolutional-only branch (114.86s), which is expected due to the sequential nature of recurrent\\noperations. While the full hybrid model (846.38s) requires the most training time, our ablation study (Experiment 5) has already\\ndemonstrated that both branches are essential for achieving optimal clustering performance. This confirms that the combination of\\nthese complementary feature extractors is an effective use of computational resources.\\nSecond, when compared against the strong baselines, DSN-STC demonstrates a highly favorable performance-to-cost trade-off.\\nWhile our model is naturally more computationally intensive than direct clustering on pre-computed embeddings, this increased\\ncost yields a massive improvement in clustering quality—a gain of over 0.16 in Test Accuracy compared to the best SBERT\\nbaseline. This substantial performance leap validates our end-to-end, cluster-aware training approach. It confirms that DSN-STC\\nprovides a state-of-the-art accuracy advantage for a reasonable and justifiable increase in computational cost, making it a viable\\nand effective solution.\\nExperiment8: Statistical validation of DSN-STC improvements.\\nTo rigorously assess whether DSN-STC’s performance gains are statistically reliable, paired t-tests (two-tailed, α\\u2009=\\u20090.05) were\\nconducted on N\\u2009=\\u200910 independent runs for both ACC and NMI. Two sets of comparisons were evaluated:\\n1. Token-Length Constraint Impact: DSN-STC trained on the full dataset versus with token-length filtering.\\n2. Baseline Competitors: DSN-STC (filtered) versus each of six baselines, all using ParsBert embeddings.\\nTable 11 summarizes the t-statistics and corresponding p-values for each comparison. All p-values fall below the 0.05 threshold,\\nconfirming that DSN-STC’s improvements in both ACC and NMI are statistically significant across preprocessing conditions and\\nwhen compared to a broad spectrum of clustering methods.\\nTable 11. Paired t-test for DSN-STC performance (ACC and NMI) on the Sep_TD_Tel01 dataset.\\nhttps://doi.org/10.1371/journal.pone.0335709.t011\\nDiscussion\\nIn this study, we propose a novel architecture for clustering short text that has a specialized Siamese network-based model. The\\nkey innovation of our approach lies in its ability to transform textual data from an initial word embedding space into a cluster-aware\\ntext representation latent space that is more efficient for clustering. This transformation that is learned within a Siamese network\\nemploying multi-scale hybrid feature extraction enables the model to cluster text data more effectively by generating\\nrepresentations that capture rich, cluster-aware features.\\nSeveral experiments were conducted to evaluate the effectiveness of our approach. In the first experiment, different recurrent\\nneural layers were implemented and compared to see that which of them is the most suitable choice that can extract and model\\nsequential dependencies and features better than others. The comparative experiment showed that the Bi-LSTM layer is the most\\nsuitable choice, outperforming other recurrent layers like LSTM, GRU, and Bi-GRU. This result underscores the importance of\\nbidirectionality in extracting richer and more informative features from text sequences.\\nIn Experiment 2, we performed a comprehensive margin sweep over the contrastive‐loss parameter m∈ [0,2]. We observed a\\ncharacteristic unimodal performance curve, with insufficient separation at m\\u2009≤\\u20090.5, excessive separation at m\\u2009≥\\u20091.9, and an optimal\\npeak at m\\u2009=\\u20091.7 that confirms that balanced intra-cluster cohesion and inter-cluster separation are essential for maximizing ACC and\\nNMI. In addition, future work could explore dynamic margin selection strategies to further adapt the contrastive objective during\\ntraining.\\nIn Experiment 3, we compared DSN-STC against both standard baselines and recent, advanced clustering approaches on the\\nSep_TD_Tel01 dataset. We focued on 2 main questions for this experiment: “Does applying token-length constraints improve\\nclustering performance?” and “How does our method compare to previous approaches?”. As shown in Table 6, enforcing the token-\\nlength constraint yielded average gains of 31% in ACC and 27% in NMI. Notably, even before applying these constraints, DSN-STC\\ndelivered improvements of 1.3% in ACC and 2.7% in NMI over competing methods, which shows its intrinsic ability to extract rich,\\ncluster-aware features.\\nThese results demonstrate that DSN-STC effectively learns to map pre-trained word embeddings into a text representation latent\\nspace where cluster overlap is minimized and separability is maximized, thereby facilitating more efficient clustering. In addition, the\\nsuperior performance of DSN-STC\\u2009+\\u2009ParsBert (ACC\\u2009=\\u20090.7669, NMI\\u2009=\\u20090.9207) derives from its capacity to fuse contextualized\\nembeddings with multi-scale hybrid feature extraction. Unlike static embeddings (e.g., GloVe), ParsBert encodes nuanced semantic\\nrelationships in short texts, while our hybrid architecture preserves and combines both long-range dependencies and local n-gram\\npatterns for enhanced cluster discrimination.\\nWhile our primary evaluation targeted Persian text, we also assessed the robustness of DSN-STC on standard English clustering\\nbenchmarks in the experiment 4. These cross-linguistic evaluations demonstrate that our architecture generalizes well beyond\\nPersian, consistently outperforming established clustering methods. Importantly, these results underscore that the quality of the\\nfinal cluster-aware latent space is intrinsically tied to the quality of the initial embedding space. This principle mirrors the findings of\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n15/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='Moslem et al. [48], who generated synthetic bilingual terminology data with an LLM, fine-tuned a machine-translation model on that\\ndata, and then applied LLM-guided post-editing to enforce domain terms, nearly doubling term integration and demonstrating the\\npower of domain-aware data augmentation for specialized tasks. Consequently, applying DSN-STC to different languages demands\\nhigh-quality embedding techniques, especially contextualized models such as BERT or its language-specific variants, to ensure that\\nthe initial representations encode sufficient syntactic and semantic nuance. Indeed, Rezaei et al. [49]demonstrated across eight\\nsentiment‐analysis benchmarks that the combination of deep architectures and carefully selected word embeddings can lead to up\\nto a 15 point accuracy swing, further underscoring the downstream impact of embedding quality in diverse NLP tasks. Also, Wassie\\net al. recently showed in [50] that fine-tuning open-source large language models on domain-specific corpora yields substantial\\naccuracy gains in specialized translation tasks, underscoring the value of domain-tuned contextual embeddings for high-fidelity\\nrepresentation learning. In practice, leveraging good embeddings for each target language is essential to realize the full potential of\\nour Siamese contrastive framework and to achieve comparable performance gains across diverse linguistic contexts.\\nExperiment 5 (the ablation study) evaluated DSN-STC by removing each core component in turn, including recurrent branch,\\nconvolutional branch, multi-scale kernels, and the contrastive-loss head, and measured the resulting ΔACC and ΔNMI. Every\\ndeletion caused a substantial performance drop (up to –0.20 in ACC), confirming that all architectural elements are indispensable\\nfor generating high-quality, cluster-aware text representations.\\nRecurrent layers excel at modeling long-range dependencies and capturing topic coherence across a sequence, while\\nconvolutional layers efficiently detect salient n-gram patterns. Their complementary roles explain why removing either branch\\nsignificantly degrades performance. Moreover, restricting the convolutional path to a single kernel size produced the largest\\ndecrease (ΔACC\\u2009=\\u2009–0.20), which highlights the necessity of multi-scale feature extraction. The reason is short texts often contain\\nphrases of varying lengths, such as “climate change” (2-gram) versus “machine learning in healthcare” (4-gram) that multi-scale\\nkernels can adapt to this variability and extract valuable features from diverse n-gram patterns, whereas a single kernel cannot\\ncapture such linguistic diversity.\\nFinally, replacing the contrastive loss with binary cross-entropy (BCE) resulted in a notable drop as well, underscoring that BCE’s\\nindependent treatment of pairs lacks the structural regularization provided by the contrastive objective, which explicitly enforces\\nboth intra-cluster compactness and inter-cluster margins. This dual push–pull mechanism is critical for learning text representations\\nthat faithfully reflect cluster structure. In addition, Experiment 6 employed paired t-tests (at p\\u2009<\\u20090.05) on ACC and NMI gains,\\nconfirming that all observed improvements, including token-length filtering and architectural design, are statistically significant and\\nnot due to chance.\\nIn addition to previous core performance comparisons and ablation studies, our final experiments were designed to validate the\\nrobustness and practical viability of the DSN-STC architecture. The hyperparameter sensitivity analysis (Experiment 6) confirmed\\nthat our chosen configuration is not fragile; the selected values for recurrent units, kernel sizes, and filter counts reside within a\\nstable performance peak, demonstrating that the model’s strong performance is a robust property of the architecture. Furthermore,\\nthe computational cost analysis (Experiment 7) provided a clear view of the performance-cost trade-offs. It confirmed that DSN-STC\\nprovides a state-of-the-art accuracy advantage for a justifiable increase in training time compared to other methods. It is important\\nto note, however, that our exhaustive pairwise training strategy has a time complexity of \\n with respect to the number of\\ntraining samples N. Consequently, while highly effective for datasets of the scale used in this study, applying DSN-STC to\\nsignificantly larger corpora would likely require more sophisticated training pair construction to maintain computational cost\\nefficiency. Taken together, these final analyses validate that DSN-STC is not only effective but also robust and computationally\\npractical for its target application.\\nA further methodological consideration is the approach to model regularization. The contrastive loss function in Equation (3) does\\nnot include an explicit regularization term, such as L2 weight decay, which is often used to penalize model complexity. In this study,\\nwe instead relied primarily on Early Stopping (with a patience of 10 on the validation loss) as our primary mechanism for preventing\\noverfitting. This technique is a powerful and widely used form of temporal regularization that halts the training process once\\ngeneralization performance on unseen data no longer improves. By selecting the model at its optimal point in the training trajectory,\\nEarly Stopping implicitly prevents the network’s weights from becoming overly specialized to the training set, serving a similar goal\\nto explicit weight decay. Furthermore, the Siamese architecture itself provides a form of structural regularization through its shared\\nweights and encourages generalization. Also, while our results demonstrate that these methods were sufficient for achieving strong\\nperformance, the exploration of explicit weight decay could be a valuable direction for future work.\\nIn the end, it is worth noting a key consideration regarding the scope of this work. While the proposed architecture outperforms\\nother baseline and more advanced recent clustering architectures, the imbalanced nature of the Sep_TD_Tel01 dataset, as seen in\\nFigs 2 and 3, may have influenced the results. Class imbalance and the sparsity of rare classes are known to complicate short-text\\nclustering: minority classes are often under-represented and therefore harder to group reliably. Contrastive and mixup-style\\napproaches have been proposed to mitigate such low-resource challenges [51]. More specifically, prior work has shown that class\\nimbalance can bias algorithms toward majority classes and degrade minority-class recovery in both clustering and classification\\ntasks, and that contrastive objectives may benefit from imbalance-aware modifications (e.g., weighted or asymmetric losses,\\nresampling strategies) to avoid deteriorated performance on underrepresented classes [52,53]. Although some clusters in the used\\nPersian dataset are relatively small, we did not apply class-imbalance mitigation (e.g., weighted loss or resampling) in this study\\nbecause our primary goal was to validate the Siamese hybrid architecture.\\nIn conclusion, the results of our experiments confirm the strength and adaptability of the proposed architecture in learning rich,\\ncluster-aware mapping from word embeddings into text representations. Our model demonstrates both effectiveness and\\nadaptability across different languages, providing a robust solution for short text clustering in diverse linguistic contexts.\\nConclusions and future works\\nIn this work, we proposed a novel architecture DSN-STC, based on a Siamese network specifically designed to improve the\\nclustering of Persian short text. Our main idea was to train a model that can transform word embeddings from an initial space which\\nis generated by a pre-trained model, into cluster-aware text representations where clusters can be detected more easily and\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n16/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='1.\\nView Article\\nGoogle Scholar\\n2.\\nView Article\\nGoogle Scholar\\n3.\\n4.\\nView Article\\nPubMed/NCBI\\nGoogle Scholar\\n5.\\nView Article\\nGoogle Scholar\\n6.\\n7.\\n8.\\nView Article\\nGoogle Scholar\\n9.\\n10.\\n11.\\n12.\\n13.\\n14.\\nimprove the clustering by minimizing the overall overlapping between existing clusters. In other words, the representations of text\\nthat are generated by the proposed architecture can capture both the structural and contextual features of each text as well as\\npreserving and improving the cluster-relevant features. So that our model provides a more accurate representations for clustering\\ntasks. The DSN-STC model employs a Siamese network with multi-scale hybrid architecture, consisting of one recurrent layer and\\nthree convolutional neural networks to extract both sequential and local features from the text respectively. These extracted\\nfeatures are then concatenated through fully connected layers to produce the final representation of the input text. During the\\ntraining phase, the Siamese network takes pairs of inputs and then is trained based on the similarity of their cluster assignments,\\nusing a contrast loss function to update the model parameters. In better words, when two inputs belong to the same cluster, they\\nare considered similar, and the model minimizes the distance between them. Conversely, for inputs from different clusters, the\\nmodel maximizes their distance. This loss function, by minimizing the distances for similar data and maximizing those for dissimilar\\ndata, enables our model to learn high-quality, cluster-aware representations that are well-suited for short text clustering. In fact,\\neach short text within its limited context inherently includes sequential and long-range dependencies and also local n-gram patterns\\nin different windows. By the proposed multi-scale hybrid feature extraction architecture, The proposed multi-scale hybrid feature\\nextractor is designed to maximize the extractable diverse features to highlight the main subject of the text and then give ability to\\nthe model to learn which complementary feature subsets it should use to generate the final text representation based on each\\ncluster. In this way model can learn a cluster-aware mapping from the initial word embedding space into a text representation latent\\nspace and improve the clustering performance. Our experiments show significant improvements in clustering accuracy and NMI\\nmetrics compared to previous methods. Although our main focus was on clustering Persian data, DSN-STC was also evaluated on\\ncommonly used English dataset, where it continued to show significant improvements over other methods. This cross-linguistic\\nperformance highlights the robustness and adaptability of our model for short text clustering tasks.\\nFor future works, we suggest exploring additional pre-trained embedding models to determine whether they can further improve our\\nmodel’s performance. Also using attention mechanisms in the architecture may provide further improvements in clustering because\\nin this way, model can learn better to attend to which set of features and use them for generating the final representations. A\\nparticularly important direction, as noted in our discussion, is the investigation of imbalance-aware training strategies, such as\\nweighted loss functions or resampling techniques, to potentially improve performance on minority clusters. Finally, while our study\\nmanaged the inherent noise of the real-world dataset through pre-processing, a valuable future direction involves leveraging these\\nsignals instead of only removing them. Techniques such as incorporating dedicated emoji embeddings or applying robust spell-\\ncorrection could potentially capture additional semantic cues and probably enhance clustering performance. These avenues\\npresent promising opportunities to build upon the strong foundation established by the DSN-STC model.\\nReferences\\nAhmed MH, Tiun S, Omar N, Sani NS. Short Text Clustering Algorithms, Application and Challenges: A Survey. Applied Sciences. 2022;13(1):342.\\nJain AK. Data clustering: 50 years beyond K-means. Pattern Recognition Letters. 2010;31(8):651–66.\\nEster M, Kriegel H-P, Sander J, Xu X, editors. A density-based algorithm for discovering clusters in large spatial databases with noise. kdd; 1996.\\nXu J, Xu B, Wang P, Zheng S, Tian G, Zhao J, et al. Self-Taught convolutional neural networks for short text clustering. Neural Netw. 2017;88:22–31.\\npmid:28157556\\nSalton G, Buckley C. Term-weighting approaches in automatic text retrieval. Information Processing & Management. 1988;24(5):513–23.\\nLiu K, Bellet A, Sha F, editors. Similarity learning for high-dimensional sparse data. Artificial Intelligence and Statistics. PMLR; 2015.\\nXie J, Girshick R, Farhadi A, editors. Unsupervised deep embedding for clustering analysis. International conference on machine learning. PMLR; 2016.\\nBromley J, Guyon I, LeCun Y, Säckinger E, Shah R. Signature verification using a “siamese” time delay neural network. Advances in neural information\\nprocessing systems. 1993;6.\\nGuo X, Gao L, Liu X, Yin J, editors. Improved deep embedded clustering with local structure preservation. Ijcai; 2017.\\nHadifar A, Sterckx L, Demeester T, Develder C, editors. A self-training approach for short text clustering. Proceedings of the 4th Workshop on\\nRepresentation Learning for NLP (RepL4NLP-2019). 2019.\\nYang B, Fu X, Sidiropoulos ND, Hong M, editors. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. international conference\\non machine learning. PMLR; 2017.\\nGuo X, Zhu E, Liu X, Yin J, editors. Deep embedded clustering with data augmentation. Asian conference on machine learning. PMLR; 2018.\\nDahal P, editor Learning embedding space for clustering from deep representations. 2018 IEEE International Conference on Big Data (Big Data). IEEE;\\n2018.\\nYin H, Song X, Yang S, Huang G, Li J, editors. Representation learning for short text clustering. Web Information Systems Engineering–WISE 2021: 22nd\\nInternational Conference on Web Information Systems Engineering, WISE 2021, Melbourne, VIC, Australia, October 26–29, 2021, Proceedings, Part II 22.\\nSpringer; 2021.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n17/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 17}, page_content='15.\\nView Article\\nGoogle Scholar\\n16.\\n17.\\n18.\\n19.\\nView Article\\nGoogle Scholar\\n20.\\nView Article\\nGoogle Scholar\\n21.\\nView Article\\nGoogle Scholar\\n22.\\nView Article\\nGoogle Scholar\\n23.\\n24.\\n25.\\n26.\\nView Article\\nGoogle Scholar\\n27.\\nView Article\\nPubMed/NCBI\\nGoogle Scholar\\n28.\\nView Article\\nGoogle Scholar\\n29.\\n30.\\nView Article\\nGoogle Scholar\\n31.\\n32.\\n33.\\nView Article\\nGoogle Scholar\\n34.\\nView Article\\nGoogle Scholar\\n35.\\nGuan R, Zhang H, Liang Y, Giunchiglia F, Huang L, Feng X. Deep Feature-Based Text Clustering and its Explanation. IEEE Trans Knowl Data Eng.\\n2022;34(8):3669–80.\\nDing X, Mei Y, editors. Research on short text classification method based on semantic fusion and BiLSTM-CNN. 4th International Conference on\\nInformation Science, Electrical, and Automation Engineering (ISEAE 2022). SPIE; 2022.\\nGuo Y, Leng Y, editors. Research on Short Text Clustering Algorithm Combining BERT and BTM. 2024 7th International Conference on Advanced\\nAlgorithms and Control Engineering (ICAACE). IEEE; 2024.\\nWang Q, Jin M, Yang N, editors. Short text classification model based on dynamic routing and CNN with attention mechanism. Sixth International\\nConference on Computer Information Science and Application Technology (CISAT 2023). SPIE; 2023.\\nDaneshfar F, Soleymanbaigi S, Nafisi A, Yamini P. Elastic deep autoencoder for text embedding clustering by an improved graph regularization. Expert\\nSystems with Applications. 2024;238:121780.\\nDaneshfar F, Saifee BS, Soleymanbaigi S, Aeini M. Elastic deep multi-view autoencoder with diversity embedding. Information Sciences.\\n2025;689:121482.\\nWang J, Jiang J. Unsupervised deep clustering via adaptive GMM modeling and optimization. Neurocomputing. 2021;433:199–211.\\nGharavi E, Veisi H. Improve document embedding for text categorization through deep siamese neural network. arXiv preprint arXiv:200600572. 2020.\\nde Souza JVA, Oliveira LESE, Gumiel YB, Carvalho DR, Moro CMC, editors. Exploiting siamese neural networks on short text similarity tasks for multiple\\ndomains and languages. International conference on computational processing of the portuguese language. Springer; 2020.\\nVilhagra LA, Fernandes ER, Nogueira BM, editors. Textcsn: a semi-supervised approach for text clustering using pairwise constraints and convolutional\\nsiamese network. Proceedings of the 35th Annual ACM Symposium on Applied Computing; 2020.\\nBilenko M, Basu S, Mooney RJ, editors. Integrating constraints and metric learning in semi-supervised clustering. Proceedings of the twenty-first\\ninternational conference on Machine learning; 2004.\\nReimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:190810084. 2019.\\nHochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9(8):1735–80. pmid:9377276\\nCho K. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:14061078. 2014.\\nKim Y, editor. Convolutional Neural Networks for Sentence Classification2014 October. Doha, Qatar: Association for Computational Linguistics.\\nZhang Y, Wallace B. A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification. arXiv preprint\\narXiv:151003820. 2015.\\nRanjbar-Khadivi M, Feizi Derakhshi M, Forouzandeh A, Gholami P, Feizi-Derakhshi A, Zafarani-Moattar E. Sep_TD_Tel01. Mendeley Data.\\n2022;1:10.17632. https://doi.org/10.17632/372rnwf9pc.1\\nAbadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al., editors. {TensorFlow}: a system for {Large-Scale} machine learning. 12th USENIX\\nsymposium on operating systems design and implementation (OSDI 16); 2016.\\nForouzandeh A, Feizi-Derakhshi M-R, Gholami-Dastgerdi P. Persian Named Entity Recognition by Gray Wolf Optimization Algorithm. Scientific\\nProgramming. 2022;2022:1–12.\\nGholami-Dastgerdi P, Feizi-Derakhshi M-R, Salehpour P. SSKG: Subject stream knowledge graph, a new approach for event detection from text. Ain\\nShams Engineering Journal. 2024;15(12):103040.\\nRanjbar-Khadivi M, Akbarpour S, Feizi-Derakhshi M-R, Anari B. A Human Word Association Based Model for Topic Detection in Social Networks. Ann\\nData Sci. 2024;12(4):1211–35.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n18/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 18}, page_content='View Article\\nGoogle Scholar\\n36.\\nView Article\\nGoogle Scholar\\n37.\\nView Article\\nGoogle Scholar\\n38.\\nView Article\\nGoogle Scholar\\n39.\\n40.\\n41.\\nView Article\\nGoogle Scholar\\n42.\\nView Article\\nGoogle Scholar\\n43.\\nView Article\\nGoogle Scholar\\n44.\\nView Article\\nGoogle Scholar\\n45.\\nView Article\\nGoogle Scholar\\n46.\\n47.\\n48.\\n49.\\nView Article\\nGoogle Scholar\\n50.\\nView Article\\nGoogle Scholar\\n51.\\nView Article\\nPubMed/NCBI\\nGoogle Scholar\\n52.\\nView Article\\nGoogle Scholar\\n53.\\nView Article\\nGoogle Scholar\\nZafarani-Moattar E, Kangavari MR, Rahmani AM. Neural Network Meaningful Learning Theory and its Application for Deep Text Clustering. IEEE Access.\\n2024;12:42411–22.\\nMolaei M, Mohamadpur D. Distributed online pre-processing framework for big data sentiment analytics. Journal of AI and Data Mining. 2022;10(2):197–\\n205.\\nKuhn HW. The Hungarian method for the assignment problem. Naval Research Logistics. 1955;2(1–2):83–97.\\nHadsell R, Chopra S, LeCun Y, editors. Dimensionality reduction by learning an invariant mapping. 2006 IEEE computer society conference on computer\\nvision and pattern recognition (CVPR’06). IEEE; 2006.\\nPennington J, Socher R, Manning CD, editors. Glove: Global vectors for word representation. Proceedings of the 2014 conference on empirical methods\\nin natural language processing (EMNLP); 2014.\\nBojanowski P, Grave E, Joulin A, Mikolov T. Enriching Word Vectors with Subword Information. TACL. 2017;5:135–46.\\nFarahani M, Gharachorloo M, Farahani M, Manthouri M. ParsBERT: Transformer-based Model for Persian Language Understanding. Neural Process Lett.\\n2021;53(6):3831–47.\\nHosseini S, Varzaneh ZA. Deep text clustering using stacked AutoEncoder. Multimed Tools Appl. 2022;81(8):10861–81.\\nAkram MW, Salman M, Bashir MF, Salman SMS, Gadekallu TR, Javed AR. A Novel Deep Auto-Encoder Based Linguistics Clustering Model for Social\\nText. ACM Trans Asian Low-Resour Lang Inf Process. 2022.\\nPark HH, Zhang KJ, Haley C, Steimel K, Liu H, Schwartz L. Morphology Matters: A Multilingual Language Modeling Analysis. Transactions of the\\nAssociation for Computational Linguistics. 2021;9:261–76.\\nLang K. Newsweeder: Learning to filter netnews. Machine learning proceedings 1995: Elsevier; 1995. p. 331–9.\\nDodda R, Alladi SB. Enhancing Document Clustering with Hybrid Recurrent Neural Networks and Autoencoders: A Robust Approach for Effective\\nSemantic Organization of Large Textual Datasets. EAI Endorsed Transactions on Intelligent Systems and Machine Learning Applications. 2024;1.\\nhttps://doi.org/10.4108/eetismla.4564\\nMoslem Y, Romani G, Molaei M, Kelleher JD, Haque R, Way A, editors. Domain Terminology Integration into Machine Translation: Leveraging Large\\nLanguage Models2023 December; Singapore: Association for Computational Linguistics.\\nRezaei S, Tanha J, Roshan S, Jafari Z, Molaei M, Mirzadoust S, et al. An experimental study of sentiment classification using deep-based models with\\nvarious word embedding techniques. Journal of Experimental & Theoretical Artificial Intelligence. 2024;:1–37.\\nWassie AK, Molaei M, Moslem Y. Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis. arXiv preprint\\narXiv:241205862. 2024.\\nXu Q, Zan H, Ji S. A lightweight mixup-based short texts clustering for contrastive learning. Front Comput Neurosci. 2024;17:1334748. pmid:38348466\\nMunguía Mondragón JC, Rendón Lara E, Alejo Eleuterio R, Granda Gutirrez EE, Del Razo López F. Density-Based Clustering to Deal with Highly\\nImbalanced Data in Multi-Class Problems. Mathematics. 2023;11(18):4008.\\nJiang Z, Chen T, Chen T, Wang Z. Improving contrastive learning on imbalanced data via open-world sampling. Advances in Neural Information\\nProcessing Systems. 2021;34:5997–6009.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n19/19'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 0}, page_content='1 of 21\\nExpert Systems, 2026; 43:e70182\\nhttps://doi.org/10.1111/exsy.70182\\nExpert Systems\\nORIGINAL ARTICLE\\nOPEN ACCESS\\nMitigating the Negative Transfer in Multi-\\xadTask Learning \\nfor Harmful Language Detection in Spanish and Arabic\\nAngel\\xa0Felipe\\xa0Magnossão\\xa0de\\xa0Paula1,2\\n\\u2002 |  Imene\\xa0Bensalem3,4\\n\\u2002 |  Damiano\\xa0Spina2\\u2002 |  Paolo\\xa0Rosso1,5\\n1Department of Computer Systems and Computation, Universitat Politècnica de València, València, Spain\\u2002 |\\u2002 2School of Computing Technologies, \\nRMIT University, Melbourne, Victoria, Australia\\u2002 |\\u2002 3MISC-\\xadLab, Constantine 2 University, Constantine, Algeria\\u2002 |\\u2002 4ESCF de Constantine, Constantine, \\nAlgeria\\u2002 |\\u2002 5ValgrAI, Valencian Graduate School and Research Network of Artificial Intelligence, València,\\xa0Spain\\nCorrespondence: Angel Felipe Magnossão de Paula (adepau@doctor.upv.es)\\nReceived: 27 September 2025\\u2002 |\\u2002 Revised: 14 November 2025\\u2002 |\\u2002 Accepted: 28 November 2025\\nKeywords: hate speech\\xa0| multi-\\xadtask learning\\xa0| negative transfer\\xa0| offensive language\\xa0| sexism\\xa0| toxic language\\nABSTRACT\\nNegative transfer continues to limit the benefits of multi-\\xadtask learning (MTL) in harmful language detection, where related \\ntasks must share representations without diluting task-\\xadspecific nuances. We introduce task awareness (TA), a methodological \\nframework that explicitly conditions MTL models on the task they must solve. TA is instantiated through two complementary \\nmechanisms: Task-\\xadaware input (TAI), which augments textual inputs with natural-\\xadlanguage task descriptions, and task embed-\\nding (TE), which learns task-\\xadspecific transformations guided by a task identification vector. Together they enable the encoder \\nto disentangle shared and task-\\xaddependent signals, reducing interference during joint optimisation. We integrate TA with BETO \\nand AraBERT encoders and evaluate on six Spanish and Arabic datasets covering sexism, toxicity, offensive language, and hate \\nspeech. Across cross-\\xadvalidation and official train-\\xadtest splits, TA consistently mitigates negative transfer, surpasses single-\\xadtask \\nand conventional MTL baselines, and yields new state-\\xadof-\\xadthe-\\xadart scores on EXIST-\\xad2021, HatEval-\\xad2019, and HSArabic-\\xad2023. The \\nproposed methodology therefore combines a principled architectural innovation with demonstrated practical gains for multilin-\\ngual harmful language detection. The resources to reproduce our experiments are publicly available at https://\\u200bgithub.\\u200bcom/\\u200bAngel\\u200b\\nFelip\\u200beMP/\\u200bArabi\\u200bc-\\xad\\u200bMulti\\u200bTask-\\xad\\u200bLearning.\\n1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fIntroduction\\nMachine learning applications are widespread, covering areas \\nfrom natural language processing (NLP)—which includes tasks \\nlike named-\\xadentity recognition and automated hate speech detec-\\ntion—to computer vision (CV), enabling systems for object de-\\ntection and classification (Otter et\\xa0al.\\xa02020; Lauriola et\\xa0al.\\xa02022; \\nVoulodimos et\\xa0al.\\xa02018; Jamil et\\xa0al.\\xa02023). Standard practice often \\ninvolves training a dedicated model or ensemble for each task, \\nrefining it until further performance gains are negligible. While \\nthis single-\\xadtask learning (STL) approach frequently produces ac-\\nceptable outcomes, it fails to leverage potential knowledge shar-\\ning from related tasks, which might otherwise improve model \\ngeneralisation. Furthermore, insufficient data can impede the \\ndevelopment of robust models. To overcome these limitations, \\nseveral methods have been proposed for transferring knowledge \\nacross different tasks (Kulis et\\xa0al.\\xa02011; Zhu et\\xa0al.\\xa02023).\\nAn emerging area, multi-\\xadtask learning (MTL) (Ruder\\xa0 2017; \\nAguilar et\\xa0al.\\xa02017; Plaza-\\xaddel-\\xadArco et\\xa0al.\\xa02021, 2021a, 2021b; \\nZhang and Yang\\xa02022; Chen et\\xa0al.\\xa02024), aims to exploit syn-\\nergies between various tasks, potentially lowering require-\\nments for data and computational power. MTL endeavours to \\nenhance generalisation by concurrently training on several \\ntasks. Within MTL using neural networks, two prevalent \\ntechniques are soft (Wu, Fei, and Ji\\xa02020; Wang et\\xa0al.\\xa02022) \\nand hard parameter-\\xadsharing (Fang et\\xa0 al.\\xa0 2022; De Freitas \\net\\xa0al.\\xa02022).\\nThis is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution and reproduction in any \\nmedium, provided the original work is properly cited and is not used for commercial purposes.\\n© 2026 The Author(s). Expert Systems published by John Wiley & Sons Ltd.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content=\"2 of 21\\nExpert Systems, 2026\\nIn soft parameter-\\xadsharing, task-\\xadspecific networks are employed, \\nwhile cross-\\xadtask communication is facilitated through feature-\\xad\\nsharing methods to encourage parameter similarity. However, \\nscalability can be an issue, as the size of the multi-\\xadtask network \\nincreases linearly with the number of tasks. In contrast, hard \\nparameter-\\xadsharing strategy divides the parameter set into shared \\nand task-\\xadspecific components, often implemented using a shared \\nencoder with multiple task-\\xadspecific decoding heads (Zhang and \\nYang\\xa02022; Chen et\\xa0al.\\xa02024). This approach has the added bene-\\nfit of reducing overfitting (Ruder\\xa02017).\\nThe hard parameter-\\xadsharing framework has been augmented \\nby multilinear relationship networks (Long et\\xa0al.\\xa02017), which \\napply tensor normal priors to the parameters of fully connected \\nlayers. Nevertheless, selecting branching points arbitrarily in \\nthese networks can result in suboptimal task arrangements. \\nTree-\\xadbased structures (Lu et\\xa0al.\\xa02017; Vandenhende et\\xa0al.\\xa02020) \\nhave been suggested to address this issue.\\nDespite these developments, learning multiple tasks concur-\\nrently can sometimes result in negative transfer (Vandenhende \\net\\xa0al.\\xa02022; Wu, Zhang, and Ré\\xa02020). This occurs when shared \\nnoisy information between tasks impairs the model's perfor-\\nmance. Negative transfer signifies a reduction in the model's \\neffectiveness on target tasks due to knowledge transfer (Wu, \\nZhang, and Ré\\xa02020; Vandenhende et\\xa0al.\\xa02022).\\nThis work introduces a novel method to address the negative \\ntransfer challenge, utilising the concept of task awareness \\n(TA) (Magnossão de Paula et\\xa0al.\\xa02023). Our technique allows \\nMTL models to use information about the particular task \\nbeing processed, enabling the model to prioritise its internal \\nweights appropriately for each task. In contrast to state-\\xadof-\\xadthe-\\xad\\nart (SOTA) approaches (see Section\\xa02), our method avoids re-\\ncursive structures, thus conserving computational resources \\nand time.\\nEmploying the TA concept, we devised two mechanisms inte-\\ngrated into two distinct MTL TA (MTL-\\xadTA) architectures. The \\ngoal of these architectures is to tackle SOTA difficulties in iden-\\ntifying sexism, toxic language, and hate speech within Spanish \\ntext, as well as sexism, offensive language, and hate speech in \\nArabic comments.\\nExamples illustrating each task in its original language, ac-\\ncompanied by English translations, are provided in Table\\xa0 1. \\nThis table also specifies the source dataset for each text sample. \\nSection\\xa04.1 offers a detailed account of the datasets used.\\nAlthough hate speech, sexism, offensive language, and toxic lan-\\nguage represent related concepts (Poletto et\\xa0al.\\xa02021; Alkomah \\nand Ma\\xa02022; Pachinger et\\xa0al.\\xa02023; Bensalem et\\xa0al.\\xa02024), they \\neach possess unique characteristics and societal consequences. \\nHate Speech typically targets specific demographics (Plaza-\\xaddel-\\xad\\nArco et\\xa0al.\\xa02021, 2021a, 2021b), whereas Sexism relates to gender-\\xad\\nbased discrimination (Frenda et\\xa0al.\\xa02019). Offensive and Toxic \\nLanguage are broader terms encompassing expressions that pro-\\nmote hostility or negativity (Derczynski et\\xa0al.\\xa02024; Magnossão \\nde Paula and Schlicht\\xa02021). The conceptual overlaps between \\nthese tasks are depicted in the Venn diagram (Figure\\xa01). Given \\ntheir interrelations, creating MTL models capable of identifying \\nthese various forms of harmful language presents a valuable \\nopportunity.\\nAmong the languages most frequently used on social media \\nplatforms like Twitter, Facebook, and TikTok are Spanish \\nand Arabic. For instance, data indicates Spanish ranks as \\nthe third most common language on Twitter, with Arabic \\nfourth Alshaabi et\\xa0 al.\\xa0 (2021). Regrettably, the prevalence of \\na language often correlates with the volume of hostile and \\nharmful content generated in it. We believe this paper is the \\nfirst to put forward effective strategies for reducing negative \\ntransfer across numerous sensitive tasks in both Spanish and \\nArabic. The source code developed for this study is openly \\naccessible.1\\nThe primary contributions of this research include:\\n•\\t Introduction of task awareness (TA): Our paper introduces \\nthe concept of TA and proposes two unified architectures \\nequipped with TA mechanisms (MTL-\\xadTAI & MTL-\\xadTE) \\nthat can mitigate the negative transfer phenomenon during \\nMTL training.\\n•\\t Development of TA mechanisms: To equip MTL models \\nwith task recognition capabilities, we developed the Task-\\xad\\naware input (TAI) and task embedding (TE) mechanisms, \\naimed at alleviating negative transfer and improving per-\\nformance compared to traditional MTL approaches.\\n•\\t Validation of MTL-\\xadTA models: We evaluated the effective-\\nness of the two TA-\\xadequipped architectures in detecting sex-\\nism, toxic language, and hate speech in Spanish comments, \\nas well as sexism, offensive language, and hate speech in \\nArabic textual comments. The results demonstrate that \\nboth MTL-\\xadTAI and MTL-\\xadTE mitigate negative transfer in \\nthese two languages.2\\n•\\t Achieving SOTA performance: Our approach exceeds \\nSOTA results on established public benchmarks for detect-\\ning sexism (EXIST-\\xad2021) and hate speech (HatEval-\\xad2019). \\nFurthermore, it sets a new SOTA benchmark for the \\nHSArabic-\\xad2023 dataset concerning offensive language \\nidentification, marking considerable advancements over \\nprior techniques.\\nThe novelty of this work lies in its focus on methods and prac-\\ntical applications. We present effective architectural mecha-\\nnisms that implement the TA principle within MTL systems and \\ndemonstrate how these mechanisms can be integrated into real \\nmoderation workflows. While the concept of using task cues to \\nguide shared representations is rooted in the theory of negative \\ntransfer, our key advances are in the practical design, integra-\\ntion, and empirical validation of TAI and TE for detecting harm-\\nful language in Spanish and Arabic.\\nThe rest of this paper is structured as follows. Section\\xa02 reviews \\nrelated work on transfer learning, MTL and its application to \\nHarmful Language detection. Section\\xa03 describes our proposed \\nmethod in detail. Section\\xa0 4 outlines the experimental setup. \\nSection\\xa0 5 presents and analyzes the experimental outcomes. \\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 2}, page_content=\"3 of 21\\nExpert Systems, 2026\\nPotential limitations of our method are discussed in Section\\xa06. \\nFinally, Section\\xa07 offers concluding remarks and outlines poten-\\ntial avenues for future investigation.\\n2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fRelated Work\\n2.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fTransfer Learning and Multi-\\xadTask Learning\\nTransfer learning represents a common machine learning \\nmethod, founded on the idea that a model developed for one \\ntask can be improved by integrating knowledge from a related \\ntask (Pan and Yang\\xa02009; Weiss et\\xa0al.\\xa02016; Zhu et\\xa0al.\\xa02023). \\nTraining models entirely from scratch often demands signifi-\\ncant data and computational power; however, situations arise \\nwhere acquiring enough training data is excessively costly or \\ninfeasible. This necessitates creating high-\\xadperforming learn-\\ning systems using more accessible data from alternate tasks. \\nKnowledge transfer methods facilitate enhancing target task \\nperformance by utilising information derived from associ-\\nated tasks. Such methods have seen successful deployment \\nin diverse machine learning domains, notably NLP (Ruder \\net\\xa0 al.\\xa0 2019; Wang and Mahadevan\\xa0 2011; Prettenhofer and \\nStein\\xa02010; Wang et\\xa0al.\\xa02022) and CV (Duan et\\xa0al.\\xa02012; Kulis \\net\\xa0al.\\xa02011). Closely associated with transfer learning is the \\nMTL framework (Ruder\\xa02017; Zhang and Yang\\xa02022), which \\nTABLE 1\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fExamples of tweets containing sexism, hate speech, toxic language, and offensive language in Spanish and Arabic.\\nTask\\nDataset\\nOriginal language\\nEnglish translation\\nSexism\\nEXIST-\\xad2021\\n@USER Que. rica putita obediente, \\nafortunado tu marido de tener \\nuna mujer como tú, saludos\\n@USER What a nice obedient little whore, your \\nhusband is lucky to have a woman like you, greetings\\nArMI-\\xad2021\\nUSER@ USER@ USER@ The wife takes care of the \\nkitchen, cleanliness and organisation of the house and \\nsits at home, while the husband works for his home.\\nHate speech\\nHatEval-\\xad2019\\nHay varias paginas de feministas a las \\nque deberia darles verguenza exponer sus \\nideas ya que no tienen ni pies ni cabeza\\nThere are several feminist pages that should \\nbe ashamed of themselves for expressing their \\nideas because they have no head or tai\\nHSArabic-\\xad2023\\nRT @USER:\\n \\n https://t.\\u200bco/\\u200bnbSrV\\u200bJdVlm\\u200b\\nRT @USER: #Hezbollah We will shut down \\nyour barking, you stinking, liar terrorist, \\nforever https://t.\\u200bco/\\u200bnbSrV\\u200bJdVlm\\u200b\\nToxic \\nlanguage\\nDETOXIS-\\xad2021\\nEstá claro que vienen los mejores. Haced \\nque pase putos rojos de mierda.\\nIt's clear that the best are coming. Make \\nit happen you fucking Reds\\nOffensive \\nlanguage\\nOSACT-\\xad2022\\nRT @USER:\\n \\n URL\\nRT @USER If Al Arabiya had not reported this \\nnews, we would have been surprised. We are \\nused to its ridiculous news that shows the world \\nthe worst image of the Saudi people URL\\nNote: The original texts are provided alongside their English translations, which were generated using Google Translate.\\nFIGURE 1\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAdaptation of the (Poletto et\\xa0 al.\\xa0 2021; Alkomah and \\nMa\\xa02022) Venn diagrams showing the relationships among sexism, hate \\nspeech, toxic language, and offensive language.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content=\"4 of 21\\nExpert Systems, 2026\\nseeks to learn multiple, potentially different, tasks in parallel. \\nThe effectiveness of this paradigm stems from its capacity to \\nleverage shared information across tasks. Nevertheless, if the \\ntasks lack sufficient relatedness, negative transfer can occur. \\nThis term describes performance decline resulting from shar-\\ning noisy or inappropriate information between tasks (Wu, \\nZhang, and Ré\\xa02020; Vandenhende et\\xa0al.\\xa02022).\\nRecent studies have focussed on identifying and mitigating \\nnegative transfer in MTL. Li et\\xa0al.\\xa0(2023) introduced a surro-\\ngate modelling approach to predict and partially prevent nega-\\ntive transfer by estimating relevance scores for each task. This \\nmethod significantly improves the accuracy of MTL by selecting \\noptimal task subsets.\\nHierarchical Prompt Learning (HiPro), as proposed by Liu \\net\\xa0al.\\xa0(2023), demonstrates how a hierarchical task-\\xadsharing ap-\\nproach can reduce the risks of negative transfer. By organising \\ntasks into more granular groups based on their relatedness, \\nHiPro constructs a task tree that allows the model to learn both \\nshared and individual task prompts, balancing generalisation \\nwith task-\\xadspecific adaptation.\\nSeveral approaches have been proposed to address nega-\\ntive transfer and balance learning across different tasks. \\nThese include re-\\xadweighting of losses through methods like \\nHomoscedastic uncertainty (Cipolla et\\xa0al.\\xa02018), Gradient nor-\\nmalisation (Chen et\\xa0al.\\xa02018), and Adversarial training (Sinha \\net\\xa0 al.\\xa0 2021), as well as task prioritisation (Guo et\\xa0 al.\\xa0 2018; \\nZhao et\\xa0al.\\xa02018; Sener and Koltun\\xa02018). Additionally, other \\napproaches (Xu et\\xa0 al.\\xa0 2018; Zhang et\\xa0 al.\\xa0 2018, 2019) utilise \\ninitial predictions from multi-\\xadtask networks to iteratively re-\\nfine each task's output, thereby overcoming the limitations \\nof methods that compute all task outputs simultaneously. \\nHowever, these approaches are often time-\\xadconsuming and re-\\nquire substantial computational resources due to their recur-\\nsive nature.\\nKnight and Duan\\xa0(2023) introduced an innovative framework \\nthat uses summary statistics to address the challenges of MTL \\nin data-\\xadsharing-\\xadconstrained environments, such as health-\\ncare settings. This approach enables efficient model training \\nwithout the need for access to individual-\\xadlevel data, preserv-\\ning privacy while still benefiting from shared information \\nacross tasks.\\nIn the domain of NLP, Chen et\\xa0al.\\xa0(2024) provide an overview \\nthat underscores the importance of MTL in mitigating over-\\nfitting and addressing data scarcity. The authors review MTL \\narchitectures and optimisation techniques, demonstrating how \\nMTL can leverage related tasks to enhance overall performance \\nacross NLP applications.\\n2.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fMTL In Harmful Language Detection\\nThe initial semi-\\xadsupervised multi-\\xadtask method for Sexism clas-\\nsification was introduced by Abburi et\\xa0al.\\xa0(2020). Their work ad-\\ndressed three tasks utilising labels derived from unsupervised \\nlearning or weak labelling processes. The neural multi-\\xadtask \\narchitecture they designed facilitates shared learning among \\ntasks through common weights and an aggregated loss function, \\nsurpassing several SOTA baselines.\\nWu, Fei, and Ji\\xa0(2020) put forward a novel MTL strategy to con-\\ncurrently manage Aggressive Language Detection (ALD) and \\ntext normalisation. They employed a shared encoder for learn-\\ning common inter-\\xadtask features and a task-\\xadspecific encoder for \\ntask-\\xadrelevant features. This configuration led to considerable \\nperformance gains in ALD.\\nAbu Farha and Magdy\\xa0 (2020) proposed CNN-\\xadBiLSTM-\\xadbased \\nmodels trained for three tasks: Hate speech detection, offensive \\nlanguage detection, and sentiment analysis. The authors evalu-\\nated their models using the OSACT2020 (Mubarak et\\xa0al.\\xa02020) \\nArabic dataset, demonstrating that their multi-\\xadtask architecture \\noutperformed traditional monotask models.\\nIn this paper, we introduce two unified architectures designed \\nto identify sexism, toxic language, and hate speech in comments \\nwritten in Spanish, as well as to detect sexism, offensive lan-\\nguage, and hate speech in Arabic textual comments. The de-\\nsigned architectures intend to lessen the negative transfer effect \\nin MTL training, consequently enhancing the identification rate \\nfor harmful content.\\nThe methodology detailed here draws inspiration from mech-\\nanisms suggested by (Abburi et\\xa0al.\\xa02020; Wu, Fei, and Ji\\xa02020). \\nWhile those methods focus on refining the representations \\npassed to task heads to improve MTL models, our TA technique \\ndistinguishes itself. It empowers the model to independently \\nascertain the task it needs to execute. Consequently, MTL-\\xadTA \\nmodels can generate suitable representations for every task \\nhead without needing an auxiliary learning task, enhancing \\nefficiency. The core concept involves learning a task-\\xadpertinent \\nlatent data representation capable of effectively addressing mul-\\ntiple NLP tasks (Wang et\\xa0 al.\\xa0 2022; Indurthi et\\xa0 al.\\xa0 2021). The \\nspecific mechanisms developed are elaborated upon in the sub-\\nsequent section.\\n3\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fProposed Approach\\nThis section details the MTL-\\xadTA models introduced in \\n(Magnossão de Paula et\\xa0al.\\xa02023). We begin by introducing the \\nconcept of TA and explaining its potential in reducing the effects \\nof negative transfer (Vandenhende et\\xa0al.\\xa02022; Wu, Zhang, and \\nRé\\xa0 2020) in multi-\\xadtask joint training (Ruder\\xa0 2017). Following \\nthis, we introduce two specific TA mechanisms designed for in-\\ncorporating task self-\\xadawareness into MTL models.\\nOur methodological innovation is anchored in three design \\nprinciples. First, we expose the encoder to explicit task descrip-\\ntors so that the shared representation can be shaped by both \\nlinguistic content and the downstream objective. Second, we \\ninterpose a lightweight task-\\xadconditional transformation that \\ncan reconfigure the shared representation before it reaches \\neach task head, thereby curbing interference from unrelated \\ngradients. Third, we ensure both mechanisms can be trained \\nend-\\xadto-\\xadend within standard hard-\\xadparameter-\\xadsharing pipelines, \\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content=\"5 of 21\\nExpert Systems, 2026\\nallowing practitioners to retrofit TA into existing moderation \\nsystems without extensive re-\\xadengineering. The remainder of this \\nsection formalises these ideas and details how they differ from \\nprior MTL formulations.\\nThe most common technique for supervised MTL utilises the \\nhard parameter-\\xadsharing strategy (Zhang and Yang\\xa02022). In \\nthis configuration, the model comprises an encoder along-\\nside N decoders (or task heads), with N representing the \\ncount of tasks the model trains on concurrently (Worsham \\nand Kalita\\xa0 2020). During operation, the encoder takes an \\ninput and produces a task-\\xadneutral latent representation, sub-\\nsequently passed to the designated task head for the final \\nprediction.\\nHowever, a weaker connection between the encoder's generated \\nlatent representation and the specific tasks can impair over-\\nall MTL model efficacy (Vandenhende et\\xa0al.\\xa02022). It is prob-\\nable that the ideal latent representations for identical inputs \\nwill differ across various task heads (De Freitas et\\xa0 al.\\xa0 2022). \\nFurthermore, during the training phase, the encoder's represen-\\ntation might develop a bias towards tasks that are more complex \\nor possess larger datasets (Ruder\\xa02017). Such reductions in per-\\nformance exemplify the negative transfer issue (Vandenhende \\net\\xa0al.\\xa02022; Wu, Zhang, and Ré\\xa02020), where a task head gets an \\nunsuitable input representation, hindering its capacity to effec-\\ntively address its assigned task.\\nTo mitigate negative transfer when tackling multiple NLP tasks \\nusing the MTL approach (Zhang and Yang\\xa02022), we propose \\ntwo TA mechanisms. These mechanisms tailor the task heard \\ninput representation based on the specific task being addressed, \\nensuring that the representation sent to each respective head is \\noptimised for that task. Furthermore, our proposed MTL model \\ncontinues to benefit from the generalisation improvements pro-\\nvided by multi-\\xadtask joint training. Updates during training apply \\nto the encoder and other MTL model components preceding the \\ntask heads for every task. It is crucial to recognise that all our \\nsuggested MTL models belong to the MTL-\\xadTA category and fol-\\nlow the standard MTL paradigm. Consequently, parameter up-\\ndates only involve the specific task head corresponding to the \\ncurrent input data.\\nThis holistic formulation means that TA augments, rather than \\nreplaces, classic hard-\\xadparameter sharing: practitioners can reuse \\nestablished optimisation recipes while equipping the model \\nwith explicit mechanisms to preserve task-\\xadspecific signals. In \\nSection\\xa04 we describe how this design readily scales to hetero-\\ngeneous datasets without bespoke tuning for each task pairing.\\n3.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fTAI\\nThe initial mechanism formulated to embed TA within MTL \\nmodels is the TAI. To help the encoder produce appropriate rep-\\nresentations for every task head, we suggest altering the stan-\\ndard MTL input structure for NLP applications.\\nConcretely, for each sample we concatenate a short natural-\\xad\\nlanguage task description (TD) to the original text snippet and \\nrely on the encoder's positional embeddings to disentangle \\nthe segments. This approach aligns the latent space with task \\nsemantics from the very first layer, steering the encoder to \\nhighlight lexical and syntactic cues that are predictive for the \\nspecified task. Unlike prompt-\\xadbased conditioning that often re-\\nquires task-\\xadspecific templates or additional pre-\\xadtraining, TAI \\nuses a uniform schema that can be populated automatically \\nfrom dataset metadata, which makes it robust across languages \\nand label distributions.\\nThe TAI comprises a text snippet (TS) paired with a TD, illus-\\ntrated in Figure\\xa02. The TS represents a segment of text, variable \\nin length based on the task, and usually serves as the primary \\ninput for MTL encoders. The TD is textual information specify-\\ning the particular task managed by a certain head, for instance, \\n“sexism detection” or “hate speech detection”. This adjusted \\ninput supplies context to the encoder, facilitating the creation of \\na task-\\xadfocused representation. An MTL model incorporating the \\nTAI mechanism is denoted as MTL task-\\xadaware input (MTL-\\xadTAI).\\n3.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fTE\\nThe second mechanism conceived to bestow TA capability \\nupon MTL models is named TE. We propose inserting an extra \\ncomponent situated between the task heads and the encoder, \\ndesignated as the task embedding block (TEB), depicted in \\nFigure\\xa03. This block takes two inputs: (i) the task identification \\nvector (TIV), and (ii) the latent representation from the encoder. \\nThe TIV is constructed as a one-\\xaddimensional one-\\xadhot vector, \\nits length matching the number of task heads. Every position \\nwithin the TIV corresponds to a distinct task head.\\nThe TEB is composed of learning units (LU), each containing a \\nlinear layer succeeded by a ReLU activation function. The quan-\\ntity of LUs acts as a hyperparameter, influenced by factors like \\ntask type and data properties. The primary goal of the TEB is to \\ncraft a fitting representation for the task the MTL model is cur-\\nrently addressing. Consequently, given the same encoder output, \\nthe TEB yields varied outputs contingent on the task. It uses the \\nTIV as a signal to determine for which task the representation \\nshould be generated. As shown in Figure\\xa03, the TIV features a “1” \\nat the index related to the task being processed, while all other \\npositions are zero. An MTL model utilising the TE mechanism is \\nidentified as MTL task embedding (MTL-\\xadTE).\\nThe TEB acts as a learned gating function that re-\\xadweights shared \\nfeatures according to the active task. During training, gradi-\\nents flowing through the TIV-\\xadconditioned layers encourage the \\nmodel to isolate features that consistently help a given task while \\nsuppressing features that trigger negative transfer. Because the \\nsame parameters are reused across tasks with different activa-\\ntions, TE mechanism promotes parameter efficiency while still \\nenabling task-\\xadspecific specialisation.\\n4\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fExperimental Setup\\nThis section commences by detailing the tasks and datasets em-\\nployed for evaluating our method. Subsequently, it provides im-\\nplementation specifics and reference models. Lastly, it outlines \\nthe experimental configurations.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 5}, page_content='6 of 21\\nExpert Systems, 2026\\n4.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fData\\nIn addition to addressing the negative transfer problem, our \\nresearch focussed on identifying and filtering harmful and \\nabusive content in social media text. Our overarching goal is \\nto promote a respectful and inclusive environment by prevent-\\ning the spread of discriminatory or harmful language online \\nand in other communication channels. We decided to focus on \\nSpanish and Arabic, as these languages are among the most \\nwidely used on social media Alshaabi et\\xa0al.\\xa0(2021). However, \\nthe prevalence of these languages also corresponds with the \\nproduction of hostile and harmful content. Therefore, it is \\ncrucial to make substantial efforts to tackle harmful language \\nbehaviour in Spanish and Arabic, rather than English (Plaza-\\xad\\ndel-\\xadArco et\\xa0 al.\\xa0 2021, 2021a, 2021b). For Spanish, we tackled \\ntasks related to detecting Sexism, Toxic Language and Hate \\nFIGURE 2\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fMulti-\\xadtask learning (MTL) model including task-\\xadaware input (TAI) mechanism (MTL-\\xadTAI).\\nFIGURE 3\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fMulti-\\xadtask learning (MTL) model including task embedding (TE) mechanism (MTL-\\xadTE).\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content=\"7 of 21\\nExpert Systems, 2026\\nSpeech. In Arabic, we focussed on detecting Sexism, Offensive \\nLanguage, and Hate Speech. We utilised six datasets, each tai-\\nlored to specific tasks within these languages. For Spanish, \\nwe used the EXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0 al.\\xa0 2021), \\nDETOXIS-\\xad2021 (Taulé et\\xa0al.\\xa02021), and HateEval-\\xad2019 (Basile \\net\\xa0al.\\xa02019) datasets. For Arabic, we employed the ArMI-\\xad2021 \\n(Mulki and Ghanem\\xa02021), HSArabic-\\xad2023, and OSACT-\\xad2022 \\n(Mubarak et\\xa0al.\\xa02022) datasets. Below is a detailed description \\nof each dataset:\\n4.1.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fEXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021)\\nThis dataset originated from the sExism Identification in \\nSocial neTworks (EXIST) shared task during the Iberian \\nLanguages Evaluation Forum (IberLEF) 2021. It comprises \\n11,345 annotated social media posts (in English and Spanish) \\nsourced from Twitter and the uncensored platform Gab.\\u200bcom \\n(Gab). Experts in gender issues supervised and monitored \\nthe dataset's creation. EXIST represented the inaugural chal-\\nlenge focused on social media Sexism detection, aiming to \\nidentify Sexism broadly, encompassing explicit misogyny to \\nsubtler sexist actions. The task related to Sexism identifica-\\ntion attracted 70 official submissions. It involves binary clas-\\nsification, categorising samples as either Sexist or Not-\\xadSexist. \\nAccuracy served as the official evaluation standard, with data \\npartitioned into training and test sets. Table\\xa0 2 presents the \\ndata breakdown.\\n4.1.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fDETOXIS-\\xad2021 (Taulé et\\xa0al.\\xa02021)\\nData collection for this set occurred for the DEtection of \\nTOxicity in comments In Spanish (DETOXIS) shared task at \\nIberLEF 2021. The task's goal was detecting Toxic Language \\nwithin comments responding to online news articles concern-\\ning immigration. The annotation method developed aimed at \\nreducing subjectivity in toxicity labeling by considering con-\\ntext (like linguistic cues and conversation threads). The data \\nannotation team comprised trained annotators and linguistics \\nexperts. The collection contains 4354 text comments respond-\\ning to various articles from Spanish online news sources (e.g., \\nABC, elDiario.es, El Mundo, NIUS) and discussion platforms \\n(like Menéame). The task involves a binary classification, as-\\nsigning samples to either Toxic or Not-\\xadToxic categories. Over 30 \\nteams assessed their machine learning models using this data-\\nset during the DETOXIS shared task participation. The F1-\\xadscore \\nfor the Toxic class was the official evaluation measure, and the \\ndataset was split into training and testing portions. The data dis-\\ntribution is shown in Table\\xa03.\\n4.1.3\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fHatEval-\\xad2019 (Basile et\\xa0al.\\xa02019)\\nThis dataset was assembled for the Detection of Hate Speech \\nAgainst Immigrants and Women in Twitter (HatEval) task, a com-\\nponent of the SemEval 2019 workshop. It includes 19,600 tweets \\n(English and Spanish) with labels for hate speech detection. The \\ncollection process involved several gathering techniques: (i) ob-\\nserving accounts likely targeted by hate; (ii) obtaining records \\nfrom known haters; (iii) applying keyword filters to Twitter feeds. \\nAnnotation involved experts and crowdsourced workers verified \\nfor annotation reliability. The task required binary classification, \\nassociating samples with hateful or not-\\xadhateful labels. The data-\\nset contains training, development, and test partitions. The offi-\\ncial metric was F1-\\xadmacro (the unweighted average F1-\\xadscore across \\nboth classes). HatEval ranked among SemEval 2019's most partic-\\nipated tasks, receiving over 100 submissions for hate speech detec-\\ntion. Table\\xa04 details the dataset's composition.\\n4.1.4\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fArMI-\\xad2021 (Mulki and Ghanem\\xa02021)\\nThe dataset served the Arabic Misogyny Identification shared \\ntask (ArMI), part of the hate speech and offensive content de-\\ntection (HASOC) track at FIRE-\\xad2021. It comprises 9833 tweets \\nin formal Arabic and various dialects, including Levantine, Gulf, \\nand Egyptian. These tweets were gathered using different expres-\\nsions and hashtags related to anti-\\xadwomen topics, as well as from \\nthe accounts of seven female journalists who were active during \\nthe Lebanon protests in October 2019. The annotators labelled \\nthe tweets for sexism identification (a binary task) and sexism \\ncategorization (a multi-\\xadclass task). The challenge received 15 of-\\nficial runs. The official evaluation metric was accuracy, and data \\nwas split into training and test sets. The dataset statistics are pre-\\nsented in Table\\xa05, with our focus exclusively on the binary task.\\n4.1.5\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fHSArabic-\\xad2023\\nThis dataset was created by Hamad Bin Khalifa University \\nand Carnegie Mellon University in Qatar as part of a research \\nproject. It contains more than 15,000 tweets in different Arabic \\ndialects. Annotators from various Arabic-\\xadspeaking countries \\nin the Middle East and North Africa labelled the tweets. The \\nTABLE 2\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fEXIST-\\xad2021 data distribution (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021).\\nTraining\\nTest\\nSpanish\\nEnglish\\nSpanish\\nEnglish\\nTwitter\\nTwitter\\nTwitter\\nGab\\nTwitter\\nGab\\nSexist\\n1741\\n1636\\n858\\n265\\n858\\n300\\nNot-\\xadSexist\\n1800\\n1800\\n812\\n225\\n858\\n192\\nTABLE 3\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fDETOXIS-\\xad2021 data distribution (Taulé et\\xa0al.\\xa02021).\\nTraining\\nTest\\nToxic\\n1147\\n239\\nNot-\\xadtoxic\\n2316\\n652\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content=\"8 of 21\\nExpert Systems, 2026\\nannotations cover different tasks, including Offensive Language \\ndetection, which we considered in our experiments. An up-\\ndated version of the dataset is described in the paper by (Charfi \\net\\xa0al.\\xa02024). We divided the dataset into training and test sub-\\nsets, as shown in Table\\xa06 and adopted F1-\\xadmacro as an official \\nevaluation metric.\\n4.1.6\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fOSACT-\\xad2022 (Mubarak et\\xa0al.\\xa02022)\\nThis dataset was utilised in the OSACT 2022 shared task on \\nArabic offensive language and hate speech detection. It com-\\nprises 12,698 examples collected from Twitter using a predefined \\nlist of emojis frequently associated with offensive texts. The \\ntweets were labelled via a crowdsourcing platform for three sub-\\ntasks: Offensive language detection (binary task), Hate Speech \\ndetection (binary task), and fine-\\xadgrained hate speech detection \\n(multi-\\xadclass task). The data is composed of training, develop-\\nment, and test sets. In total, 40 teams signed up to participate in \\nthe offensive language detection task, and the official evaluation \\nmetric was the F1-\\xadmacro. Our analysis focuses on the binary an-\\nnotations, with statistics detailed in Table\\xa07.\\n4.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fImplementation Details\\nThe encoder employs a bidirectional encoder representation \\nfrom transformers (BERT) (Devlin et\\xa0al.\\xa02019), pre-\\xadtrained in \\nthe language of the applied task data. We utilised the most \\npopular BERT versions for each language: BERT for Spanish \\ntransformers (BETO) (Canete et\\xa0 al.\\xa0 2020) and Arabic BERT \\n(AraBERT) (Antoun et\\xa0 al.\\xa0 2020). Following the BERT en-\\ncoding, we applied both max pooling and mean pooling \\ncalculations to its output. These BERT models consist of 12 \\nself-\\xadattention layers, each with 12 attention heads, and a hid-\\nden size of 768 dimensions, totaling approximately 110 mil-\\nlion parameters.\\nThe respective encoder (BETO or AraBERT) handles a text se-\\nquence, yielding a hidden representation per token equivalent \\nto the 768 hidden size dimensions. Concatenating the max and \\nmean pooling results derived from the full sequence of encoder \\noutput tokens forms the latent encoder representation. Within \\nthe TE method, the TEB preserves the precise dimensionality of \\nthis latent encoder representation.\\nTask heads function as linear classifiers. Their input dimensions \\nalign with the latent encoder representation, while output di-\\nmensions vary by task. For binary classification tasks, the linear \\nclassifier outputs two values; the larger value determines the pre-\\ndicted class. Additionally, task descriptions (TDs) for each data-\\nset were formulated by appending ‘detection’ to the task name; \\nfor example, for EXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021), the \\nTD used was “Sexism Detection”.\\nModel training utilised the AdamW optimizer (Loshchilov and \\nHutter\\xa02019), incorporating a linear decay learning rate sched-\\nule spanning from 5e-\\xad6 to 1e-\\xad4. Training involved 15 epochs, a \\ndropout rate of 0.3, and a batch size of 64. We tested configura-\\ntions using 1, 2, and 3 LUs. Adopting an approach akin to early \\nstopping (Caruana et\\xa0al.\\xa02000), the model demonstrating best \\nperformance on the task's official metric was chosen.\\n4.3\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fComparison Models\\nOur method is compared against two model categories: (i) \\nBaseline models and (ii) SOTA models. We specifically imple-\\nmented two baseline types: STL and MTL models. These base-\\nlines are essential for determining whether negative transfer \\noccurred during the training of the classic MTL model, as re-\\nvealed by comparing the models' performance on the test data. \\nNegative transfer is identified when the classic MTL model per-\\nforms worse—according to the chosen evaluation metric—than \\nthe STL model. In such cases, we further evaluate the perfor-\\nmance of the MTL Task-\\xadAware (MTL-\\xadTA) models against the \\nclassic MTL model to determine whether our proposed solu-\\ntions effectively address the negative transfer issue by achiev-\\ning superior results. Below there is a detailed description of the \\ntwo modes:\\nTABLE 4\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fHatEval-\\xad2019 data distribution (Basile et\\xa0al.\\xa02019).\\nTraining\\nDevelopment\\nTest\\nSpanish\\nEnglish\\nSpanish\\nEnglish\\nSpanish\\nEnglish\\nHate\\n1741\\n1636\\n1741\\n1636\\n858\\n300\\nNot-\\xadhate\\n1800\\n1800\\n1800\\n1800\\n812\\n192\\nTABLE 5\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fArMI-\\xad2021 data distribution (Mulki and Ghanem\\xa02021).\\nTraining\\nTest\\nSexist\\n4805\\n1201\\nNot-\\xadsexist\\n3061\\n766\\nTABLE 6\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fHSArabic-\\xad2023 data distribution.\\nTraining\\nTest\\nOffensive\\n2234\\n559\\nNot-\\xadoffensive\\n10,057\\n2514\\nTABLE 7\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fOSACT-\\xad2022 data distribution (Mubarak et\\xa0al.\\xa02022).\\nTraining\\nDevelopment\\nTest\\nHate\\n959\\n109\\n271\\nNot-\\xadhate\\n7928\\n1161\\n2270\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content=\"9 of 21\\nExpert Systems, 2026\\n•\\t MTL refers to the standard MTL model. Its construction \\nmirrors the MTL-\\xadTA model architecture (see Section\\xa03) but \\nlacks the TAI mechanism. Consequently, the MTL model \\nprocesses only the TS as input.\\n•\\t STL designates the standard STL model. While sharing the \\nMTL model's architecture, it features just one task head. \\nTherefore, comparing this model type against MTL mod-\\nels necessitates training a separate model for every task \\naddressed.\\nSOTA models signify the top-\\xadperforming methods currently avail-\\nable for the datasets included in our experiments. Comparing the \\nperformance of classic MTL, SOTA, and MTL-\\xadTA models pro-\\nvides valuable insights into how effectively a simple MTL model \\ncan approach SOTA results when negative transfer is mitigated. \\nBelow is a comprehensive overview of the SOTA models:\\n•\\t AI-\\xadUPV (Magnossão de Paula et\\xa0al.\\xa02021): A deep learn-\\ning architecture leveraging a combination of different \\nTransformer models (Vaswani et\\xa0al.\\xa02017). It capitalises on \\nensemble techniques and incorporates data augmentation \\nduring training. This model holds the SOTA position for \\nEXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021).\\n•\\t SINAI (Plaza-\\xaddel-\\xadArco et\\xa0al.\\xa02021, 2021a, 2021b): A BERT \\nbase model (Devlin et\\xa0al.\\xa02019) trained via the MTL hard \\nparameter-\\xadsharing approach. Despite covering five tasks \\nand six datasets, its primary focus was Toxic Language \\ndetection, utilising other tasks as auxiliary support. It rep-\\nresents the SOTA for DETOXIS-\\xad2021 (Taulé et\\xa0al.\\xa02021).\\n•\\t Atalaya (Pérez and Luque\\xa0 2019): This model employs \\nSupport Vector Machines (Boser et\\xa0 al.\\xa0 1992). Training in-\\nvolved multiple representations derived from FastText \\n(Bojanowski et\\xa0 al.\\xa0 2017) sentiment-\\xadfocused word vectors, \\nincluding tweet embeddings (Mikolov et\\xa0 al.\\xa0 2013), bag-\\xad\\nof-\\xadcharacters (Bojanowski et\\xa0 al.\\xa0 2017), and bag-\\xadof-\\xadwords \\n(Blizard\\xa01988). It is recognised as the SOTA for HatEval-\\xad2019 \\n(Basile et\\xa0al.\\xa02019).\\n•\\t UM6P-\\xadNLP (Mahdaouy et\\xa0 al.\\xa0 2021): This approach uti-\\nlises MARBERT (Abdul-\\xadMageed and Elmadany\\xa0 2021), a \\nlanguage model pre-\\xadtrained on a corpus of 1 billion tweets \\nspanning various Arabic dialects. It uses a multi-\\xadtask meth-\\nodology, formulated to address both binary and multiclass \\nclassification tasks within the ArMI-\\xad2021 shared task \\n(Mulki and Ghanem\\xa02021), where it secured SOTA status.\\n•\\t GOF (Mostafa et\\xa0 al.\\xa0 2022): This method uses an ensem-\\nble strategy based on majority voting among three pre-\\xad\\ntrained models: QARiB (Abdelali et\\xa0al.\\xa02021), MARBERT, \\nand MERBERT v2 (Abdul-\\xadMageed and Elmadany\\xa0 2021). \\nOptimization for each model involved a distinct loss func-\\ntion. It stands as the SOTA for OSACT-\\xad2022 (Mubarak \\net\\xa0al.\\xa02022).\\n4.4\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fEvaluation Setup\\nTwo experiments were performed to assess our TA method's ef-\\nficacy in reducing negative transfer (Vandenhende et\\xa0al.\\xa02022; \\nWu, Zhang, and Ré\\xa02020), detailed below.\\n4.4.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fCross-\\xadValidation Experiment\\nTo determine if the TAI and TE mechanisms could decrease \\nnegative transfer in MTL training contexts, a cross-\\xadvalidation \\nprocedure was executed. For every dataset mentioned in \\nSubsection\\xa04.1, the constituent sets were merged into one con-\\nsolidated set. Subsequently, 5-\\xadfold cross-\\xadvalidation was applied \\nto the STL, MTL, MTL-\\xadTAI, and MTL-\\xadTE models.\\n4.4.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fOfficial Training-\\xadTest Split\\nFor comparison against SOTA models (Magnossão de Paula \\net\\xa0 al.\\xa0 2021; Plaza-\\xaddel-\\xadArco et\\xa0 al.\\xa0 2021, 2021a, 2021b; Pérez \\nand Luque\\xa02019; Mahdaouy et\\xa0al.\\xa02021; Mostafa et\\xa0al.\\xa02022) \\nrelevant to the datasets, an experiment was run using the \\nofficial train-\\xadtest partitions provided with these datasets. \\nHSArabic-\\xad2023 was the sole exception due to being a single \\npartition; for this, we performed a stratified 80/20 split into \\ntraining and test sets. Model training utilised the designated \\ntraining set, or a combination of training and development \\nsets if available. Post-\\xadtraining, model evaluation occurred on \\nthe test partitions.\\nFor both experimental setups, only Spanish or Arabic data sam-\\nples were employed. Models were assessed using the official \\nmetrics specific to each dataset (as described in Section\\xa04.1). In \\npractice, three evaluation metrics were required. Accuracy—used \\nfor EXIST-\\xad2021 and ArMI-\\xad2021—measures the proportion of cor-\\nrectly classified texts across both labels, which is appropriate for \\nthe moderately balanced sexism datasets. The DETOXIS-\\xad2021 and \\nHSArabic-\\xad2023 benchmark specifies the F1-\\xadscore for the Toxic \\nclass. We therefore compute precision and recall for that class \\nand report their harmonic mean to reflect performance on the \\nminority label. HatEval-\\xad2019 and OSACT-\\xad2022 rely on F1-\\xadmacro, \\ndefined as the average of the per-\\xadclass F1-\\xadscores, so that the score \\nweights positive and negative classes equally, despite dataset im-\\nbalance. When aggregating results across tasks we keep the metric \\nrequired by each dataset, ensuring that comparisons remain faith-\\nful to the official evaluation protocols.\\nAll metrics are computed on the corresponding validation or test \\nsplits for every fold or official partition. During cross-\\xadvalidation, \\nwe calculate the metric on each fold before averaging them to \\nobtain the reported values. For the training-\\xadtest experiments, \\nthe metric is derived once on the held-\\xadout test portion. This con-\\nsistent procedure allows us to attribute performance differences \\ndirectly to the presence or absence of the TA mechanisms. We \\ninvestigated MTL model versions combining two tasks and ver-\\nsions combining three tasks. The 95% confidence interval for \\nresults was computed via the formula:\\nwhere value denotes the obtained evaluation metric score (like \\naccuracy or F1-\\xadscore), nsample signifies the test set size, and \\nZ = 1.96 relates to the 95% confidence level. This calculation \\nquantifies the uncertainty associated with the reported perfor-\\nmance figures by supplying confidence intervals.\\nMargin of Error = Z ×\\n√\\nvalue × (1 −value)\\nnsample\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content=\"10 of 21\\nExpert Systems, 2026\\n5\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fResults and Analysis\\nThis section reports the outcomes of the experiments and con-\\ntrasts the performance of the models evaluated (detailed in \\nSection\\xa04). The results are displayed in two table formats. The \\nexperimental results (large tables) are organised into three parts: \\nmodel type, model's task heads, and model's performance. The \\naggregated experimental results (small tables) are organised into \\ntwo sections: model type and aggregated task heads' performance. \\nBold values indicate the highest values among all analysed mod-\\nels (column), while underlined values indicate the highest values \\namong the MTL models. We also include bar charts showcasing \\nthe best result of each model for every dataset.\\n5.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fCross-\\xadValidation Experiment\\n5.1.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fSpanish\\nThe outcomes from the Spanish cross-\\xadvalidation experiment \\nare presented in Table\\xa08. Analysis of the Baseline models (out-\\nlined in Section\\xa04.3) indicates the conventional MTL model ex-\\nperienced negative transfer in almost all scenarios. Relative to \\nthe STL model, the MTL model demonstrated improvements \\nonly in the Sexism detection task under two conditions: when \\ntrained jointly for Sexism and Hate-\\xadspeech detection, and when \\ntrained across all three tasks. For every other configuration, the \\nSTL model yielded better performance. This suggests that neg-\\native transfer likely impeded the MTL model's learning process \\nin those instances. Our findings suggest the TA mechanisms \\nsuccessfully reduced negative transfer. As detailed in Table\\xa08, \\nboth the MTL-\\xadTAI model (using the TAI mechanism) and the \\nMTL-\\xadTE model (using the TE mechanism) achieved consistently \\nsuperior performance compared to the standard MTL model. \\nFurthermore, the MTL-\\xadTAI and MTL-\\xadTE models surpassed \\nthe STL model's performance across the three assessed tasks. \\nBetween the two TA models, the MTL-\\xadTE generally performed \\nbetter than the MTL-\\xadTAI model.\\nFigure\\xa04 compares the top results of each model across the datasets in \\nthe Spanish cross-\\xadvalidation experiment. Negative transfer is evident \\nin bar charts (b) DETOXIS-\\xad2021 and (c) HatEval-\\xad2019, where the STL \\nmodel outperforms the classic MTL model. However, the MTL-\\xadTA \\nmodels (MTL-\\xadTAI and MTL-\\xadTE) effectively mitigate this negative \\ntransfer and outperform the classic MTL model. Additionally, the \\nMTL-\\xadTA models also surpass the classic MTL model in bar chart (a) \\nEXIST-\\xad2021. This indicates that negative transfer may have influ-\\nenced the learning process of the MTL model, but not to the extent \\nthat it performs worse than the STL model.\\nTable\\xa09 presents the aggregated results of the MTL models for \\nthe Spanish cross-\\xadvalidation experiment. The classic MTL model \\nperformed poorly, achieving the lowest aggregated results across \\nall task combinations. This outcome is likely attributable to the \\nnegative transfer effect hindering the model's learning capabil-\\nity. In contrast, the MTL-\\xadTAI model achieved the highest aggre-\\ngated results for Toxic-\\xadlanguage and Hate-\\xadspeech detection. The \\nMTL-\\xadTE model obtained the highest aggregated results for all \\nother task combinations. Consistent with the results in Table\\xa08, \\nthe TAI and TE mechanisms lessen the negative transfer effect \\nduring MTL training. As a result, the MTL-\\xadTAI and MTL-\\xadTE \\nmodels outperform the traditional MTL model in all cases. These \\nTABLE 8\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Spanish cross-\\xadvalidation experiment with 95% confidence intervals.\\nModel\\nTask heads\\nEXIST-\\xad2021\\nDETOXIS-\\xad2021\\nHatEval-\\xad2019\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nSTL\\nSexism\\n0.789\\u2009±\\u20090.011\\n—\\n—\\nToxic-\\xadlanguage\\n—\\n0.640\\u2009±\\u20090.014\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.846\\u2009±\\u20090.009\\nMTL\\nSexism + toxic-\\xadlanguage\\n0.788\\u2009±\\u20090.011\\n0.628\\u2009±\\u20090.014\\n—\\nSexism + hate-\\xadspeech\\n0.791\\u2009±\\u20090.011\\n—\\n0.843\\u2009±\\u20090.009\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.632\\u2009±\\u20090.014\\n0.841\\u2009±\\u20090.009\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.799\\u2009±\\u20090.010\\n0.634\\u2009±\\u20090.014\\n0.842\\u2009±\\u20090.009\\nMTL-\\xadTAI\\nSexism + toxic-\\xadlanguage\\n0.799\\u2009±\\u20090.010\\n0.649\\u2009±\\u20090.014\\n—\\nSexism + hate-\\xadspeech\\n0.805\\u2009±\\u20090.010\\n—\\n0.984\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.649\\u2009±\\u20090.014\\n0.988\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.800\\u2009±\\u20090.010\\n0.650\\u2009±\\u20090.014\\n0.980\\u2009±\\u20090.003\\nMTL-\\xadTE\\nSexism + toxic-\\xadlanguage\\n0.797\\u2009±\\u20090.011\\n0.653\\u2009±\\u20090.014\\n—\\nSexism + hate-\\xadspeech\\n0.806\\u2009±\\u20090.010\\n—\\n0.992\\u2009±\\u20090.002\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.653\\u2009±\\u20090.014\\n0.980\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech + Sexism\\n0.801\\u2009±\\u20090.010\\n0.659\\u2009±\\u20090.014\\n0.988\\u2009±\\u20090.003\\nNote: Evaluation metric values are shown with their 95% confidence intervals. Bold values indicate the highest scores across all analysed models, while underlined \\nvalues denote the highest scores among the MTL models.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 10}, page_content=\"11 of 21\\nExpert Systems, 2026\\nresults demonstrate the superiority of the MTL-\\xadTA approach \\nover the traditional MTL model, owing to its ability to mitigate \\nthe negative transfer phenomenon.\\n5.1.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fArabic\\nTable\\xa010 shows the results of the Arabic cross-\\xadvalidation exper-\\niment. Comparing the outcomes of the classic MTL and STL \\nmodels, we observe that the MTL model performs worse due to \\nthe negative transfer phenomenon. The STL model outperforms \\nthe classic MTL model in Sexism and Offensive-\\xadlanguage detec-\\ntion. The MTL-\\xadTA approach demonstrates superior performance \\ncompared to the traditional MTL approach in two of the three \\ntasks. Among the MTL models, the MTL-\\xadTAI model achieves the \\nbest performance for Sexism detection, while the MTL-\\xadTE model \\nobtains the highest F1-\\xadmacro score for Offensive-\\xadlanguage detec-\\ntion. The TA mechanisms effectively minimise negative transfer, \\nleading to consistent improvements. For Hate-\\xadspeech detection, \\nthe traditional MTL approach performs slightly better than the \\nMTL-\\xadTA models. Overall, the results in Table\\xa010 suggest that in-\\ncorporating TAI and TE mechanisms in MTL models significantly \\nenhances performance, reducing the adverse effects of negative \\ntransfer. The MTL-\\xadTE model, in particular, demonstrates the best \\noverall performance across the evaluated tasks, making it the \\nmost effective model for the Arabic cross-\\xadvalidation experiment.\\nFigure\\xa05 compares the top result of each model across the data-\\nsets in the Arabic cross-\\xadvalidation experiment. Negative trans-\\nfer is present in bar charts (a) ArMI-\\xad2021 and (c) OSACT-\\xad2022, \\nwhere the STL model outperforms the classic MTL model. In \\nboth cases, the MTL-\\xadTAI and MTL-\\xadTE models show slight im-\\nprovement over the classic MTL model. This suggests that the \\ntask-\\xadaware capability helped to mitigate the negative transfer \\neffect at least partially.\\nTable\\xa011 presents the aggregated results of the MTL models for \\nthe Arabic cross-\\xadvalidation experiment. The classic MTL model \\nperformed poorly, achieving the lowest aggregated results in \\nthree out of the four task combinations. This is due to the neg-\\native transfer phenomenon that compromised MTL model's \\nlearning process. In contrast, the MTL-\\xadTAI model achieved the \\nhighest aggregated results for the Sexism, Toxic-\\xadlanguage and \\nHate-\\xadspeech task combination. The MTL-\\xadTE model obtained the \\nhighest aggregated results for the combinations of Sexism and \\nOffensive-\\xadlanguage tasks and Offensive-\\xadlanguage and Hate-\\xad\\nspeech tasks. Consistent with the results in Table\\xa010, the TAI \\nand TE mechanisms mitigate the negative transfer effect during \\nMTL training. As a result, the MTL-\\xadTAI and MTL-\\xadTE models \\noutperform the traditional MTL model in three out of the four \\ntask combinations. These results demonstrate the superiority of \\nthe MTL-\\xadTA approach over the traditional MTL model, owing to \\nits ability to mitigate the negative transfer phenomenon.\\nFIGURE 4\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Spanish cross-\\xadvalidation \\nexperiment. The bars include 95% Confidence Intervals at the top. (a) Displays the models' best result for sexism detection on the EXIST-\\xad2021 dataset; \\n(b) showcases the models' best result for toxic language detection on the DETOXIS-\\xad2021 dataset; (c) illustrates the models' best result for Hate Speech \\ndetection on the HatEval-\\xad2019 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) EXIST-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) DETOXIS-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) HatEval-2019\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\nTABLE 9\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Spanish cross-\\xadvalidation results for the MTL models by model type. Bold values indicate the highest score across all \\nanalysed models within a column.\\nModels\\nTask Heads\\nSexism\\nSexism\\nSexism\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.708\\n0.817\\n0.737\\n0.758\\nMTL-\\xadTAI\\n0.724\\n0.895\\n0.819\\n0.810\\nMTL-\\xadTE\\n0.725\\n0.899\\n0.817\\n0.816\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 11}, page_content=\"12 of 21\\nExpert Systems, 2026\\n5.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fOfficial Training-\\xadTest Split\\n5.2.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fSpanish\\nThe experiment involving the three Spanish datasets using their \\nofficial train-\\xadtest partitions is detailed in Table\\xa012. The results \\nindicate that MTL training did not benefit the standard MTL \\nmodel for the Sexism detection task, yielding lower accuracy \\nthan the STL model. This outcome is likely a consequence of the \\nnegative transfer phenomenon. Nevertheless, the MTL-\\xadTAI and \\nMTL-\\xadTE models, incorporating TA mechanisms, counteracted \\nthe negative transfer observed during standard MTL train-\\ning. They achieved superior accuracy compared to both the \\nSTL model and the SOTA model for EXIST-\\xad2021 (AI-\\xadUPV \\n(Magnossão de Paula et\\xa0al.\\xa02021)). For Toxic-\\xadlanguage detection, \\nMTL training yielded better results than the STL baseline in the \\ntraining-\\xadtest setup. Broadly, the MTL, MTL-\\xadTAI, and MTL-\\xadTE \\nmodels produced comparable outcomes, suggesting minimal \\nnegative transfer effects for this specific task during the stan-\\ndard MTL training process. Table\\xa0 12 also demonstrates that \\nMTL training enhanced performance for Hate-\\xadspeech detec-\\ntion. The MTL model registered a higher F1-\\xadmacro score than \\nTABLE 10\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Arabic cross-\\xadvalidation experiment with 95% confidence intervals. Evaluation metric values are shown with their 95% \\nconfidence intervals.\\nModel\\nTask Heads\\nArMI-\\xad2021\\nHSArabic-\\xad2023\\nOSACT-\\xad2022\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nSTL\\nSexism\\n0.884\\u2009±\\u20090.006\\n—\\n—\\nOffensive-\\xadlanguage\\n—\\n0.608\\u2009±\\u20090.008\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.778\\u2009±\\u20090.007\\nMTL\\nSexism + offensive-\\xadlanguage\\n0.874\\u2009±\\u20090.007\\n0.617\\u2009±\\u20090.008\\n—\\nSexism + hate-\\xadspeech\\n0.876\\u2009±\\u20090.007\\n—\\n0.777\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.603\\u2009±\\u20090.008\\n0.771\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.878\\u2009±\\u20090.007\\n0.613\\u2009±\\u20090.008\\n0.770\\u2009±\\u20090.007\\nMTL-\\xadTAI\\nSexism + offensive-\\xadlanguage\\n0.883\\u2009±\\u20090.006\\n0.612\\u2009±\\u20090.008\\n—\\nSexism + hate-\\xadspeech\\n0.880\\u2009±\\u20090.006\\n—\\n0.766\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.603\\u2009±\\u20090.008\\n0.770\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.883\\u2009±\\u20090.006\\n0.618\\u2009±\\u20090.008\\n0.776\\u2009±\\u20090.007\\nMTL-\\xadTE\\nSexism + offensive-\\xadlanguage\\n0.881\\u2009±\\u20090.006\\n0.620\\u2009±\\u20090.008\\n—\\nSexism + hate-\\xadspeech\\n0.878\\u2009±\\u20090.007\\n—\\n0.771\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + Hate-\\xadspeech\\n—\\n0.619\\u2009±\\u20090.008\\n0.772\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.883\\u2009±\\u20090.006\\n0.618\\u2009±\\u20090.008\\n0.772\\u2009±\\u20090.007\\nNote: Bold values indicate the highest scores across all analysed models, while underlined values denote the highest scores among the MTL models.\\nFIGURE 5\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Arabic cross-\\xadvalidation ex-\\nperiment. The bars include 95% confidence intervals at the top. (a) Displays the models' best result for Sexism detection on the ArMI-\\xad2021 dataset; (b) \\nshowcases the models' best result for Toxic Language detection on the HSArabic-\\xad2023 dataset; (c) illustrates the models' best result for hate speech \\ndetection on the OSACT-\\xad2022 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) ArMI-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) HSArabic-2023\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) OSACT-2022\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 12}, page_content=\"13 of 21\\nExpert Systems, 2026\\nboth the HatEval-\\xad2019 SOTA (Atalaya (Pérez and Luque\\xa02019)) \\nand the STL baseline. Models equipped with TA mechanisms \\nfurther boosted these results, effectively lessening the negative \\ntransfer associated with conventional MTL training and yield-\\ning higher F1-\\xadmacro scores than the standard MTL approach.\\nFigure\\xa0 6 compares the top results of each model across the \\ndatasets in the Spanish training-\\xadtest experiment. Bar chart (a) \\nEXIST-\\xad2021 shows negative transfer, where the STL model out-\\nperforms the classic MTL model. However, the MTL-\\xadTA mod-\\nels (MTL-\\xadTAI and MTL-\\xadTE) effectively mitigate this negative \\ntransfer, outperforming the classic MTL model. In bar chart (c) \\nHatEval-\\xad2019, the MTL-\\xadTA models also surpass the classic MTL \\nmodel. This suggests negative transfer happened during train-\\ning, even though the classic MTL's results were higher than the \\nSTL model's in this instance.\\nTable\\xa013 displays the aggregated results of the MTL models for \\nthe Spanish official training-\\xadtest split experiment. The tradi-\\ntional MTL model performed the worst, achieving the lowest \\naggregate results across all task combinations. This poor perfor-\\nmance is attributed to the negative transfer phenomenon, which \\nimpaired the model's learning during training and resulted in \\nsubpar performance on the test. The MTL-\\xadTAI model achieved \\nthe best results for the task combinations of Sexism and Hate-\\xad\\nspeech, and Toxic-\\xadlanguage and Hate-\\xadspeech. Along with the \\nTABLE 11\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Arabic cross-\\xadvalidation results for the MTL models, shown by model type.\\nModels\\nTask Heads\\nSexism\\nSexism\\nSexism\\nOffensive-\\xadlanguage\\nOffensive-\\xadlanguage\\nOffensive-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.746\\n0.826\\n0.687\\n0.754\\nMTL-\\xadTAI\\n0.748\\n0.823\\n0.687\\n0.759\\nMTL-\\xadTE\\n0.751\\n0.825\\n0.696\\n0.758\\nNote: Bold values indicate the highest score across all analysed models within a column.\\nTABLE 12\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Spanish training-\\xadtest experiment with 95% confidence intervals.\\nModel\\nTask heads\\nEXIST-\\xad2021\\nDETOXIS-\\xad2021\\nHatEval-\\xad2019\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nAI-\\xadUPV\\n—\\n0.790\\u2009±\\u20090.018\\n—\\n—\\nSINAI\\n—\\n—\\n0.646\\u2009±\\u20090.031\\n—\\nAtalaya\\n—\\n—\\n—\\n0.730\\u2009±\\u20090.022\\nSTL\\nSexism\\n0.790\\u2009±\\u20090.017\\n—\\n—\\nToxic-\\xadlanguage\\n—\\n0.620\\u2009±\\u20090.032\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.764\\u2009±\\u20090.021\\nMTL\\nSexism + toxic-\\xadlanguage\\n0.776\\u2009±\\u20090.018\\n0.639\\u2009±\\u20090.032\\n—\\nSexism + hate-\\xadspeech\\n0.785\\u2009±\\u20090.017\\n—\\n0.778\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.593\\u2009±\\u20090.032\\n0.777\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.775\\u2009±\\u20090.018\\n0.629\\u2009±\\u20090.032\\n0.773\\u2009±\\u20090.021\\nMTL-\\xadTAI\\nSexism + toxic-\\xadlanguage\\n0.797\\u2009±\\u20090.017\\n0.633\\u2009±\\u20090.032\\n—\\nSexism + hate-\\xadspeech\\n0.809\\u2009±\\u20090.017\\n—\\n0.789\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.628\\u2009±\\u20090.032\\n0.790\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.792\\u2009±\\u20090.017\\n0.629\\u2009±\\u20090.032\\n0.782\\u2009±\\u20090.020\\nMTL-\\xadTE\\nSexism + toxic-\\xadlanguage\\n0.804\\u2009±\\u20090.017\\n0.626\\u2009±\\u20090.032\\n—\\nSexism + hate-\\xadspeech\\n0.804\\u2009±\\u20090.017\\n—\\n0.786\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.623\\u2009±\\u20090.032\\n0.786\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.802\\u2009±\\u20090.017\\n0.633\\u2009±\\u20090.032\\n0.789\\u2009±\\u20090.020\\nNote: Evaluation metric values are shown with their 95% confidence intervals. Bold values indicate the highest scores across all analysed models, while underlined \\nvalues denote the highest scores among the MTL models.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 13}, page_content=\"14 of 21\\nExpert Systems, 2026\\nMTL-\\xadTE model, it also achieved top results for the combination \\nof Sexism and Toxic-\\xadlanguage. Additionally, the MTL-\\xadTE model \\nachieved the highest results for the combination of all three \\ntasks. The aggregated results demonstrate that the TAI and TE \\nmechanisms alleviated the negative transfer phenomenon. In all \\ncases, models equipped with these mechanisms outperformed \\nthe traditional MTL model, achieving superior aggregated \\nresults.\\n5.2.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fArabic\\nThe study conducted on the three Arabic datasets using their \\nspecified training-\\xadtest partitions is documented in Table\\xa014. \\nObservations from the table indicate that standard MTL train-\\ning did not improve results for the Sexism detection task com-\\npared to STL. The peak accuracy achieved by the conventional \\nMTL model matched that of the STL model. This lack of im-\\nprovement is ascribed to the negative transfer effect constrain-\\ning the MTL model's learning phase. Conversely, the MTL-\\xadTAI \\nand MTL-\\xadTE models, utilising TA mechanisms, lessened the \\nnegative transfer observed in the standard MTL approach, \\nyielding higher accuracy than both STL and standard MTL \\nmodels. For the Sexism detection task, the SOTA model from \\nArMI-\\xad2021, UM6P-\\xadNLP (Mahdaouy et\\xa0 al.\\xa0 2021), produced \\nthe best outcome. Regarding Offensive-\\xadlanguage detection, \\nMTL training showed enhanced results over the STL baseline \\nwithin the training-\\xadtest framework. Models incorporating TA \\nmechanisms further amplified these gains, effectively reduc-\\ning negative transfer from traditional MTL training and re-\\nsulting in a better F1-\\xadscore than the conventional MTL model. \\nNotably, the MTL-\\xadTE model recorded the highest F1-\\xadscore for \\nOffensive-\\xadlanguage detection, establishing a new SOTA for \\nthe HSArabic-\\xad2023 dataset. As seen in Table\\xa014, MTL train-\\ning also led to better outcomes for Hate-\\xadspeech detection, with \\nthe standard MTL model scoring a higher F1-\\xadmacro than the \\nSTL baseline. The MTL-\\xadTA models (MTL-\\xadTAI and MTL-\\xadTE) \\nadvanced these results further by mitigating the negative \\ntransfer effects present in standard MTL training, achieving \\nsuperior F1-\\xadmacro scores compared to the traditional MTL \\nmodel. The top performance for Hate-\\xadspeech detection was \\nby the OSACT-\\xad2021 SOTA model, GOF (Mostafa et\\xa0al.\\xa02022), \\nwhile among the MTL variants, the MTL-\\xadTE model achieved \\nthe best result for this task. Across all evaluated scenarios in \\nthe Arabic training-\\xadtest split experiment, the MTL-\\xadTA models \\nconsistently delivered superior results relative to the classic \\nMTL model.\\nFigure\\xa0 7 compares the top results of each model across the \\ndatasets in the Arabic training-\\xadtest experiment. Negative \\ntransfer is not clearly observed in any of the charts, as the \\nclassic MTL model consistently outperforms the STL model. \\nFIGURE 6\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Spanish training-\\xadtest exper-\\niment. The bars include 95% confidence intervals at the top. (a) Displays the models' best result for sexism detection on the EXIST-\\xad2021 dataset; (b) \\nshowcases the models' best result for Toxic language detection on the DETOXIS-\\xad2021 dataset; (c) illustrates the models' best result for hate speech \\ndetection on the HatEval-\\xad2019 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) EXIST-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) DETOXIS-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) HatEval-2019\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\nAI-UPV\\nSINAI\\nAtalaya\\nTABLE 13\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Spanish training-\\xadtest results for the MTL models by model type.\\nModels\\nTask Heads\\nSexism\\nSexism\\nSexism\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.708\\n0.782\\n0.685\\n0.726\\nMTL-\\xadTAI\\n0.715\\n0.799\\n0.709\\n0.731\\nMTL-\\xadTE\\n0.715\\n0.795\\n0.702\\n0.738\\nNote: Bold values indicate the highest score across all analysed models within a column.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 14}, page_content=\"15 of 21\\nExpert Systems, 2026\\nHowever, in all three charts, at least one of the MTL-\\xadTA mod-\\nels (MTL-\\xadTAI and MTL-\\xadTE) surpasses the classic MTL model, \\ndemonstrating their ability to mitigate negative transfer and \\nimprove performance.\\nTable\\xa015 displays the aggregated results of the MTL models for \\nthe Arabic official training-\\xadtest split experiment. The classic \\nMTL model performed poorly, achieving the lowest aggregated \\nresults in three of the four task combinations. This likely stems \\nfrom the negative transfer effect impeding the MTL model's \\nlearning progress. The MTL-\\xadTAI model obtained higher ag-\\ngregated results than the MTL model in all cases except for the \\nSexism and Offensive-\\xadlanguage task combination, where the \\ndifference was marginal. The MTL-\\xadTE model achieved the high-\\nest aggregated results for all task combinations. In line with the \\nresults from Table\\xa014, the TAI and TE mechanisms reduce the \\nTABLE 14\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Arabic training-\\xadtest experiment with 95% confidence intervals.\\nModel\\nTask Heads\\nArMI-\\xad2021\\nHSArabic-\\xad2023\\nOSACT-\\xad2022\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nUM6P-\\xadNLP\\n—\\n0.919 ±\\u20090.012\\n—\\n—\\nGOF\\n—\\n—\\n—\\n0.852 ±\\u20090.014\\nSTL\\nSexism\\n0.892 ±\\u20090.014\\n—\\n—\\nOffensive-\\xadlanguage\\n—\\n0.605 ±\\u20090.017\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.775 ±\\u20090.016\\nMTL\\nSexism + offensive-\\xadlanguage\\n0.892 ±\\u20090.014\\n0.617 ±\\u20090.017\\n—\\nSexism + hate-\\xadspeech\\n0.890 ±\\u20090.014\\n—\\n0.767 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.613 ±\\u20090.017\\n0.768 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.884 ±\\u20090.014\\n0.610 ±\\u20090.017\\n0.790 ±\\u20090.016\\nMTL-\\xadTAI\\nSexism + offensive-\\xadlanguage\\n0.888 ±\\u20090.014\\n0.617 ±\\u20090.017\\n—\\nSexism + hate-\\xadspeech\\n0.895 ±\\u20090.014\\n—\\n0.776 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.625 ±\\u20090.017\\n0.786 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.888 ±\\u20090.014\\n0.630 ±\\u20090.017\\n0.786 ±\\u20090.016\\nMTL-\\xadTE\\nSexism + offensive-\\xadlanguage\\n0.893 ±\\u20090.014\\n0.632 ±\\u20090.017\\n—\\nSexism + hate-\\xadspeech\\n0.889 ±\\u20090.014\\n—\\n0.790 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.634 ±\\u20090.017\\n0.786 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.894 ±\\u20090.014\\n0.635 ±\\u20090.017\\n0.794 ±\\u20090.016\\nNote: Evaluation metric values are shown with their 95% confidence intervals. Bold values indicate the highest scores across all analysed models, while underlined \\nvalues denote the highest scores among the MTL models.\\nFIGURE 7\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Arabic training-\\xadtest exper-\\niment. The bars include 95% confidence intervals at the top. (a) Displays the models' best result for sexism detection on the ArMI-\\xad2021 dataset; (b) \\nshowcases the models' best result for toxic language detection on the HSArabic-\\xad2023 dataset; (c) illustrates the models' best result for hate speech \\ndetection on the OSACT-\\xad2022 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) ArMI-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) HSArabic-2023\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) OSACT-2022\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\nUM6P-NLP\\nGOF\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content=\"16 of 21\\nExpert Systems, 2026\\nnegative transfer impact during MTL training. As a result, the \\nMTL-\\xadTAI and MTL-\\xadTE models outperform the traditional MTL \\nmodel in all cases.\\n6\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fDiscussion and Limitations\\n6.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fDiscussion\\nThe results from our experiments demonstrate the impact of \\nnegative transfer on the performance of traditional MTL mod-\\nels. Across both the Spanish and Arabic datasets, the classic \\nMTL models consistently underperformed compared to STL \\nmodels, particularly in tasks prone to negative transfer phe-\\nnomena such as Sexism and Offensive Language detection. \\nHowever, the introduction of TA mechanisms (TAI and TE) \\nsignificantly mitigated the effects of negative transfer. Both \\nMTL-\\xadTAI and MTL-\\xadTE models showed improvements over \\nthe traditional MTL models, achieving higher accuracy and F1 \\nscores in almost all task combinations. In the Spanish cross-\\xad\\nvalidation experiment, the MTL-\\xadTE model outperformed all \\nothers for the three evaluated tasks. The MTL-\\xadTAI model ex-\\ncelled for Sexism and Hate-\\xadspeech detection in the Spanish \\nofficial training-\\xadtest split and the MTL-\\xadTE model became the \\nnew DETOXIS-\\xad2021 SOTA model for Toxic-\\xadlanguage detec-\\ntion. These findings were consistent across the aggregated re-\\nsults evaluation for cross-\\xadvalidation and official training-\\xadtest \\nsplit, where the MTL-\\xadTA models continued to achieve superior \\nresults than the classic MTL model.\\nThese gains align with the linguistic overlap among the Spanish \\ndatasets: sexism, toxic language, and hate speech corpora share \\nrecurring lexical markers (e.g., slurs and gendered stereotypes) \\nbut differ in their annotation scope. By conditioning the en-\\ncoder on the task through TAI, the model learns to differentiate \\nwhen a term should be interpreted as toxic rhetoric versus ex-\\nplicitly sexist and hate speech. TE further exploits this structure \\nby amplifying shared signals—such as intensity modifiers and \\ntarget mentions—only for the tasks that benefit from them. As \\na result, the TA-\\xadequipped models can capitalise on beneficial \\ncross-\\xadtask cues while suppressing misleading correlations that \\ncaused the baseline MTL model to underperform. Similarly, \\nfor the Arabic cross-\\xadvalidation experiment, the MTL-\\xadTE model \\noutperformed all others in Offensive Language and obtained \\ncompetitive results for Sexism and Hate-\\xadspeech detection. In \\nthe official training-\\xadtest split experiment, the MTL-\\xadTAI obtained \\nthe top result for Offensive-\\xadlanguage detection and became the \\nnew HSArabic-\\xad2023 SOTA model for the task. These results \\nwere consistent throughout the aggregated evaluations for cross-\\xad\\nvalidation and the official training-\\xadtest split, where the MTL-\\xadTA \\nmodels consistently outperformed the traditional MTL model.\\nArabic datasets exhibit stronger dialectal variation and class \\nimbalance than their Spanish counterparts, which amplifies \\nnegative transfer when models rely solely on shared represen-\\ntations. Injecting task context via TAI helps the encoder focus \\non morphological patterns that are discriminative for each phe-\\nnomenon (e.g., misogynistic verb forms versus generic insults), \\nwhile TE recalibrates the representation to handle skewed label \\ndistributions by emphasising features that consistently charac-\\nterise the minority class. This explains why the largest relative \\nimprovements arise on HSArabic-\\xad2023, where offensive con-\\ntent is substantially rarer than neutral statements. The analy-\\nsis of the bar charts presenting the best results for each model \\nacross all datasets reveals a consistent pattern: when negative \\ntransfer is identified—indicated by the classic MTL model un-\\nderperforming the STL model—the MTL-\\xadTA model effectively \\nmitigates this issue, outperforming both the classic MTL and \\nSTL models. Furthermore, the MTL-\\xadTA model often achieves \\nsuperior performance compared to the classic MTL model, even \\nin scenarios where negative transfer is not evident, as demon-\\nstrated by the comparative analysis of the classic MTL and STL \\nmodels' results. In summary, the incorporation of TAI and TE \\nmechanisms in MTL models not only provides a robust solu-\\ntion to the negative transfer problem but also opens up new \\npossibilities for enhancing overall performance. The MTL-\\xadTAI \\nand MTL-\\xadTE models emerge as the most effective MTL models \\nacross the evaluated tasks, demonstrating the exciting potential \\nfor performance improvement in MTL scenarios.\\nAcross both languages, a common trend is that TA mitigates sit-\\nuations where dataset-\\xadspecific annotation guidelines diverge. By \\nexplicitly signalling the task objective, the model can maintain \\nseparate decision boundaries for nuanced categories while still \\nsharing underlying lexical knowledge. This supports our hy-\\npothesis that negative transfer stems from conflating task intent \\nrather than from a lack of shared information. Consequently, TA \\ndelivers the most benefit when tasks are semantically related yet \\noperationalised differently—a regime that typifies harmful lan-\\nguage detection benchmarks.\\n6.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fLimitations\\nDespite the promising results demonstrated by the incorpora-\\ntion of TAI and TE mechanisms in MTL models, several lim-\\nitations should be acknowledged. Firstly, the negative transfer \\nphenomenon remains a challenge, especially for certain task \\ncombinations. While the TA mechanisms effectively mitigate \\nthis issue, the extent of their effectiveness is not fully known. \\nWe are still unable to determine whether the TA mechanisms \\nentirely eliminate negative transfer, and traditional MTL models \\ncontinue to suffer from its effects, particularly in tasks such as \\nSexism and Offensive Language detection. The evaluation is lim-\\nited to specific datasets and tasks within the Spanish and Arabic \\nlanguages. This scope may not fully capture the generalisability \\nTABLE 15\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Arabic training-\\xadtest results for the MTL \\nmodels by model type.\\nModels\\nTask heads\\nSexism\\nSexism\\nOffensive-\\xad\\nlanguage\\nHate-\\xad\\nspeech\\nOffensive-\\xad\\nlanguage\\nHate-\\xad\\nspeech\\nMTL\\n0.755\\n0.828\\n0.691\\n0.761\\nMTL-\\xadTAI\\n0.752\\n0.835\\n0.705\\n0.768\\nMTL-\\xadTE\\n0.763\\n0.839\\n0.710\\n0.774\\nNote: Bold values indicate the highest score across all analysed models within a \\ncolumn.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content=\"17 of 21\\nExpert Systems, 2026\\nof the findings across different languages, datasets, or task do-\\nmains. Future work should explore a broader range of datasets \\nand tasks to validate the robustness of the proposed mecha-\\nnisms. Additionally, achieving strong performance with the two \\nMTL-\\xadTA models hinges on employing a potent encoder. This \\ndependence could pose difficulties for computational systems \\nwith limited resources that lack the capacity for deep learning \\nframeworks like Transformers (Vaswani et\\xa0al.\\xa02017) as the en-\\ncoder. The inclusion of supplementary layers and mechanisms \\nmight also escalate computational requirements, potentially \\nrestricting the scalability of these models for practical, real-\\xad\\nworld uses. A detailed examination of the balance between \\nperformance gains and computational overhead is warranted. \\nHandling an increased number of tasks necessitates more task \\nheads, consequently enlarging the model's parameter count. As \\na result, fine-\\xadtuning MTL-\\xadTA models demands greater compu-\\ntational resources. This escalation in resource needs could pres-\\nent a considerable barrier for applications operating under tight \\ncomputational constraints. A further consideration is whether \\nthe fine-\\xadtuning process, which uses task-\\xadspecific information, \\ndiminishes the MTL-\\xadTA models' capacity for adapting to novel, \\npreviously unseen tasks (e.g., in few-\\xadshot learning or instruction-\\xad\\nfollowing scenarios). The specialisation towards specific tasks \\nduring fine-\\xadtuning could potentially impede their adaptability \\nand generalisation capabilities for new tasks, an area deserving \\ninvestigation in future studies.\\n7\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fConclusion and Future Work\\nThis paper introduced the TA strategy aimed at tackling the \\nnegative transfer issue (Wu, Zhang, and Ré\\xa02020; Vandenhende \\net\\xa0 al.\\xa0 2022; Li et\\xa0 al.\\xa0 2023) encountered during MTL training \\nphases. Our approch presented two distinct mechanisms: TAI \\nand TE. The TAI mechanism enhances the MTL model encoder's \\ninput by integrating task description details. Concurrently, the \\nTE method adds a TEB, an extra module processing the encod-\\ner's latent output alongside a Task Identification Vector (TIV). \\nThrough these mechanisms, the MTL model can generate task-\\xad\\ntailored representations, which effectively reduce negative trans-\\nfer effects and boost overall model performance.\\nOur experimental results demonstrate that the TA mechanisms sig-\\nnificantly reduce negative transfer and improve performance over \\nstandard MTL models across different tasks. Notably, we achieved \\ncompetitive results compared to SOTA methods for both the \\nSpanish and Arabic datasets. The proposed models set new SOTA \\nbenchmarks on the EXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021) \\nand HatEval-\\xad2019 (Basile et\\xa0al.\\xa02019) datasets for Spanish, as well \\nas on the HSArabic-\\xad2023 dataset for Arabic. These findings under-\\nscore the generalisability and effectiveness of the TA approach in \\nmitigating negative transfer across different languages and tasks.\\nBeyond the immediate results, the broader impact of our ap-\\nproach lies in its potential to shape future research in NLP. By \\nenabling more accurate and efficient MTL systems, the intro-\\nduction of TA mechanisms paves the way for improved detection \\nand moderation of harmful content on social media platforms. \\nThis has significant implications for the development of auto-\\nmated systems tasked with moderating online spaces, reducing \\nhuman bias, and fostering safer digital environments.\\nFurthermore, the ability of TA-\\xadequipped models to enhance per-\\nformance across multiple languages and tasks suggests broader \\napplicability in multilingual and cross-\\xadlinguistic NLP challenges. \\nThis opens the door for future research to explore TA in other \\nlanguages and domains, where traditional single-\\xadtask models \\noften struggle due to data scarcity and computational constraints.\\nFor future work, it would be valuable to further investigate the \\nminimum amount of labelled data or information volume re-\\nquired for MTL to outperform STL models. Additionally, exploring \\nthe augmentation of MTL models with low-\\xadlevel task supervision, \\nwhere the decoder leverages the entirety or a portion of the en-\\ncoder's hidden states, could provide further performance gains. \\nWe also plan to extend the application of MTL combined with \\nTA into novel areas, such as identifying Sexism within learning-\\xad\\nwith-\\xaddisagreement paradigms (Uma et\\xa0al.\\xa02021; Plaza et\\xa0al.\\xa02024; \\nPlaza, de Carrillo-\\xadAlbornoz, Morante, Amigó, et\\xa0al.\\xa02023; Plaza, \\nde Carrillo-\\xadAlbornoz, Morante, Gonzalo, et\\xa0al.\\xa02023), where mul-\\ntiple annotator labels are considered rather than relying on a sin-\\ngle aggregated gold label (Frenda et\\xa0al.\\xa02025).\\nFinally, future research will also focus on incorporating unsu-\\npervised learning techniques to enhance the proposed models for \\nidentifying Hate Speech, Toxic Language, and Sexism. Potential \\ntechniques include Latent Dirichlet Allocation (Blei et\\xa0al.\\xa02003), \\nSelf-\\xadOrganising Maps (Miljković\\xa02017), and K-\\xadMeans Clustering \\n(Ezugwu et\\xa0al.\\xa02022), which could offer further improvements \\nin model robustness and accuracy across different linguistic and \\ncultural contexts.\\nAuthor Contributions\\nAngel Felipe Magnossão de Paula: conceptualization, data cura-\\ntion, formal analysis, investigation, methodology, resources, software, \\nvalidation, writing – original draft. Imene Bensalem: data curation, \\nresources, supervision, writing – original draft. Damiano Spina: \\nconceptualization, methodology, resources, supervision, funding ac-\\nquisition, writingen – original draft. Paolo Rosso: conceptualization, \\nmethodology, resources, supervision, funding acquisition.\\nAcknowledgements\\nTThis research is partially supported by the Australian Research Council \\n(ARC) Centre of Excellence for Automated Decision-Making and Society \\n(ADM+S, CE200100005).  The research work of Paolo Rosso was in the \\nframework of the Malicious Actors Profiling and Detection in Online \\nSocial Networks Through Artificial Intelligence (MARTINI) project(Grant \\nPCI2022-135008-2) funded by MCIN/AEI/ 10.13039/501100011033 and \\nby European Union NextGenerationEU/PRTR.\\nFunding\\nThis work was supported by MCIN/AEI/10.13039/501100011033 \\n(PCI2022-\\xad135008-\\xad2); European Union NextGenerationEU/PRTR.\\nConflicts of Interest\\nThe authors declare no conflicts of interest.\\nData Availability Statement\\nThe data that support the findings of this study are available on request \\nfrom the corresponding sources. The data are not publicly available due \\nto privacy or ethical restrictions.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content=\"18 of 21\\nExpert Systems, 2026\\nEndnotes\\n\\t1\\thttps://\\u200bgithub.\\u200bcom/\\u200bAngel\\u200bFelip\\u200beMP/\\u200bArabi\\u200bc-\\xad\\u200bMulti\\u200bTask-\\xad\\u200bLearning.\\nReferences\\nAbburi, H., P. Parikh, N. Chhaya, and V. Varma. 2020. “Semi-\\xadSupervised \\nMulti-\\xadTask Learning for Multi-\\xadLabel Fine-\\xadGrained Sexism Classification.” \\nIn Proceedings of the 28th International Conference on Computational \\nLinguistics, 5810–5820. International Committee on Computational \\nLinguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2020.\\u200bcolin\\u200bg-\\xad\\u200bmain.\\u200b511.\\nAbdelali, A., S. Hassan, H. Mubarak, K. Darwish, and Y. Samih. 2021. \\n“Pre-\\xadTraining BERT on Arabic Tweets: Practical Considerations.” \\narXiv:2102.10684.\\nAbdul-\\xadMageed, M., and A. Elmadany. 2021. “ARBERT & MARBERT: \\nDeep Bidirectional Transformers for Arabic.” In Proceedings of the 59th \\nAnnual Meeting of the Association for Computational Linguistics and \\nthe 11th International Joint Conference on Natural Language Processing \\n(Volume 1: Long Papers), 7088–7105.\\nAbu Farha, I., and W. Magdy. 2020. “Multitask Learning for Arabic \\nOffensive Language and Hate-\\xadSpeech Detection.” In Proceedings of the \\n4th Workshop on Open-\\xadSource Arabic Corpora and Processing Tools, \\nWith a Shared Task on Offensive Language Detection, edited by H. Al-\\xad\\nKhalifa, W. Magdy, K. Darwish, T. Elsayed, and H. Mubarak, 86–90. \\nEuropean Language Resource Association. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b\\n2020.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b14.\\nAguilar, G., S. Maharjan, A. P. López-\\xadMonroy, and T. Solorio. 2017. “A \\nMulti-\\xadTask Approach for Named Entity Recognition in Social Media \\nData.” In Proceedings of the 3rd Workshop on Noisy User-\\xadGenerated Text, \\nedited by L. Derczynski, W. Xu, A. Ritter, and T. Baldwin, 148–153. \\nAssociation for Computational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200b\\nv1/\\u200bW17-\\xad\\u200b4419.\\nAlkomah, F., and X. Ma. 2022. “A Literature Review of Textual Hate \\nSpeech Detection Methods and Datasets.” Information 13: 273. https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b3390/\\u200binfo1\\u200b3060273.\\nAlshaabi, T., D. R. Dewhurst, J. R. Minot, et\\xa0 al. 2021. “The Growing \\nAmplification of Social Media: Measuring Temporal and Social Contagion \\nDynamics for Over 150 Languages on Twitter for 2009–2020.” EPJ Data \\nScience 10: 15. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1140/\\u200bepjds/\\u200bs1368\\u200b8-\\xad\\u200b021-\\xad\\u200b00271\\u200b-\\xad\\u200b0.\\nAntoun, W., F. Baly, and H. Hajj. 2020. “AraBERT: Transformer-\\xadBased \\nModel for Arabic Language Understanding.” In Proceedings of the 4th \\nWorkshop on Open-\\xadSource Arabic Corpora and Processing Tools, With a \\nShared Task on Offensive Language Detection, 9–15. European Language \\nResource Association. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b2020.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b2.\\nBasile, V., C. Bosco, E. Fersini, et\\xa0 al. 2019. “SemEval-\\xad2019 Task 5: \\nMultilingual Detection of Hate Speech Against Immigrants and \\nWomen in Twitter.” In Proceedings of the 13th International Workshop \\non Semantic Evaluation, 54–63. Association for Computational \\nLinguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200bS19-\\xad\\u200b2007.\\nBensalem, I., P. Rosso, and H. Zitouni. 2024. “Toxic Language Detection: \\nA Systematic Review of Arabic Datasets.” Expert Systems 41: e13551. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1111/\\u200bexsy.\\u200b13551\\u200b.\\nBlei, D. M., A. Y. Ng, and M. I. Jordan. 2003. “Latent Dirichlet \\nAllocation.” Journal of Machine Learning Research 3: 993–1022. https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b944919.\\u200b944937.\\nBlizard, W. D. 1988. “Multiset Theory.” Notre Dame Journal of Formal \\nLogic 30: 36–66. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1305/\\u200bndjfl/\\u200b10936\\u200b34995\\u200b.\\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017. “Enriching \\nWord Vectors With Subword Information.” Transactions of the \\nAssociation for Computational Linguistics 5: 135–146. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1162/\\u200btacl_a_\\u200b00051\\u200b.\\nBoser, B. E., I. M. Guyon, and V. N. Vapnik. 1992. “A Training Algorithm \\nfor Optimal Margin Classifiers.” In Proceedings of the Fifth Annual \\nWorkshop on Computational Learning Theory COLT '92, 144–152. \\nAssociation for Computing Machinery. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1145/\\u200b130385.\\u200b\\n130401.\\nCanete, J., G. Chaperon, R. Fuentes, J.-\\xadH. Ho, H. Kang, and J. Pérez. \\n2020. “Spanish Pre-\\xadtrained Bert Model and Evaluation Data.” In \\nPractical Machine Learning for Developing Countries (PML4DC) at \\nEleventh International Conference on Learning Representations (ICLR), \\n2020, 1–10. https://\\u200bpml4dc.\\u200bgithub.\\u200bio/\\u200biclr2\\u200b020/\\u200bpapers/\\u200bPML4D\\u200bC2020_\\u200b\\n10.\\u200bpdf.\\nCaruana, R., S. Lawrence, and L. Giles. 2000. “Overfitting in Neural \\nNets: Backpropagation, Conjugate Gradient, and Early Stopping.” In \\nProceedings of the 13th International Conference on Neural Information \\nProcessing Systems NIPS'00, 381–387. MIT Press. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n5555/\\u200b30087\\u200b51.\\u200b3008807.\\nCharfi, A., M. Besghaier, R. Akasheh, A. Atalla, and W. Zaghouani. \\n2024. “Hate Speech Detection With ADHAR: A Multi-\\xadDialectal Hate \\nSpeech Corpus in Arabic.” Frontiers in Artificial Intelligence 7: 1391472. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b3389/\\u200bfrai.\\u200b2024.\\u200b1391472.\\nChen, S., Y. Zhang, and Q. Yang. 2024. “Multi-\\xadTask Learning in Natural \\nLanguage Processing: An Overview.” ACM Computing Surveys 56: 1–32. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1145/\\u200b3663363.\\nChen, Z., V. Badrinarayanan, C.-\\xadY. Lee, and A. Rabinovich. 2018. \\n“GradNorm: Gradient Normalization for Adaptive Loss Balancing in \\nDeep Multitask Networks.” In Proceedings of the 35th International \\nConference on Machine Learning (794–803). PMLR volume 80 of \\nProceedings of Machine Learning Research. http://\\u200bproce\\u200bedings.\\u200bmlr.\\u200b\\npress/\\u200bv80/\\u200bchen1\\u200b8a/\\u200bchen1\\u200b8a.\\u200bpdf.\\nCipolla, R., Y. Gal, and A. Kendall. 2018. “Multi-\\xadTask Learning Using \\nUncertainty to Weigh Losses for Scene Geometry and Semantics.” In \\n2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, \\nSalt Lake City, UT, USA, 7482–7491. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b\\n2018.\\u200b00781\\u200b.\\nDe Freitas, J. M., S. Berg, B. C. Geiger, and M. Mucke. 2022. \\n“Compressed Hierarchical Representations for Multi-\\xadTask Learning \\nand Task Clustering.” In 2022 International Joint Conference on \\nNeural Networks (IJCNN), 1–8. IEEE. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bijcnn\\u200b\\n55064.\\u200b2022.\\u200b9892342.\\nDerczynski, L., M. Guerini, D. Nozza, F. M. del Plaza-\\xadArco, J. \\nSorensen, and M. Zampieri. 2024. “Countering Hateful and Offensive \\nSpeech Online—Open Challenges.” In Proceedings of the 2024 \\nConference on Empirical Methods in Natural Language Processing: \\nTutorial Abstracts, edited by J. Li and F. Liu, 11–16. Association for \\nComputational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2024.\\u200bemnlp\\u200b\\n-\\xad\\u200btutor\\u200bials.\\u200b2.\\nDevlin, J., M.-\\xadW. Chang, K. Lee, and K. Toutanova. 2019. “BERT: \\nPre-\\xadTraining of Deep Bidirectional Transformers for Language \\nUnderstanding.” In Proceedings of the 2019 Conference of the North \\nAmerican Chapter of the Association for Computational Linguistics: \\nHuman Language Technologies, 4171–4186. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200b\\nv1/\\u200bN19-\\xad\\u200b1423.\\nDuan, L., D. Xu, and I. W. Tsang. 2012. “Learning With Augmented \\nFeatures for Heterogeneous Domain Adaptation.” In Proceedings of the \\n29th International Coference on International Conference on Machine \\nLearning ICML'12, 667–674. Omnipress. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b30425\\u200b\\n73.\\u200b3042661.\\nEzugwu, A. E., A. M. Ikotun, O. O. Oyelade, et\\xa0 al. 2022. “A \\nComprehensive Survey of Clustering Algorithms: State-\\xadof-\\xadthe-\\xadArt \\nMachine Learning Applications, Taxonomy, Challenges, and Future \\nResearch Prospects.” Engineering Applications of Artificial Intelligence \\n110: 104743. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200bengap\\u200bpai.\\u200b2022.\\u200b104743.\\nFang, L., G. Liu, and R. Zhang. 2022. “Sense-\\xadaware BERT and Multi-\\xadtask \\nFine-\\xadtuning for Multimodal Sentiment Analysis.” In 2022 International \\nJoint Conference on Neural Networks (JCNN), 1–8. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1109/\\u200bIJCNN\\u200b55064.\\u200b2022.\\u200b9892116.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content=\"19 of 21\\nExpert Systems, 2026\\nFrenda, S., G. Abercrombie, V. Basile, et\\xa0 al. 2025. “Perspectivist \\nApproaches to Natural Language Processing: A Survey.” Language \\nResources and Evaluation 59: 1719–1746. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200bs1057\\u200b\\n9-\\xad\\u200b024-\\xad\\u200b09766\\u200b-\\xad\\u200b4.\\nFrenda, S., B. Ghanem, M. M. y Gómez, and P. Rosso. 2019. “Online \\nHate Speech Against Women: Automatic Identification of Misogyny \\nand Sexism on Twitter.” Journal of Intelligent & Fuzzy Systems 36: 4743–\\n4752. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b3233/\\u200bJIFS-\\xad\\u200b179023.\\nGuo, M., A. Haque, D.-\\xadA. Huang, S. Yeung, and L. Fei-\\xadFei. 2018. \\n“Dynamic Task Prioritization for Multitask Learning.” In Computer \\nVision—ECCV 2018: 15th European Conference, Munich, Germany, \\nSeptember 8–14, 2018, Proceedings, Part XVI, 282–299. Springer-\\xadVerlag. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01270\\u200b-\\xad\\u200b0_\\u200b17.\\nIndurthi, S., M. A. Zaidi, N. Kumar Lakumarapu, et\\xa0 al. 2021. “Task \\nAware Multi-\\xadTask Learning for Speech to Text Tasks.” In ICASSP 2021–\\n2021 IEEE International Conference on Acoustics, Speech and Signal \\nProcessing (ICASSP), 7723–7727. IEEE. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bICASS\\u200b\\nP39728.\\u200b2021.\\u200b9414703.\\nJamil, S., M. Jalil Piran, and O.-\\xadJ. Kwon. 2023. “A Comprehensive \\nSurvey of Transformers for Computer Vision.” Drones 7: 287. https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b3390/\\u200bdrone\\u200bs7050287.\\nKnight, P., and R. Duan. 2023. “Multi-\\xadTask Learning With Summary \\nStatistics.” In Proceedings of the 37th International Conference on Neural \\nInformation Processing Systems NIPS '23, vol. 36, 54020–54031. Curran \\nAssociates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b36661\\u200b22.\\u200b3668472.\\nKulis, B., K. Saenko, and T. Darrell. 2011. “What You Saw Is Not What \\nYou Get: Domain Adaptation Using Asymmetric Kernel Transforms.” \\nIn Proceedings of the 2011 IEEE Conference on Computer Vision and \\nPattern Recognition CVPR '11, 1785–1792. IEEE Computer Society. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b2011.\\u200b5995702.\\nLauriola, I., A. Lavelli, and F. Aiolli. 2022. “An Introduction to Deep \\nLearning in Natural Language Processing: Models, Techniques, and \\nTools.” Neurocomputing 470: 443–456. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200bneu-\\ncom.\\u200b2021.\\u200b05.\\u200b103.\\nLi, D., H. Nguyen, and H. R. Zhang. 2023. “Identification of Negative \\nTransfers in Multitask Learning Using Surrogate Models.” Transactions \\non Machine Learning Research. https://\\u200bopenr\\u200beview.\\u200bnet/\\u200bforum?\\u200bid=\\u200b\\nKgfFA\\u200bI9f3E\\u200b.\\nLiu, Y., Y. Lu, H. Liu, et\\xa0al. 2023. “Hierarchical Prompt Learning for \\nMulti-\\xadTask Learning.” In 2023 IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR), 10888–10898. https://\\u200bdoi.\\u200borg/\\u200b\\n10.\\u200b1109/\\u200bCVPR5\\u200b2729.\\u200b2023.\\u200b01048\\u200b.\\nLong, M., Z. Cao, J. Wang, and P. S. Yu. 2017. “Learning Multiple Tasks \\nwith Multilinear Relationship Networks.” In Proceedings of the 31st \\nInternational Conference on Neural Information Processing Systems \\nNIPS'17, 1593–1602. Curran Associates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b\\n32947\\u200b71.\\u200b3294923.\\nLoshchilov, I., and F. Hutter. 2019. “Decoupled Weight Decay \\nRegularization.” In 7th International Conference on Learning \\nRepresentations, ICLR, New Orleans, LA, USA. https://\\u200bopenr\\u200beview.\\u200bnet/\\u200b\\npdf?\\u200bid=\\u200bBkg6R\\u200biCqY7\\u200b.\\nLu, Y., A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. Feris. 2017. “Fully-\\xad\\nAdaptive Feature Sharing in Multi-\\xadTask Networks With Applications in \\nPerson Attribute Classification.” In 2017 IEEE Conference on Computer \\nVision and Pattern Recognition (CVPR), 1131–1140. IEEE. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b2017.\\u200b126.\\nMagnossão de Paula, A. F., R. F. da Silva, and I. B. Schlicht. 2021. \\n“Sexism Prediction in Spanish and English Tweets Using Monolingual \\nand Multilingual BERT and Ensemble Models.” In Proceedings of the \\nIberian Languages Evaluation Forum (IberLEF 2021) Co-\\xadLocated With \\nthe XXXVII International Conference of the Spanish Society for Natural \\nLanguage Processing (SEPLN 2021), 356–373. CEUR. https://\\u200bceur-\\xad\\u200bws.\\u200b\\norg/\\u200bVol-\\xad\\u200b2943/\\u200bexist_\\u200bpaper2.\\u200bpdf.\\nMagnossão de Paula, A. F., P. Rosso, and D. Spina. 2023. “Mitigating \\nNegative Transfer With Task Awareness for Sexism, Hate Speech, and \\nToxic Language Detection.” In 2023 International Joint Conference on \\nNeural Networks (IJCNN), 1–8. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bIJCNN\\u200b54540.\\u200b\\n2023.\\u200b10191347.\\nMagnossão de Paula, A. F., and I. B. Schlicht. 2021. “AI-\\xadUPV at \\nIberLEF-\\xad2021 DETOXIS Task: Toxicity Detection in Immigration-\\xad\\nRelated Web News Comments Using Transformers and Statistical \\nModels.” In Proceedings of the Iberian Languages Evaluation Forum \\n(IberLEF 2021) Co-\\xadLocated With the XXXVII International Conference \\nof the Spanish Society for Natural Language Processing (SEPLN 2021), \\n547–566. CEUR. https://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b2943/\\u200bdetox\\u200bis_\\u200bpaper2.\\u200bpdf.\\nMahdaouy, A. E., A. E. Mekki, A. Oumar, H. Mousannif, and I. \\nBerrada. 2021. “Deep Multi-\\xadTask Models for Misogyny Identification \\nand Categorization on Arabic Social Media.” In Working Notes of FIRE \\n2021 -\\xad Forum for Information Retrieval Evaluation, Gandhinagar, India, \\n852–860. CEUR-\\xadWS.org Volume 3159 of CEUR Workshop Proceedings. \\nhttps://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b3159/\\u200bT5-\\xad\\u200b5.\\u200bpdf.\\nMikolov, T., K. Chen, G. Corrado, and J. Dean. 2013. “Efficient \\nEstimation of Word Representations in Vector Space.” In 1st \\nInternational \\nConference \\non \\nLearning \\nRepresentations, \\nICLR \\n2013, Scottsdale, Arizona, USA, May 2–4, 2013, Workshop Track \\nProceedings. \\nhttps://\\u200bpeople.\\u200bfjfi.\\u200bcvut.\\u200bcz/\\u200bvybir\\u200bja2/\\u200bSemin\\u200bar/\\u200bword2\\u200b\\nvec_\\u200b1301.\\u200b3781.\\u200bpdf.\\nMiljković, D. 2017. “Brief Review of Self-\\xadOrganizing Maps.” In 2017 \\n40th International Convention on Information and Communication \\nTechnology, Electronics and Microelectronics (MIPRO), 1061–1066. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b23919/\\u200bMIPRO.\\u200b2017.\\u200b7973581.\\nMostafa, A., O. Mohamed, and A. Ashraf. 2022. “GOF at Arabic \\nHate Speech 2022: Breaking the Loss Function Convention for Data-\\xad\\nImbalanced Arabic Offensive Text Detection.” In Proceedings of the 5th \\nWorkshop on Open-\\xadSource Arabic Corpora and Processing Tools With \\nShared Tasks on Qur'an QA and Fine-\\xadGrained Hate Speech Detection, \\n167–175. European Language Resources Association. https://\\u200baclan\\u200btholo\\u200b\\ngy.\\u200borg/\\u200b2022.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b21.\\nMubarak, H., H. Al-\\xadKhalifa, and A. Al-\\xadThubaity. 2022. “Overview of \\nOSACT5 Shared Task on Arabic Offensive Language and Hate Speech \\nDetection.” In Proceedings of the 5th Workshop on Open-\\xadSource Arabic \\nCorpora and Processing Tools With Shared Tasks on Qur'an QA and \\nFine-\\xadGrained Hate Speech Detection (Pp. 162–166). European Language \\nResources Association. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b2022.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b20.\\nMubarak, H., K. Darwish, W. Magdy, T. Elsayed, and H. Al-\\xadKhalifa. \\n2020. “Overview of OSACT4 Arabic Offensive Language Detection \\nShared Task.” In Proceedings of the 4th Workshop on Open-\\xadSource \\nArabic Corpora and Processing Tools, With a Shared Task on Offensive \\nLanguage Detection, edited by H. Al-\\xadKhalifa, W. Magdy, K. Darwish, \\nT. Elsayed, and H. Mubarak, 48–52. European Language Resource \\nAssociation. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b2020.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b7.\\nMulki, H., and B. Ghanem. 2021. “ArMI at FIRE 2021: Overview of \\nthe First Shared Task on Arabic Misogyny Identification.” In Working \\nNotes of FIRE 2021 -\\xad Forum for Information Retrieval Evaluation, \\nGandhinagar, India, December 13–17, 2021, 820–830. CEUR-WS.org \\nvolume 3159 of CEUR Workshop Proceedings. https://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b\\n3159/\\u200bT5-\\xad\\u200b1.\\u200bpdf.\\nOtter, D. W., J. R. Medina, and J. K. Kalita. 2020. “A Survey of the Usages \\nof Deep Learning for Natural Language Processing.” IEEE Transactions \\non Neural Networks and Learning Systems 32: 604–624. https://\\u200bdoi.\\u200borg/\\u200b\\n10.\\u200b1109/\\u200bTNNLS.\\u200b2020.\\u200b2979670.\\nPachinger, P., A. Hanbury, J. Neidhardt, and A. Planitzer. 2023. \\n“Toward Disambiguating the Definitions of Abusive, Offensive, Toxic, \\nand Uncivil Comments.” In Proceedings of the First Workshop on Cross-\\xad\\nCultural Considerations in NLP (C3NLP), 107–113. Association for \\nComputational \\nLinguistics. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2023.\\u200bc3nlp\\u200b\\n-\\xad\\u200b1.\\u200b11.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content=\"20 of 21\\nExpert Systems, 2026\\nPan, S. J., and Q. Yang. 2009. “A Survey on Transfer Learning.” IEEE \\nTransactions on Knowledge and Data Engineering 22: 1345–1359. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bTKDE.\\u200b2009.\\u200b191.\\nPérez, J. M., and F. M. Luque. 2019. “Atalaya at SemEval 2019 Task \\n5: Robust Embeddings for Tweet Classification.” In In Proceedings \\nof the 13th International Workshop on Semantic Evaluation, 64–69. \\nAssociation for Computational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200b\\nv1/\\u200bS19-\\xad\\u200b2008.\\nPlaza, L., J. de Carrillo-\\xadAlbornoz, R. Morante, et\\xa0al. 2023. “Overview \\nof EXIST 2023—Learning With Disagreement for Sexism Identification \\nand Characterization.” In Experimental IR Meets Multilinguality, \\nMultimodality, and Interaction, edited by A. Arampatzis, E. Kanoulas, \\nT. Tsikrika, et\\xa0 al., 316–342. Springer Nature Switzerland. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b031-\\xad\\u200b42448\\u200b-\\xad\\u200b9_\\u200b23.\\nPlaza, L., J. de Carrillo-\\xadAlbornoz, R. Morante, et\\xa0al. 2023. “Overview of \\nEXIST 2023: sEXism Identification in Social neTworks.” In Proceedings \\nof ECIR'23, 593–599. Springer Nature Switzerland. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b031-\\xad\\u200b28241\\u200b-\\xad\\u200b6_\\u200b68.\\nPlaza, L., J. de Carrillo-\\xadAlbornoz, V. Ruiz, et\\xa0 al. 2024. “Overview of \\nEXIST 2024—Learning With Disagreement for Sexism Identification \\nand Characterization in Tweets and Memes.” In Experimental IR Meets \\nMultilinguality, Multimodality, and Interaction, 93–117. Springer Nature \\nSwitzerland. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b031-\\xad\\u200b71908\\u200b-\\xad\\u200b0_\\u200b5.\\nPlaza-\\xaddel-\\xadArco, F. M., M. D. Molina-\\xadGonzález, and L. Alfonso. 2021. \\n“SINAI at IberLEF-\\xad2021 DETOXIS Task: Exploring Features as Tasks \\nin a Multi-\\xadTask Learning Approach to Detecting Toxic Comments.” In \\nProceedings of the Iberian Languages Evaluation Forum (IberLEF 2021) \\nCo-\\xadLocated With the XXXVII International Conference of the Spanish \\nSociety for Natural Language Processing (SEPLN 2021), 580–590. \\nMálaga, Spain. https://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b2943/\\u200bdetox\\u200bis_\\u200bpaper5.\\u200bpdf.\\nPlaza-\\xaddel-\\xadArco, F. M., M. D. Molina-\\xadGonzález, L. A. Ureña-\\xadLópez, and \\nM. T. Martín-\\xadValdivia. 2021a. “A Multi-\\xadTask Learning Approach to \\nHate Speech Detection Leveraging Sentiment Analysis.” IEEE Access 9: \\n112478–112489. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bACCESS.\\u200b2021.\\u200b3103697.\\nPlaza-\\xaddel-\\xadArco, F. M., M. D. Molina-\\xadGonzález, L. A. Ureña-\\xadLópez, \\nand M. T. Martín-\\xadValdivia. 2021b. “Comparing Pre-\\xadTrained Language \\nModels for Spanish Hate Speech Detection.” Expert Systems with \\nApplications 166: 114120. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200beswa.\\u200b2020.\\u200b114120.\\nPoletto, F., V. Basile, M. Sanguinetti, C. Bosco, and V. Patti. 2021. \\n“Resources and Benchmark Corpora for Hate Speech Detection: A \\nSystematic Review.” Language Resources and Evaluation 55: 477–523. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200bs1057\\u200b9-\\xad\\u200b020-\\xad\\u200b09502\\u200b-\\xad\\u200b8.\\nPrettenhofer, P., and B. Stein. 2010. “Cross-\\xadLanguage Text Classification \\nUsing Structural Correspondence Learning.” In Proceedings of the 48th \\nAnnual Meeting of the Association for Computational Linguistics, 1118–\\n1127. Association for Computational Linguistics. https://\\u200baclan\\u200btholo\\u200bgy.\\u200b\\norg/\\u200bP10-\\xad\\u200b1114.\\nRodríguez-\\xadSánchez, F., J. de Carrillo-\\xadAlbornoz, L. Plaza, et\\xa0 al. 2021. \\n“Overview of EXIST 2021: sEXism Identification in Social neTworks.” \\nProcesamiento del Lenguaje Natural 67: 195–207. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n26342/\\u200b2021-\\xad\\u200b67-\\xad\\u200b17.\\nRuder, S. 2017. “An Overview of Multi-\\xadTask Learning in Deep Neural \\nNetworks.” CoRR, abs/1706.05098. http://\\u200barxiv.\\u200borg/\\u200babs/\\u200b1706.\\u200b05098\\u200b.\\nRuder, S., M. E. Peters, S. Swayamdipta, and T. Wolf. 2019. “Transfer \\nLearning in Natural Language Processing.” In Proceedings of the \\n2019 Conference of the North American Chapter of the Association \\nfor Computational Linguistics: Tutorials, 15–18. Association for \\nComputational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200bN19-\\xad\\u200b5004.\\nSener, O., and V. Koltun. 2018. “Multi-\\xadTask Learning as Multi-\\xadObjective \\nOptimization.” In Proceedings of the 32nd International Conference \\non Neural Information Processing Systems NIPS'18, 525–536. Curran \\nAssociates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b33269\\u200b43.\\u200b3326992.\\nSinha, A. T., A. Rabinovich, Z. Chen, and V. Badrinarayanan. 2021. \\n“Gradient Adversarial Training of Neural Networks.” US Patent App. \\n17/051, 982.\\nTaulé, M., A. Ariza, M. Nofre, E. Amigó, and P. Rosso. 2021. “Overview \\nof DETOXIS at IberLEF 2021: DEtection of TOxicity in Comments in \\nSpanish.” Procesamiento del Lenguaje Natural 67: 209–221. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b26342/\\u200b2021-\\xad\\u200b67-\\xad\\u200b18.\\nUma, A., T. Fornaciari, A. Dumitrache, et\\xa0 al. 2021. “SemEval-\\xad2021 \\nTask 12: Learning With Disagreements.” In Proceedings of the 15th \\nInternational Workshop on Semantic Evaluation (SemEval-\\xad2021), edited \\nby A. Palmer, N. Schneider, N. Schluter, G. Emerson, A. Herbelot, and \\nX. Zhu, 338–347. Association for Computational Linguistics. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2021.\\u200bsemev\\u200bal-\\xad\\u200b1.\\u200b41.\\nVandenhende, S., S. Georgoulis, L. V. Gool, and B. D. Brabandere. 2020. \\n“Branched Multi-\\xadTask Networks: Deciding What Layers to Share.” In \\nProceedings of the 31st British Machine Vision Conference BMVC '20. BMVA \\nPress. https://\\u200bwww.\\u200bbmvc2\\u200b020-\\xad\\u200bconfe\\u200brence.\\u200bcom/\\u200bassets/\\u200bpapers/\\u200b0213.\\u200bpdf.\\nVandenhende, S., S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. \\nDai, and L. Van Gool. 2022. “Multi-\\xadTask Learning for Dense Prediction \\nTasks: A Survey.” IEEE Transactions on Pattern Analysis and Machine \\nIntelligence \\n44: \\n3614–3633. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bTPAMI.\\u200b2021.\\u200b\\n3054719.\\nVaswani, A., N. Shazeer, N. Parmar, et\\xa0al. 2017. “Attention Is All You \\nNeed.” In Proceedings of the 31st International Conference on Neural \\nInformation Processing Systems NIPS'17, 6000–6010. Curran Associates \\nInc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b32952\\u200b22.\\u200b3295349.\\nVoulodimos, A., N. Doulamis, A. Doulamis, E. Protopapadakis, and D. \\nAndina. 2018. “Deep Learning for Computer Vision: A Brief Review.” \\nComputational Intelligence and Neuroscience 2018: 1–13. https://\\u200bdoi.\\u200borg/\\u200b\\n10.\\u200b1155/\\u200b2018/\\u200b7068349.\\nWang, C., and S. Mahadevan. 2011. “Heterogeneous Domain Adaptation \\nUsing Manifold Alignment.” In Proceedings of the Twenty-\\xadSecond \\nInternational Joint Conference on Artificial Intelligence, 1541–1546. \\nAAAI Press. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b22835\\u200b16.\\u200b2283652.\\nWang, Y., M. Xu, Y. Yan, T. Zhao, Y. Chen, and J. Yang. 2022. “Exploring \\nTopic Supervision With BERT for Text Matching.” In 2022 International \\nJoint Conference on Neural Networks (IJCNN), 1–7. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1109/\\u200bIJCNN\\u200b55064.\\u200b2022.\\u200b9892023.\\nWeiss, K., T. M. Khoshgoftaar, and D. Wang. 2016. “A Survey of Transfer \\nLearning.” Journal of Big Data 3: 1–40. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1186/\\u200bs4053\\u200b\\n7-\\xad\\u200b016-\\xad\\u200b0043-\\xad\\u200b6.\\nWorsham, J., and J. Kalita. 2020. “Multi-\\xadTask Learning for Natural Language \\nProcessing in the 2020s: Where Are We Going?” Pattern Recognition Letters \\n136: 120–126. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200bpatrec.\\u200b2020.\\u200b05.\\u200b031.\\nWu, S., H. Fei, and D. Ji. 2020. “Aggressive Language Detection With \\nJoint Text Normalization via Adversarial Multi-\\xadTask Learning.” \\nIn Natural Language Processing and Chinese Computing: 9th CCF \\nInternational Conference, NLPCC 2020, Zhengzhou, China, 683–696. \\nSpringer-\\xadVerlag. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b60450\\u200b-\\xad\\u200b9_\\u200b54.\\nWu, S., H. R. Zhang, and C. Ré. 2020. “Understanding and Improving \\nInformation Transfer in Multi-\\xadTask Learning.” In 8th International \\nConference on Learning Representations, ICLR 2020, Addis Ababa, \\nEthiopia. \\nOpenReview.net. \\nhttps://\\u200bopenr\\u200beview.\\u200bnet/\\u200bforum?\\u200bid=\\u200bSylzh\\u200b\\nkBtDB\\u200b.\\nXu, D., W. Ouyang, X. Wang, and N. Sebe. 2018. “Pad-\\xadnet: Multi-\\xadAsks \\nGuided Prediction and Distillation Network for Simultaneous Depth \\nEstimation and Scene Parsing.” In 2018 IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, 675–684. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1109/\\u200bCVPR.\\u200b2018.\\u200b00077\\u200b.\\nZhang, Y., and Q. Yang. 2022. “A Survey on Multi-\\xadTask Learning.” \\nIEEE Transactions on Knowledge and Data Engineering 34: 5586–5609. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bTKDE.\\u200b2021.\\u200b3070203.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 20}, page_content='21 of 21\\nExpert Systems, 2026\\nZhang, Z., Z. Cui, C. Xu, Z. Jie, X. Li, and J. Yang. 2018. “Joint Task-\\xad\\nRecursive Learning for Semantic Segmentation and Depth Estimation.” \\nIn Computer Vision—ECCV 2018: 15th European Conference, Munich, \\nGermany, September 8–14, 2018, Proceedings, Part X, 238–255. Springer-\\xad\\nVerlag. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01249\\u200b-\\xad\\u200b6_\\u200b15.\\nZhang, Z., Z. Cui, C. Xu, Y. Yan, N. Sebe, and J. Yang. 2019. “Pattern-\\xad\\nAffinitive Propagation Across Depth, Surface Normal and Semantic \\nSegmentation.” In 2019 IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, 4106–4115. IEEE. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b\\n2019.\\u200b00423\\u200b.\\nZhao, X., H. Li, X. Shen, X. Liang, and Y. Wu. 2018. “A Modulation \\nModule for Multi-\\xadTask Learning With Applications in Image Retrieval.” \\nIn Proceedings of the European Conference on Computer Vision (ECCV), \\n415–432. Springer International Publishing. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b\\n978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01246\\u200b-\\xad\\u200b5_\\u200b25.\\nZhu, Z., K. Lin, A. K. Jain, and J. Zhou. 2023. “Transfer Learning in \\nDeep Reinforcement Learning: A Survey.” IEEE Transactions on \\nPattern Analysis and Machine Intelligence 45: 13344–13362. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b1109/\\u200bTPAMI.\\u200b2023.\\u200b3292075.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents=dir_loader.load()\n",
    "all_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8966741c",
   "metadata": {},
   "source": [
    "Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12f15bfc",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\"\",\" \"]\n",
    "    )\n",
    "    split_docs=text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    return split_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e912809",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 82 documents into 492 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 0}, page_content='International Journal of Digital Earth\\nISSN: 1753-8947 (Print) 1753-8955 (Online) Journal homepage: www.tandfonline.com/journals/tjde20\\nA systematic review and comparative analysis of\\ndeep learning models for Twitter/X-based traﬃc\\nevent detection\\nDanya Qutaishat & Songnian Li\\nTo cite this article: Danya Qutaishat & Songnian Li (2026) A systematic review and comparative\\nanalysis of deep learning models for Twitter/X-based traﬃc event detection, International\\nJournal of Digital Earth, 19:1, 2604977, DOI: 10.1080/17538947.2025.2604977\\nTo link to this article:  https://doi.org/10.1080/17538947.2025.2604977\\n© 2025 The Author(s). Published by Informa\\nUK Limited, trading as Taylor & Francis\\nGroup.\\nView supplementary material \\nPublished online: 29 Dec 2025.\\nSubmit your article to this journal \\nArticle views: 145\\nView related articles \\nView Crossmark data\\nFull Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=tjde20'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 1}, page_content='INTERNATIONAL JOURNAL OF DIGITAL EARTH \\n2026, VOL. 19, NO. 1, 2604977 \\nhttps://doi.org/10.1080/17538947.2025.2604977\\nREVIEW ARTICLE                                                                          \\nA systematic review and comparative analysis of deep learning models for \\nTwitter/X-based traffic event detection\\nDanya Qutaishat and Songnian Li\\nDepartment of Civil Engineering, Toronto Metropolitan University, Toronto, Canada\\nABSTRACT\\nTraffic anomalies caused by accidents, sports events, and lane closures are spatiotemporal \\nevents that reduce free-flow speed, increase vehicular queues, and impair human mobility. \\nEarly detection may provide better route planning before traffic gets worse. Recent and \\nongoing research, as well as a review of transportation literature, have revealed three \\nessential topics: big data, data mining and representation, and Deep Learning (DL). \\nFurthermore, traffic studies have adopted DL to extract hidden features that efficiently'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 1}, page_content='essential topics: big data, data mining and representation, and Deep Learning (DL). \\nFurthermore, traffic studies have adopted DL to extract hidden features that efficiently \\ninfer human activities and interactions and detect the underlying relationships to generate \\nuseful fine-grained information. This paper reviews current research that adopts state-of-the- \\nart DL in detecting traffic events from big data, specifically Twitter/X data. In addition, it \\ninvestigates the detailed pipeline for developing a DL-based model using data from Twitter/X \\nfor traffic event detection (TED). The review is a timely addition that clarifies the roadmap of \\ndetecting traffic events from big social media data, which benefits transportation and DL \\ncommunity researchers.\\nARTICLE HISTORY \\nReceived 24 February 2025 \\nAccepted 12 December 2025 \\nKEYWORDS\\nDeep learning; feature \\nlearning; word embedding; \\nmodel selection; Twitter/X\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 1}, page_content='community researchers.\\nARTICLE HISTORY \\nReceived 24 February 2025 \\nAccepted 12 December 2025 \\nKEYWORDS\\nDeep learning; feature \\nlearning; word embedding; \\nmodel selection; Twitter/X\\n1 Introduction\\nTraffic Event Detection (TED) holds significant promise in assisting road users. It helps choose optimal paths, \\nreduce travel time, mitigate traffic congestion, minimise fuel consumption, and reduce environmental pollution \\n(Nejjari, Benhlima, and Bah 2016; Kim et al. 2023; Lee et al. 2023; Gannina Kumar et al. 2024; Qutaishat and Li \\n2025a). Moreover, it help reduce traffic accidents that lead to injuries, fatalities, and property damage, which \\nresult in substantial social and economic costs (Es Swidi et al. 2023; Gannina Kumar et al. 2024). Additionally, \\ngenerating multiple scenarios equips traffic management departments with a tool to suggest timely and practical \\nplans to improve traffic conditions. This enables prompt responses by emergency services and supports traffic'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 1}, page_content=\"plans to improve traffic conditions. This enables prompt responses by emergency services and supports traffic \\nrerouting to enhance traffic management (Nejjari, Benhlima, and Bah 2016).\\nTransportation agencies' most significant challenge is acquiring real-time, large-scale, and up-to-date \\nobservational data (Anda, Erath, and Fourie 2017; Liu et al. 2020; Saeedi et al. 2020). Traditional data sources \\nconsist of structured data collected using physical condition monitoring devices deployed in the field or \\nsensing devices installed in moving vehicles (Hall, Shi, and Atala 1993; Sethi et al. 1995; Samant and Adeli \\n2000). Although incorporating conventional data for detecting traffic events provides accurate information \\nregarding their location and time, several challenges have been identified. First, studies are built on the \\nassumption of data reliability; however, incident detection has proven to be difficult due to detector failure,\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 1}, page_content='assumption of data reliability; however, incident detection has proven to be difficult due to detector failure, \\nsensor malfunctions, and communication errors in large-scale areas. Second, external factors impact traffic \\noperations, reducing the effectiveness of traffic metrics in detecting traffic incidents. Third, physical sensors \\nor detectors require regular maintenance and cover small-scale areas such as intersections or short road \\nsegments. This limitation restricts the capture of traffic patterns and non-recurring events across entire urban \\nareas (Münz, Sa, and Georg 2007; Zhang et al. 2018).\\nSupplemental data for this article can be accessed online at https://doi.org/10.1080/17538947.2025.2604977. \\nThis article has been corrected with minor changes. These changes do not impact the academic content of the article.\\n© 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 1}, page_content='© 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. \\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (http://creativecommons.org/licenses/by-nc/4 \\n.0/), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited. The terms on which \\nthis article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) or with their consent.\\nCONTACT Songnian Li \\nsnli@torontomu.ca\\nDepartment of Civil Engineering, Toronto Metropolitan University, Toronto, Canada'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='Nowadays, social media data makes a significant contribution to traffic studies, including human \\nactivity patterns or travel behaviours (Hasan and Ukkusuri 2014; Hasnat and Hasan 2018), traffic flow \\nforecasting (Lin et al. 2015; Ni, He, and Gao 2016; Cottrill et al. 2017), transportation management and \\nplanning (Cottrill et al. 2017), travel mode extraction (Maghrebi, Abbasi, and Waller 2016), and destina\\xad\\ntion choice modelling (Huang, Gallegos, and Lerman 2017; Molloy and Moeckel 2017; Hasnat and Hasan \\n2018; Hasnat et al. 2019). However, concerns are rising due to the lack of tools and techniques to unlock \\nthe power of data and extract valuable knowledge from this kind of massive, complex, and diverse big data.\\nTraditional machine learning (ML) models have been recognised as cornerstones of Twitter/X-based TED. \\nThey laid the foundation for utilising structured textual data and feature engineering to classify and predict traffic-'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='They laid the foundation for utilising structured textual data and feature engineering to classify and predict traffic- \\nrelated events. The most commonly investigated models are Support Vector Machine (SVM) (Noori and Mehra \\n2020; Afyouni, Aghbari, and Razack 2022; Dinesh, Kuhaneswaran, and Ravikumar 2023), Random Forest \\n(Alomari, Mehmood, and Katib 2019; Jiang and Deng 2020; ElSahly and Abdelfatah 2023), and Naive Bayes \\n(Alomari, Mehmood, and Katib 2019; Nirbhaya and Suadaa 2023). However, several challenges arise from issues \\nrelated to data quality, model performance, and computational efficiency. Selection bias arises from non- \\nrepresentative Twitter/X data, leading to skewed outcomes and reduced generalisability (Liu et al. 2024). Class \\nimbalance negatively impacts detection accuracy due to the higher number of non-traffic tweets compared to \\nrelevant ones (Liu et al. 2024). Linguistic challenges, including informal language, expressions, and negative'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='relevant ones (Liu et al. 2024). Linguistic challenges, including informal language, expressions, and negative \\nphrases, contribute to frequent detection errors (Dhiman and Toshniwal 2020; Liu et al. 2024). Additionally, short \\ntweet lengths limit contextual understanding, reducing model effectiveness (Dhiman and Toshniwal 2020). High \\ncomputational costs are another issue, as processing large-scale Twitter/X data with complex algorithms demands \\nsignificant resources (Dhiman and Toshniwal 2020). Lastly, feature extraction issues arise from automated social \\naccounts mimicking human behaviour, making traffic event detection more difficult (Sethurajan and K 2023).\\nIn the context of TED using Tweets, Deep Learning (DL) models have overcome ML in multiple aspects. The \\nfirst aspect is the ability of DL models to excel in automatically extracting relevant features from unstructured text'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='first aspect is the ability of DL models to excel in automatically extracting relevant features from unstructured text \\ndata, eliminating the need for manual feature engineering that is labour-intensive and prone to human error \\n(Hussain 2024; Qutaishat and Li 2025a). Second, techniques like transformers enable these models to understand \\nthe context of informal language, expressions, and abbreviations commonly found in tweets (Neruda and \\nWinarko 2021). They also process sequential and multimodal data, such as text, time, and location, using \\narchitectures like Recurrent Neural Networks (RNN) and multi-input neural networks, which provide a more \\ncomprehensive analysis (Alifi and Supangkat 2018; Zhang et al. 2018). Third, DL scales efficiently and can handle \\nlarge, real-time datasets while adapting real-time patterns and language changes on social media. Finally, its ability'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='large, real-time datasets while adapting real-time patterns and language changes on social media. Finally, its ability \\nto detect complex, non-linear patterns leads to significantly higher accuracy in identifying traffic events, making it \\na superior choice for real-time, dynamic environments (Kisters and Bauer 2023 ; Li, Dou, and Zhou 2023; \\nNirbhaya and Suadaa 2023; Qutaishat and Li 2025a, Suat-Rojas, Gutierrez-Osorio, and Pedraza 2022; Yang 2022; \\nQutaishat and Li 2025b).\\nThis paper reviews the literature on DL-based TED using social media data, focusing on Twitter/X. Twitter/X \\nhas emerged as a valuable platform for supporting the detection and modelling of traffic events and deserves \\nclose attention in this context. Several reviews have explored the topic of event detection using social media data, \\nwith some studies focusing on traditional machine learning techniques and statistical methods for analysing'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='with some studies focusing on traditional machine learning techniques and statistical methods for analysing \\nTwitter/X data to identify traffic-related events (Garg and Kumar 2016; Xu, Li, and Wen 2018; Liu et al. 2024). \\nThese studies lack emphasis on DL approaches, which have proven to be more effective in handling \\nunstructured text, capturing complex relationships, and integrating multimodal data sources. Other studies \\nhave focused on investigating event detection on Twitter/X, without specifically addressing traffic events or \\ndetailing the techniques used, such as (Saeed et al. 2019; Atefeh and Khreich 2013). There has been a lack of \\nsystematic reviews that provide comprehensive insights into the research framework for traffic event detection \\nusing Twitter/X data, specifically in the context of DL techniques.\\nThe main contributions of this paper are as follows:\\n1. A comprehensive systematic review that consolidates and critically analyses published studies related to'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 2}, page_content='The main contributions of this paper are as follows:\\n1. A comprehensive systematic review that consolidates and critically analyses published studies related to \\nTwitter/X-based TED using DL techniques.  \\n2. A detailed description of the general Twitter/X-based TED workflow, including preprocessing steps and \\nmodel development. \\n2\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 3}, page_content='3. An analysis of feature learning and models, evaluating their strengths and weaknesses, and the influence \\nof parameters on DL performance. \\n4. Identification of the challenges and future research directions, offering essential guidance in transpor\\xad\\ntation engineering and related domains.\\n2 Review methodology\\nThis research followed the PRISMA 2020 guidelines for systematic reviews (Page et al. 2021; Page and \\nMcKenzie, 2020 ). Figure 1 presents the PRISMA flow diagram, which shows the number of studies \\nidentified, screened, excluded, and ultimately included in the review.\\n2.1 Eligibility criteria\\nStudies were assessed based on the following inclusion and exclusion criteria:\\n2.1.1 Inclusion criteria\\nStudies were included if they:\\n• Applied DL models such as CNNs, RNNs, or LSTMs for TED.  \\n• Used Twitter/X as a standalone source or integrated it with other data.  \\n• Were published in English, from 2010 onward, reflecting DL advancements from that time.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 3}, page_content='• Used Twitter/X as a standalone source or integrated it with other data.  \\n• Were published in English, from 2010 onward, reflecting DL advancements from that time.  \\n• Provided detailed methodologies, including DL architecture, preprocessing, and evaluation metrics (e.g. \\naccuracy, F1-score). \\n• Were published in peer-reviewed journals or reputable conferences in relevant fields, such as transpor\\xad\\ntation engineering or computer science.                           \\nFigure 1. The PRISMA flow diagram of the article selection process. Note: Although the main flow shows 30 studies from \\ndatabase searches, additional studies were identified via citation/web methods, bringing the total to 44.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n3'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 4}, page_content='• Cover both recurring (e.g. rush hour) and non-recurring (e.g. accidents) traffic events.\\n2.1.2 Exclusion criteria\\nStudies were excluded if they:\\n• Used observational or non-social media data for TED.  \\n• Depended on traditional ML instead of DL.  \\n• Focused on events unrelated to traffic (e.g. natural disasters, crime, health)  \\n• Lacked sufficient methodological detail or empirical evaluation.  \\n• Were theoretical, non-English, or published before 2010.\\n2.2 Information sources and search strategy\\nFor the scope of this review, the search was narrowed down to journal articles and conference proceedings. \\nFive scholarly databases were searched: Engineering Village, Scopus, Web of Science, IEEE, and Summon \\n2.0. Citation searches help trace prior, derivative, or related works. ResearchRabbit, a citation-based \\nliterature mapping tool, facilitated this process in conjunction with Google Scholar. The timeframe, set'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 4}, page_content='literature mapping tool, facilitated this process in conjunction with Google Scholar. The timeframe, set \\nfrom 2010 onward, reflects the rise of DL, driven by the revolution in computational power and the \\navailability of large datasets. Earlier studies predominantly used ML or statistical techniques.\\nBoolean Keyword combinations used included:\\n(traffic event detection AND deep learning) AND (social media OR Twitter/X OR crowdsourcing).\\nInitial database search yielded:\\n• Engineering Village: 57  \\n• IEEE Xplore: 19  \\n• Scopus: 16  \\n• Web of Science: 13  \\n• Summon 2.0: 20\\nAn additional 1,123 studies were identified via citation and web-based searching.\\n2.3 Screening and selection process\\nA double-screening strategy was applied (Nama et al. 2019) in which two reviewers independently \\nscreened records. First, EndNote X9 was used to import references, remove duplicates, and sort studies \\nby publication year, title, and author to structure the screening sequence. Each reviewer then indepen\\xad'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 4}, page_content='by publication year, title, and author to structure the screening sequence. Each reviewer then indepen\\xad\\ndently assessed titles and abstracts; a random subset was cross-checked to ensure consistency. Rayyan \\nfacilitated collaborative full‐text review and inclusion/exclusion decisions.\\nTo ensure inter-rater reliability, a random subset of studies was jointly reviewed, and reviewer \\nagreement was monitored. Although Cohen’s kappa coefficient was not formally calculated, consistency \\nwas verified through regular calibration and consensus discussions. Any discrepancies between reviewers \\nwere resolved through consensus discussion, and a third reviewer was not needed.\\n2.4 Final inclusion and data extraction\\nAfter removing duplicates and applying inclusion/exclusion criteria, a total of 44 studies were included, \\nwith 30 from database searches and 14 from other methods, as shown in Figure 1.\\nWe extracted the following data from each included study:'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 4}, page_content='with 30 from database searches and 14 from other methods, as shown in Figure 1.\\nWe extracted the following data from each included study:\\n• Title, year, publication type, and author list  \\n• Country or region of focus \\n4\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 5}, page_content='• DL models used  \\n• Data representation methods  \\n• Performance metrics (e.g. accuracy, precision, F1-score)\\n3 Results and discussion\\n3.1 Characteristics of reviewed studies\\nFigure 2 presents a word cloud of key terms. ‘Deep Learning’, ‘Social Networking’, and ‘Twitter/X’, which \\nwere clearly stated in the article search, appear with high prominence in the word cloud. The CNN and \\nLSTM models are the most frequently used, alongside the terms related to semantic feature representation, \\nsuch as word embeddings and information management, which are essential for Twitter/X data mining.\\nStudies focusing on keyword generation emphasise the term ‘Traffic’ in various contexts, such as ‘Traffic \\nCongestion’, ‘Traffic Incident’,  and ‘Traffic information’,  indicating a primary focus on unexpected or \\nnon-recurring traffic events. For model evaluation, terms like ‘accuracy’  and ‘Accuracy assessment’  were'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 5}, page_content='non-recurring traffic events. For model evaluation, terms like ‘accuracy’  and ‘Accuracy assessment’  were \\ncommonly used, suggesting that the studies emphasise textual feature representation, model selection, and \\nframework construction over evaluation measures. Given the novelty of social media data analysis in traffic \\nresearch, diverse keywords like ‘Detection’,   ‘Event Detection’,  and ‘TED’  were employed.\\nFigure 3 is a world map showing the geographic distribution of study sites. India leads in publications, \\nfollowed by the UK and the US, which together account for approximately 45% of the reviewed studies. \\nIndonesia, China, and Pakistan contributed 25%. While India-centric data (8/44 studies) demonstrates feasibility \\nin local contexts, reliance on region-specific corpora may restrict generalisability. Future research should expand \\nvalidation efforts in underrepresented regions such as Africa and South America to ensure global applicability.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 5}, page_content='validation efforts in underrepresented regions such as Africa and South America to ensure global applicability. \\nFigure 2. Word cloud of the prominent keywords in 44 studies.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n5'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 6}, page_content=\"Based on Figure 4, 60.98% of studies were published between 2021 and 2023, with publications doubling in 2023, \\nreflecting a surge in research interest during that period.\\nThe dominant subject areas were computer science (40.4%), engineering (25.8%), and mathematics \\n(12.4%). India's dominance is attributed to its large social media user base, extensive volume of Twitter/X \\ndata, and robust AI/ML capabilities, which are supported by collaborations among academia, industry, and \\ngovernment. The UK and US benefit from top-tier research institutions, advanced infrastructure, and \\nrobust funding for AI, ML, and DL studies in transportation research (Sahni and Raja 2018).\\n3.2 Deep-learning Twitter/X Data analysis for traffic events detection preprocessing workflow\\nA detailed workflow for deep-learning-based Twitter/X data analysis in TED, as shown in Figure 5, is \\nsummarised from the reviewed studies. However, variations exist in the techniques and algorithms as\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 6}, page_content='summarised from the reviewed studies. However, variations exist in the techniques and algorithms as \\nresearchers explore different models, methods, and architectures. At its core is the data crawling stage, \\nwhere tweets are collected via REST or Streaming APIs using filters like keywords and geolocation. Data \\nFigure 3. Map of the world based on No. of publications related to Twitter/X TED using DL.\\nFigure 4. The trend of published studies on Twitter/X TED using DL (2017–2024) peaked in 2022–2023.\\n6\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 7}, page_content='quality is enhanced during preprocessing, while feature representation prepares the data for the DL model. \\nA suitable model is selected, followed by training and evaluation to improve and assess performance. This \\nframework is adaptable to various social media platforms, including Facebook, Instagram, Twitter/X, and \\nSina Weibo.\\n3.2.1 Traffic tweets crawling on twitter/X\\nBased on the included studies, traffic-related data from Twitter/X was historically collected using two \\nprimary methods: REST APIs (programmatic Representational State Transfer) and Streaming APIs. These \\nAPIs allowed researchers to define a centroid (latitude, longitude), a radius, and a set of keywords using \\noperators, including AND, OR, and EXCLUDE, in their format (Ali et al. 2017; Ali et al. 2019).\\nEach API serves distinct purposes and offers unique advantages for researchers and developers. The \\nREST API was typically used for historical data collection, allowing users to query based on specific'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 7}, page_content='REST API was typically used for historical data collection, allowing users to query based on specific \\nkeywords, locations (centroid + radius), and timeframes. It supported up to 3,200 tweets per request, with \\na 15-minute rate limit of 350 requests and was suited for historical analysis or offline model training (Gu, \\nQian, and Chen 2016; Xu, Li, and Wen 2018; Ali et al. 2019). In contrast, the Streaming API offers real- \\ntime tweet collection as they are posted, making it ideal for live TED and traffic monitoring (Doguc and \\nAhmet 2023). However, it only returned a limited sample of tweets and lacked the flexibility of advanced \\nquery customisation.\\nAs of 2024, access to Twitter/X’s APIs has undergone a significant transformation. Following corporate \\nownership changes and a strategic shift toward monetisation, Twitter/X now severely restricts free-tier \\naccess. Real-time streaming and full historical search functionalities are available only through premium or'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 7}, page_content='access. Real-time streaming and full historical search functionalities are available only through premium or \\nenterprise-level subscriptions. The cost of full access, which may reach up to $42,000/month, has rendered \\nlarge-scale data collection impractical for many academic researchers (Murtfeldt et al. 2024).\\nThis change has introduced a new challenge to reproducibility, limiting the ability to replicate earlier \\nstudies. Many previously effective crawling methods (e.g. Tweepy, REST API scripts, TWINT) have \\nbecome unreliable or non-functional under the updated API policies. In response, some researchers \\nhave shifted to alternative methods, such as web scraping; however, these approaches raise both ethical and \\ntechnical concerns (Poudel and Weninger 2024).\\nIn light of restricted Twitter/X API access as of 2024, researchers are encouraged to utilise existing \\npublicly available Twitter/X datasets to support reproducible research. For instance, the CrisisLexT6'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 7}, page_content='publicly available Twitter/X datasets to support reproducible research. For instance, the CrisisLexT6 \\ndataset includes annotated tweets from natural disasters, some of which are transportation-related \\n(Imran et al. 2015).\\nThe T4SA dataset contains over 4.5 million sentiment-labelled tweets and is accessible via GitHub \\n[https://github.com/codiceSpaghetti/T4SA-2.0]. Table 1 summarises widely available datasets, their \\ndomains, access types, and representative studies in which they have been used.\\nTable 2 provides a comparative overview of past TED studies, highlighting the data crawling methods \\nused and their viability under present-day Twitter/X API policies. It is essential to note that while some \\nresearchers have collected upwards of 1 million tweets (e.g. Alomari, Mehmood, and Katib 2019), such \\nvolumes are no longer realistically attainable without institutional resources or paid access. Jonnalagadda'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 7}, page_content='volumes are no longer realistically attainable without institutional resources or paid access. Jonnalagadda \\nand Hashemi (2021) collected approximately 10,000 tweets, of which 5,000 were used for developing the \\ndeep learning model. Dabiri and Heaslip (2019) retrieved around 50,000 tweets, with 17,437 manually \\nFigure 5. Generalised workflow for DL-based social media TED. It includes data crawling from social media platforms \\n(e.g. Twitter/X, Facebook), preprocessing, feature representation, DL model selection, training and validation, and final \\nmodel evaluation for accurate event detection.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n7'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 8}, page_content='identified as traffic-related tweets. Alomari, Mehmood, and Katib (2019) gathered over 1 million tweets, \\nbut only an estimated 5,000 were categorised as relevant to traffic events; the remainder reflected broader \\nsocial or environmental contexts. Neruda and Winarko (2021) attempted to supplement data using \\nTWINT, which is now non-operational due to changes in X’s frontend structure and API restrictions.\\nThe variability in Twitter/X data affects model development in multiple ways. While large datasets \\nenhance statistical strength, they often come with noisy labels and higher preprocessing demands (Tsou, \\nZhang, and Jung 2017; Effrosynidis, Sylaios, and Arampatzis 2024). In contrast, smaller datasets are cleaner \\nbut less generalisable and more prone to overfitting, especially when class imbalance exists, as seen in cases \\nwhere there are fewer tweets related to traffic incidents (Liu et al. 2024). Additionally, geotagged Tweets are'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 8}, page_content='where there are fewer tweets related to traffic incidents (Liu et al. 2024). Additionally, geotagged Tweets are \\nrare (found in only about 1–2% of tweets), limiting spatial analysis unless complemented by inferred or \\nexternal location data (Tsou, Zhang, and Jung 2017).\\nTable 1. Sample dataset comparison table.       \\nDataset name\\nDomain\\nLanguage\\nAccess type\\nUsed by\\nCrisisLexT6\\nCrisis/Traffic\\nEnglish\\nPublic via website\\n(Olteanu, Vieweg, and Castillo  2015; Alam, Ofli, and Imran  2018)\\nTwevent\\nEvent detection\\nEnglish\\nGitHub\\n(Li et al.  2012)\\nQCRI datasets\\nCrisis/Traffic\\nMultilingual\\nGitHub\\n(Alam, Ofli, and Imran  2018)\\nGeoCOV19\\nMobility\\nEnglish\\nPublic via portal\\n(Qazi, Imran, and Ofli  2020)\\nCustom (Dabiri)\\nTraffic-specific\\nEnglish\\nGitHub\\n(Dabiri and Heaslip  2019)\\nTable 2. Summary of studies highlighting social media platforms, crawling techniques, tweet collection volume, and data \\nrepresentation methods.         \\nStudy\\nPlatform\\nData crawling  \\nmethod\\nNo. of Tweets\\nFeature learning'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 8}, page_content='representation methods.         \\nStudy\\nPlatform\\nData crawling  \\nmethod\\nNo. of Tweets\\nFeature learning  \\nmethod\\nNotes on current feasibility\\n(Ali et al.  2021)\\nTwitter/X \\nFacebook\\nstandard APIs\\n60000 tweets \\n5000 Facebook \\nposts\\nWord2vec \\nFastText\\nStandard API access is \\ndeprecated or highly \\nlimited on Twitter/X as \\nof 2024.\\n(Dabiri and Heaslip  2019)\\nTwitter/X\\nn/a\\n50,000 tweets\\nWord2vec \\nFastText\\nMethod unspecified. \\nAssumed pre-2022 \\nfeasibility\\n(Alomari, Mehmood, and \\nKatib  2019)\\nREST API\\n1 million\\nTF-IDF\\nLarge-scale tweet \\ncollection using REST API \\nno longer viable \\n(requires enterprise \\naccess).\\n(Ali et al.  2019)\\nTwitter/X\\nREST and \\nStreaming \\nAPIs\\n30,000 tweets\\nString2word \\nGlove2vec \\nLexicon Features \\nDoc2vec\\nStreaming API deprecated \\nunder v2. Results not \\nreproducible with free- \\ntier access.\\n(Chen et al.  2018)\\nSina Weibo\\nSina Weibo \\ncrawler\\n11,000\\nContinuous Bag of \\nWords (CBOW)\\nNon-Twitter/X platform. \\nMethod not affected.\\n(Lu et al.  2018)\\nNews articles \\nand Weibo'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 8}, page_content='tier access.\\n(Chen et al.  2018)\\nSina Weibo\\nSina Weibo \\ncrawler\\n11,000\\nContinuous Bag of \\nWords (CBOW)\\nNon-Twitter/X platform. \\nMethod not affected.\\n(Lu et al.  2018)\\nNews articles \\nand Weibo \\nposts,\\nNetwork of \\nsocial sensors\\n1.15 million texts\\nWord2Vec \\nCBOW\\nNon-Twitter/X platform.\\n(Zhang et al.  2018)\\nTwitter/X\\nStreaming API\\n3 million tweets\\nA systematic feature \\nselection process\\nHigh-volume collection no \\nlonger feasible under \\ncurrent Twitter/X API \\nlimits.\\n(Fatichah et al.  2020)\\nTwitter/X and \\nImages\\nTwitter/X API\\n10000 tweets \\n1000 images\\nWord Embedding\\nAPI usage requires \\nelevated or paid access \\ntiers.\\n(Jonnalagadda and \\nHashemi  2021)\\nTwitter/X\\nn/a\\n10,000 tweets\\nWord2vec\\nMethod unspecified. \\nPresumed historical \\nfeasibility.\\n(Ambastha and \\nDesarkar  2020)\\nTwitter/X\\nStreaming API\\n1887 tweets\\nTF-IDF \\nWord2Vec\\nStreaming API deprecated.\\n(Puangnak and \\nRachsiriwatcharabul  2022)\\nTwitter/X\\nn/a.\\n3363 tweets\\nWord Embedding Word \\nIndexing\\nNo crawling method \\nprovided.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 8}, page_content=\"Streaming API\\n1887 tweets\\nTF-IDF \\nWord2Vec\\nStreaming API deprecated.\\n(Puangnak and \\nRachsiriwatcharabul  2022)\\nTwitter/X\\nn/a.\\n3363 tweets\\nWord Embedding Word \\nIndexing\\nNo crawling method \\nprovided.\\n(Neruda and Winarko  2021)\\nTwitter/X\\nTweepy \\nTWINT\\n6319 tweets\\nBidirectional Encoder \\nRepresentations from \\nTransformers (BERT)\\nTWINT is no longer \\nfunctional due to X's API \\nand frontend \\nobfuscation changes.\\n(Almassar and Girsang  2022)\\nTwitter/X\\nTwitter/X API\\n4,087 tweets\\nWord2vec \\nFastText\\nTweet API access is limited \\nunder new pricing tiers.\\n8\\nD. QUTAISHAT AND S. LI\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 9}, page_content='In summary, while Twitter/X data remains valuable for TED, practical access is now significantly \\nconstrained. Researchers must adapt by utilising limited-access APIs, employing ethical web scraping, or \\nleveraging multimodal datasets to compensate for the limitations imposed by the post-API era (Blakey \\n2024; Poudel and Weninger 2024).\\n3.2.2 Twitter/X data preprocessing\\nPreprocessing of raw Twitter/X data is a critical step in TED, as it addresses the platform’s non-standard \\nlanguage and reduces ambiguity, abbreviations, and uncertainty (Ramadhani and Goo 2017; Ali et al. 2019; \\nKarthik et al. 2023; Rezaeinia, Ghodsi, and Rouhollah 2017; Safitri et al. 2024). A key challenge is the \\nscarcity of traffic-related tweets relative to general posts, which further necessitates efficient preprocessing \\nto mitigate linguistic ambiguity and noise (Fatichah et al. 2020; Afyouni, Aghbari, and Razack 2022; Li, \\nDou, and Zhou 2023).'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 9}, page_content='to mitigate linguistic ambiguity and noise (Fatichah et al. 2020; Afyouni, Aghbari, and Razack 2022; Li, \\nDou, and Zhou 2023).\\nThe research addresses three primary goals: noise reduction, improved traffic event extraction, and enhanced \\ndataset generalisability (Garg and Kumar 2016; Ramadhani and Goo 2017; Xu, Li, and Wen 2018; Zhang et al. \\n2018; Ali et al. 2019; Azhar et al. 2022). However, challenges persist in handling the evolving nature of informal \\nlanguage, slang, emojis, sarcasm, and ambiguity in tweets (Fatichah et al. 2020). Additionally, privacy concerns \\nnecessitate adherence to regulations, including data anonymization and obtaining user consent (Ramadhani and \\nGoo 2017; Karthik et al. 2023). Integrating sentiment analysis and contextual information may help address \\nthese linguistic challenges, while privacy concerns require appropriate anonymization and consent procedures. \\nFigure 6 illustrates the preprocessing workflow for Twitter/X data for TED.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 9}, page_content='these linguistic challenges, while privacy concerns require appropriate anonymization and consent procedures. \\nFigure 6 illustrates the preprocessing workflow for Twitter/X data for TED.\\n3.2.3 Semantic feature learning: from traditional vectors to semantic embeddings\\nA critical challenge in applying DL to traffic event detection TED lies in converting raw text into \\nmeaningful, vectorized representations suitable for algorithmic processing. Text representation plays a \\nfoundational role, influencing model accuracy and robustness, particularly when dealing with noisy and \\ninformal Twitter/X data (Zhang et al. 2018; Dabiri and Heaslip 2019; Neruda and Winarko 2021).\\nText representation in TED has evolved across three major phases:\\na. frequency-based models including Bag-of-Words (BoW) and Term Frequency-Inverse Document \\nFrequency (TF-IDF).  \\nb. Word embeddings such as Word2Vec and GloVe.  \\nc. contextual embeddings using transformer-based models such as BERT (discussed further in'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 9}, page_content='Frequency (TF-IDF).  \\nb. Word embeddings such as Word2Vec and GloVe.  \\nc. contextual embeddings using transformer-based models such as BERT (discussed further in \\nSection 3.2.4).\\na. Traditional Frequency-Based Models: BoW and TF-IDF\\nEarly approaches to representing textual data in TED systems relied on BoW and TF-IDF. The BoW \\napproach converts unstructured tweets into fixed-size numerical vectors based on word occurrence, \\nignoring context and word order (Dabiri and Heaslip 2019). Each word is assigned a unique index, and \\ntweets are represented as N-dimensional vectors, where N is the vocabulary size. TF-IDF refines this by \\nweighting terms based on their frequency across multiple documents, which emphasises more informative \\nwords (Rajaraman and Ullman 2011).\\nSeveral studies have employed these methods in analysing traffic-related tweets. D’Andrea et al. (2015) \\nused IDF-based feature selection to classify Italian tweets, while Alomari et al. (2021) developed a Spark-'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 9}, page_content='used IDF-based feature selection to classify Italian tweets, while Alomari et al. (2021) developed a Spark- \\nbased feature extraction pipeline for Arabic tweets. However, both BoW and TF-IDF suffer from key \\nlimitations: they ignore word order, semantic similarity, and contextual meaning. This reduces their \\neffectiveness in handling the informal, slang-heavy, and abbreviation-rich nature of social media data \\n(Deho et al. 2018; Rudkowsky et al. 2018; Dabiri and Heaslip 2019).\\nAs deep learning models require dense, semantically rich input vectors, the limitations of BoW and TF- \\nIDF have led to a shift toward word embedding techniques that can capture syntactic and semantic \\nrelationships more effectively. This evolution reflects the broader transition from classical vectorisation to \\nneural embedding in modern TED pipelines.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n9'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 10}, page_content='Figure 6. Twitter/X data preprocessing steps applied to a traffic-related tweet, including text cleaning, normalisation, \\ntokenization, and lemmatization for model-ready input.\\n10\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 11}, page_content='b. Transition to Word Embeddings\\nWord embeddings extend beyond traditional vectorisation by capturing contextual relationships \\nbetween terms (Ali et al. 2019; Dabiri and Heaslip 2019). Widely applied across natural language \\nprocessing, computer science, artificial intelligence, machine learning, and computational linguistics, \\nthese methods map words into low-dimensional vector spaces that encode syntactic, semantic, and \\ndistributional meanings (Noori and Mehra 2020; Sampath and Supriya 2023). Common word embedding \\ntechniques include String2Vec, Word2Vec, Doc2Vec, GloVe, and FastText, with Word2Vec and FastText \\nbeing the most frequently used in Twitter/X-based TED. In summary, while Twitter/X data remains \\nvaluable for TED, practical access is now significantly constrained. Researchers must adapt by utilising \\nlimited-access APIs, employing ethical web scraping, or leveraging multimodal datasets to compensate for'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 11}, page_content=\"limited-access APIs, employing ethical web scraping, or leveraging multimodal datasets to compensate for \\nthe limitations imposed in the post-API era (Blakey 2024; Poudel and Weninger 2024). Table 2 sum\\xad\\nmarises the feature learning methods utilised in this domain.\\nWord2Vec, introduced by Google in 2013, is an unsupervised DL technique that learns word represen\\xad\\ntation by capturing semantic relations, synonyms, and analogies through analysis of large text corpora like \\nTwitter/X (Bilgin and Şentürk 2017; Mikolov, Quoc V, and Ilya 2013; Mikolov and Sutskever, 2013). \\nAlthough it generalises well, it struggles with Out-of-Vocabulary words (Bilgin and Şentürk 2017).\\nFastText, developed by Facebook's AI Research lab, represents words as sub-word n-grams. It is \\neffective in handling typos, abbreviations, and informal language commonly found in Twitter/X traffic \\nreports. FastText enhances NER and DL models by capturing morphological similarities (Mannes 2017).\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 11}, page_content='reports. FastText enhances NER and DL models by capturing morphological similarities (Mannes 2017).\\nGloVe (Global Vectors for Word Representation) differs from Word2Vec in that it leverages global co- \\noccurrence statistics. While it performs well on similar tasks, it struggles with words that have multiple \\nmeanings (Pennington, Socher, and Manning 2014; Abad et al. 2016).\\nDoc2Vec, an extension of Word2Vec, generates embeddings for full documents, enabling phrase and \\nparagraph-level analysis (Bilgin and Şentürk 2017; Kamkarhaghighi and Makrehchi 2017). Figure 7\\npresents the efficiency and comparative performance of word embedding methods in TED.\\nc. Performance and Trade-Offs in TED Applications\\nSeveral studies have demonstrated the effectiveness of word embeddings in Twitter/X-based TED, often \\noutperforming traditional BoW and TF-IDF models. Dabiri and Heaslip (2019) found that integrating'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 11}, page_content='outperforming traditional BoW and TF-IDF models. Dabiri and Heaslip (2019) found that integrating \\nCNN with Word2Vec achieved better accuracy and F-score. Similarly, Lu et al. (2018) developed a \\nWord2Vec-based approach that outperformed CBOW in terms of traffic incident detection accuracy.\\nAli et al. (2019) evaluated String2Word, Word2Vec, Doc2Vec, and GloVe for transportation sentiment \\nanalysis, identifying key terms such as ‘Crash,’ ‘Accident,’ ‘Traffic,’ ‘Speed,’ and ‘Event.’ While \\nFigure 7. Comparison of word embedding models for traffic text, showing their strengths in context capture, noise \\nhandling, and classification. The bottom panel compares typo tolerance, context understanding, data efficiency, speed, \\nand classification performance.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n11'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 12}, page_content='String2Word had good accuracy, its high dimensionality increased computational time and reduced \\nprecision. Doc2Vec achieved 74% accuracy using logistic regression and 73% with deep learning, out\\xad\\nperforming both Word2Vec and GloVe. Finally, GloVe, trained on a transportation corpus, reached 66% \\naccuracy in deep learning tasks, excelling in feature recognition but underperforming in accuracy \\ncompared to Word2Vec and Doc2Vec. Almassar and Girsang (2022) tested FastText, achieving 86.33% \\naccuracy and a 96.61 F-score in congestion detection. Fatichah et al. (2020) combined FastText with CNN, \\nLSTM, and C-LSTM for handling out-of-vocabulary terms for incident detection. Ambastha and Desarkar \\n(2020) compared TF-IDF and Word2Vec, using SVM, Naïve Bayes, CNN, and LSTM models. Azhar et al. \\n(2022) proposed an integrated model combining word embeddings with numeric traffic and sentiment \\nlayers to enhance TED performance.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 12}, page_content='(2022) proposed an integrated model combining word embeddings with numeric traffic and sentiment \\nlayers to enhance TED performance.\\nDespite their advantages, advanced embeddings can pose computational challenges in real-time \\nsystems. Lightweight models, such as Word2Vec and FastText, are widely preferred due to their fast \\ninference and low memory usage, making them ideal for large-scale tweet streams. In contrast, Doc2Vec \\nprovides richer semantic representations but requires greater computational resources, including GPU \\nacceleration and longer training times. Embeddings exceeding 300 dimensions can slow processing \\nconsiderably in high-throughput environments. Thus, choosing an embedding requires balancing seman\\xad\\ntic accuracy against processing efficiency, particularly in latency-sensitive deployments (Rudkowsky et al. \\n2018; Ali et al. 2019; Gu et al. 2021).\\nTo improve conceptual clarity while considering practical limitations, we compare embeddings based'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 12}, page_content='2018; Ali et al. 2019; Gu et al. 2021).\\nTo improve conceptual clarity while considering practical limitations, we compare embeddings based \\non both performance and suitability for different use cases. FastText, due to sub-word modelling, is ideal \\nfor short and informal text (Bonandrini and Gatti 2024). Conversely, semantically rich models like \\nDoc2Vec or GloVe are better suited to batch processing and archival analysis, where computational \\nlatency is less critical (Pita and Pappa 2018).\\nWord embeddings offer clear benefits but also introduce challenges. One major issue is optimising the \\nembedding dimension, typically set between 100 and 300, which affects both accuracy and computational \\ncost (Asudani et al. 2023). Embeddings also struggle with informal and evolving social media language, \\nsuch as slang and abbreviations. Additionally, relying on pre-trained embeddings may limit adaptability,'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 12}, page_content='such as slang and abbreviations. Additionally, relying on pre-trained embeddings may limit adaptability, \\nthus requiring domain-specific fine-tuning (Raunak 2017; Wilson et al. 2020; Torregrossa et al. 2021; \\nAsudani et al. 2023). Table 3 summarises the trade-offs among various embedding methods in terms of \\nsemantic richness and computational demands.\\nFurthermore, high-dimensional embeddings increase computational complexity, which affects scalabil\\xad\\nity in real-time TED applications (Deho et al. 2018; Rudkowsky et al. 2018; Gu et al. 2021). In summary, \\nthe selection of an appropriate embedding technique depends on dataset characteristics, computational \\nconstraints, and task-specific requirements (Gu et al. 2021). These considerations are especially important \\nwhen deploying models in real-time systems, where the trade-off between embedding accuracy and \\ncomputational efficiency becomes critical.\\n3.2.4 DL models for TED'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 12}, page_content='when deploying models in real-time systems, where the trade-off between embedding accuracy and \\ncomputational efficiency becomes critical.\\n3.2.4 DL models for TED\\nTo build a solid foundation, it is essential to contextualise the development of TED models from ML to DL \\nand, more recently, to transformer-based architectures. This shift reflects advances in data availability, \\nalgorithmic design, and computing power.\\nTable 4 presents the evolution of TED methodologies from traditional ML approaches to transformer- \\nbased models. During the ML era, algorithms such as SVM, Decision Trees, and Random Forests formed \\nthe foundation of early TED systems. These models were typically combined with TF-IDF or Bag-of- \\nTable 3. The trade-offs between embedding methods in terms of semantic richness and computational efficiency.         \\nEmbedding \\nmethod\\nSematic richness\\nTraining time\\nInference \\nspeed\\nComputational cost\\nDimensionality\\nBest use case\\nWord2Vec\\nModerate\\nFast\\nFast\\nLow\\n100-300'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 12}, page_content='Embedding \\nmethod\\nSematic richness\\nTraining time\\nInference \\nspeed\\nComputational cost\\nDimensionality\\nBest use case\\nWord2Vec\\nModerate\\nFast\\nFast\\nLow\\n100-300\\nReal-time systems, short texts\\nFastText\\nModerate–High\\nVery Fast\\nVery Fast\\nVery Low\\n100-300\\nReal-time, Out-of-Vocabulary \\nhandling\\nDoc2Vec\\nHigh\\nSlow\\nModerate\\nMedium–High\\n+ 300\\nDocument-level classification\\nGloVe\\nModerate\\nModerate\\nFast\\nMedium\\n100-300\\nSentiment analysis, pre- \\ntrained use\\n12\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 13}, page_content='Words (BoW) for feature representation. Prior studies have shown that such approaches lack context \\nsensitivity and require extensive manual feature engineering (Atefeh and Khreich 2013; D’Andrea \\net al. 2015).\\nA major shift occurred in the deep learning era, with models such as CNNs, LSTMs, and GRUs \\nemerging as more effective alternatives. These architectures learn hierarchical and temporal patterns \\ndirectly from raw text, enhancing performance on unstructured tweet data (Dabiri and Heaslip 2019; \\nAlmassar and Girsang 2022).\\nFinally, the transformer era introduced models like BERT, which offer bidirectional contextual under\\xad\\nstanding, effectively handle multilingual and noisy data, and support multimodal data integration. These \\nmodels, however, are computationally intensive (Neruda and Winarko 2021; Nirbhaya and Suadaa 2023).\\nThe effectiveness of DL models in TED depends on the quantity and quality of training data, as well as'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 13}, page_content='The effectiveness of DL models in TED depends on the quantity and quality of training data, as well as \\nthe complexity of the tasks. DL Models like CNN, LSTM, and BERT require large, labelled datasets, \\nwhereas simpler ML models can perform reasonably well on smaller datasets. Model selection is also \\ninfluenced by the application. Complex models tend to improve accuracy, while simpler models offer faster \\ninference. Integrating features such as emotion, weather, and geo-location data can further enhance TED \\nperformance. However, no single DL model guarantees optimal results. The choice depends on dataset \\ncharacteristics, feature engineering, and computational constraints (Azhar et al. 2022; Yang 2022). This \\nsection reviews the DL models adopted for TED in the selected studies. Figure 8 illustrates their capabilities \\nfor Twitter/X-based TED.\\n3.2.4.1 Multi-Layer Perceptron (MLP)\\nMLP is a feed-forward neural network comprising an input layer,'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 13}, page_content='for Twitter/X-based TED.\\n3.2.4.1 Multi-Layer Perceptron (MLP)\\nMLP is a feed-forward neural network comprising an input layer, \\na hidden layer, and an output layer. Jonnalagadda and Hashemi (2021) compared MLP with SVM, Bi- \\nLSTM, and CNN, finding that MLP underperformed due to its inability to capture sequential \\ndependencies. Similarly, Puangnak and Rachsiriwatcharabul (2022) showed that CNN and \\nCNN + LSTM outperformed MLP, underscoring the importance of sequence modelling. MLP remains \\nuseful but is highly dependent on data quality and feature integration.\\n3.2.4.2 Convolutional Neural Networks (CNNs)\\nOriginally developed for image recognition, CNNs \\nhave been effectively adapted for natural language processing by applying one-dimensional \\nconvolutions to text embeddings. They excel at identifying local patterns, such as n-grams, making \\nthem suitable for keyword-based incident detection. Integrated with pretrained embeddings like'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 13}, page_content='convolutions to text embeddings. They excel at identifying local patterns, such as n-grams, making \\nthem suitable for keyword-based incident detection. Integrated with pretrained embeddings like \\nFastText, CNNs perform well even on noisy text. Hybrid models, such as CNN + LSTM, combine \\nspatial pattern recognition with temporal learning, thereby improving overall accuracy (Alifi and \\nSupangkat 2018; Dabiri and Heaslip 2019; Liu et al. 2020; Neruda and Winarko 2021; Jain et al. 2023; \\nQutaishat and Li 2025b). Almassar and Girsang (2022) found that CNN + FastText outperformed \\nCNN + Word2Vec and SVM. Dabiri and Heaslip (2019) found that CNN outperformed both LSTM \\nand CNN + LSTM. CNN + LSTM hybrids further enhance performance by combining spatial and \\ntemporal features. Chen et al. (2018) confirmed this in Sina Weibo traffic analysis, where LSTM–CNN \\nachieved the top F1 score.\\n3.2.4.3 Recurrent neural networks (RNNs) and long short-term memory (LSTM)\\nRNNs process'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 13}, page_content='achieved the top F1 score.\\n3.2.4.3 Recurrent neural networks (RNNs) and long short-term memory (LSTM)\\nRNNs process \\nsequential data but struggle with long-term dependencies due to the vanishing gradient problem. LSTM \\naddresses this issue. Azhar et al. (2022) reported that LSTM achieved 94.2% accuracy in road accident \\ndetection, followed by GRU of 91.6% and RNN of 39.7%. Yang (2022) combined stacked autoencoders \\nwith LSTM, achieving 98.25% accuracy, which outperforms CNN + LSTM at 96.36%.\\nTable 4. Evolution of TED methodologies from ML to transformer-based models, highlighting models, data \\nrepresentation, and modelling capabilities.       \\nMethodology era\\nModels\\nData representation\\nStrengths\\nWeaknesses\\nML\\nSVM, Decision Trees, Random Forests\\nTF-IDF, BoW\\nSimple, interpretable\\nNo semantic/contextual \\nunderstanding\\nDL\\nCNN, LSTM, GRU\\nWord Embedding\\nLearns hierarchical \\nfeatures\\nRequires more data & \\ncomputing\\nTransformers\\nBERT, Multimodal Bi-transformers (MMBT),'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 13}, page_content='No semantic/contextual \\nunderstanding\\nDL\\nCNN, LSTM, GRU\\nWord Embedding\\nLearns hierarchical \\nfeatures\\nRequires more data & \\ncomputing\\nTransformers\\nBERT, Multimodal Bi-transformers (MMBT), \\nVision-and-Language Transformer (ViLT)\\nMultimodal inputs\\nContextual & cross-modal \\nreasoning\\nResource intensive\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n13'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 14}, page_content='3.2.4.4 Bidirectional LSTM (Bi-LSTM)\\nBi-LSTM improves upon LSTM by capturing bidirectional \\ndependencies. Puangnak and Rachsiriwatcharabul (2022) reported an accuracy of 93.53% in incident \\ndetection using Bi-LSTM, although the accuracy for severity classification was lower, reaching 77.92%. Alifi \\nand Supangkat (2018) developed a hybrid model combining Bi-LSTM and CNN, achieving an F-score of 78.9%.\\n3.2.4.5 CNN-LSTM(C-LSTM)\\nC-LSTM integrates CNNs for spatial feature extraction with LSTMs for \\ntemporal learning. Fatichah et al. (2020) adapt C-LSTM for incident detection, achieving 99.09% accuracy. \\nCombined with VGG16, it yielded the highest multimodal prediction confidence. Zeng et al. (2019) \\nreported C-LSTM outperformed both CNN at 80.27% and LSTM at 80.96% in traffic classification.\\n3.2.4.6 Generative Adversarial Networks (GANs)\\nGANs consist of a generator and a discriminator \\nworking together to improve data quality. They are used for detecting traffic anomalies and generating'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 14}, page_content='3.2.4.6 Generative Adversarial Networks (GANs)\\nGANs consist of a generator and a discriminator \\nworking together to improve data quality. They are used for detecting traffic anomalies and generating \\nsynthetic, yet realistic, data. Liu et al. (2024) proposed a GAN-Transformer for TED, using GANs to \\nbalance datasets while Transformers extracted complex relations. Lin et al. (2020) utilised GANs to \\nmitigate data imbalance, resulting in a 3.2% improvement in accuracy and a 5.65% reduction in false \\nalarms.\\n3.2.4.7 Gated Recurrent Unit (GRU)\\nGRU, developed by Kyunghyun Cho in 2014, is an RNN variant \\nwith update and reset gates to manage information flow. Azhar et al. (2022) demonstrated that GRU \\nachieved an accuracy of 93.7% in detecting accident-related tweets. Suat-Rojas, Gutierrez-Osorio, and \\nPedraza (2022) proposed a GRU-CNN hybrid that outperforms baseline models in accident prediction.\\n3.2.4.8 Bidirectional Encoder Representations from Transformers (BERT)'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 14}, page_content='Pedraza (2022) proposed a GRU-CNN hybrid that outperforms baseline models in accident prediction.\\n3.2.4.8 Bidirectional Encoder Representations from Transformers (BERT)\\nBERT was developed in two \\nmodel sizes: BERTBASE (110 M parameters) and BERTLARGE (340 M parameters). Although studies on \\nBERT for TED are limited, it has proven effective for detecting and analysing traffic incidents in noisy, \\nunstructured social media data (Qutaishat and Li 2025a). Nirbhaya and Suadaa (2023) reported 99.26% \\naccuracy using IndoBERT in Indonesian-language TED, outperforming traditional models. Neruda and \\nWinarko (2021) demonstrated that BERT-CNN outperformed ELMo-CNN and Word2Vec-CNN in terms \\nof performance.\\nFigure 8. DL models for Twitter/X traffic event detection, highlighting their roles in feature extraction, sequential \\nprocessing, and noise robustness.\\n14\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 15}, page_content=\"3.2.5 DL model training and validation\\nTo achieve effective training and model validation, Twitter/X-based TED studies commonly divide \\ndatasets into three subsets: training, validation, and testing (Raschka 2018). The training set is repeatedly \\nused for model learning, allowing it to discover the underlying patterns, while the validation and test sets \\nassess the model's performance and generalisation.\\nThe proportions of these subsets often vary based on dataset size and characteristics. In social media-based \\nTED, where data sparsity and informal language are common, selecting appropriate splits becomes crucial (Jain \\net al. 2022; Savvides and Mäkelä 2023; Qiu 2024). As illustrated in Figure 9 although a general protocol for \\ndataset partitioning exists in DL, many studies fail to adhere to it.\\nSeveral studies report their data split strategies. Alifi and Supangkat (2018) used a 70:15:15 split for training,\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 15}, page_content='dataset partitioning exists in DL, many studies fail to adhere to it.\\nSeveral studies report their data split strategies. Alifi and Supangkat (2018) used a 70:15:15 split for training, \\ntesting, and validation. Nirbhaya and Suadaa (2023) employed k-fold cross-validation, dividing the data into 80% \\ntraining, 10% validation, and 10% testing sets. Almassar and Girsang (2022) used 60% for training, 20% for \\nvalidation, and 20% for testing. Puangnak and Rachsiriwatcharabul (2022) explored multiple ratios, including \\n50:50, 60:40, 70:30, 80:20, 90:10, and 95:5, and identified 90:10 and 95:5 as optimal; however, they ultimately \\nadopted 70:30. Nevertheless, their study lacked detail on the validation procedure. Neruda and Winarko (2021) \\nemployed a 64:16:20 split and explicitly noted measures to prevent data leakage through preprocessing and \\nfeature extraction. Table 5 presents the dataset partitioning strategies across reviewed TED studies.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 15}, page_content='feature extraction. Table 5 presents the dataset partitioning strategies across reviewed TED studies.\\nWhile many studies report their dataset split ratios, few evaluate how these choices impact model \\nperformance. High training proportions, such as 90:10 or 95:5, may boost training accuracy but risk \\noverfitting due to limited evaluation data. In contrast, balanced splits, such as 70:30 or k-fold cross- \\nvalidation, generally provide more robust estimates of generalisability. For instance, although Puangnak \\nand Rachsiriwatcharabul (2022) reported strong accuracy with 90:10 and 95:5 splits, the absence of a \\ndetailed validation procedure limits interpretability. Figure 10 illustrates how reported performance \\nmeasures vary by split strategy, reinforcing the need for standardised, transparent validation procedures \\nto support fair benchmarking across TED studies.\\n3.2.6 Model performance measures and evaluations'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 15}, page_content='to support fair benchmarking across TED studies.\\n3.2.6 Model performance measures and evaluations\\nBased on Table 5, most studies included in this review used accuracy and F-score to evaluate model \\nperformance. Accuracy is a common starting point due to its simplicity and clear indication of overall correct                       \\nFigure 9. The DL dataset split for model selection and evaluation. Though widely recommended, this protocol is often \\ninconsistently applied in practice.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n15'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 16}, page_content='Table 5. DL-based twitter/X TED studies.           \\nStudy author\\nStudy objective\\nModel\\nDat split (train/ \\nvalidation/test) & \\nvalidation \\nmethod\\nPerformance measures\\nFuture work\\nAccuracy \\n(%)\\nF1-score (%)\\nPrecision (%)\\nRecall \\n(%)\\nAlmassar and \\nGirsang ( 2022)\\nTraffic \\ncongestion \\ndetection\\nCNN + Word2Vec, \\nCNN + FastText \\nSVM\\n60/20/20—Grid \\nSearch Cross- \\nValidation\\n85.79% \\n86.33% \\n67.62%\\n86.11% \\n96.61% \\n68.53%\\n80.59% \\n81.18% \\n63.84%\\n92.45% \\n92.83% \\n73.96%\\n• Improve model accuracy.  \\n• \\nIntegrate data sources.  \\n• \\nEnhance predictive capabilities.  \\n• \\nScale to other regions/languages.  \\n• \\nExplore real-time implementation and \\nperformance.\\nZhang \\net al. ( 2018)\\nTraffic accident \\ndetection\\nDeep Belief Network \\nANN \\nLSTM \\nSVM\\nNot reported—5- \\nFold Cross- \\nValidation\\n85% \\n82% \\n81% \\n79%\\nNA \\nNA \\nNA \\nNA\\n92% \\n81% \\n87% \\n84%\\nNA \\nNA \\nNA \\nNA\\n• \\nInvestigate underreported accidents on \\nTwitter/X.  \\n• \\nIntegrate social media data for better traffic \\njam detection.  \\n•'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 16}, page_content='85% \\n82% \\n81% \\n79%\\nNA \\nNA \\nNA \\nNA\\n92% \\n81% \\n87% \\n84%\\nNA \\nNA \\nNA \\nNA\\n• \\nInvestigate underreported accidents on \\nTwitter/X.  \\n• \\nIntegrate social media data for better traffic \\njam detection.  \\n• \\nDevelop a common dataset via community \\ncollaboration.\\nPuangnak and \\nRachsiriwatchara\\xad\\nbul ( 2022)\\nRoad traffic \\nincident reports \\nclassification\\nMLP \\nCNN \\nBi-LSTM \\nLSTM + CNN\\n70/30—10-Fold \\nCross-Validation\\n44.4% \\n93.24% \\n77.92% \\n93.44%\\nNA \\nNA \\nNA \\nNA\\nNA \\nNA \\nNA \\nNA\\nNA \\nNA \\nNA \\nNA\\n• \\nAutomate incident detection.  \\n• \\nReduce reliance on manual surveillance.  \\n• \\nBuild accurate DL models for traffic \\nclassification.\\nAzhar et al. ( 2022)\\nRoad accident \\ndetection and \\nprediction\\nGRU \\nRNN \\nLSTM\\n80/20—k-Fold \\nCross-Validation \\n(k not specified)\\n93.7% \\n91.6% \\n94.2%\\n93% \\n90% \\n95%\\n91% \\n90% \\n94%\\nNA \\nNA \\nNA\\n• \\nManage large Twitter/X datasets for \\naccident detection.  \\n• \\nImprove geo-location accuracy.  \\n• \\nManage incomplete datasets, such as \\nweather info.\\n(Neruda and \\nWinarko ( 2021)\\nTED\\nBERT + CNN'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 16}, page_content='NA\\n• \\nManage large Twitter/X datasets for \\naccident detection.  \\n• \\nImprove geo-location accuracy.  \\n• \\nManage incomplete datasets, such as \\nweather info.\\n(Neruda and \\nWinarko ( 2021)\\nTED\\nBERT + CNN \\nELMo + CNN \\nWord2Vec + CNN\\n64/16/20—5- \\nFold Cross- \\nValidation\\nNA \\nNA \\nNA\\n90% \\n88% \\n87%\\n90% \\n90% \\n88%\\n92% \\n87% \\n87%\\n• \\nEnhance data labelling for better TED.  \\n• \\nAssess fine-tuning of pre-trained word \\nembeddings on task performance.  \\n• \\nExplore advanced oversampling techniques \\nlike MLSMOTE.  \\n• \\nTest various neural network architectures for \\ndetection improvements.\\nJonnalagadda and \\nHashemi ( 2021)\\nTED\\nRF \\nSVM \\nMLP \\nBLSTM \\nCNN\\n20% \\nValidation—5- \\nFold Cross- \\nValidation\\n67% \\n73.1% \\n78.7% \\n88.1% \\n93.3%\\n67% \\n73.4% \\n78% \\n88% \\n93%\\nNA \\nNA \\nNA \\nNA\\nNA \\nNA \\nNA \\nNA\\n• \\nEnhance TED by integrating additional data \\nsources and refining algorithms.  \\n• \\nImprove accurate identification of traffic \\nevents from social media data.  \\n• \\nRefine algorithms for better accuracy and \\nreliability.  \\n•'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 16}, page_content='sources and refining algorithms.  \\n• \\nImprove accurate identification of traffic \\nevents from social media data.  \\n• \\nRefine algorithms for better accuracy and \\nreliability.  \\n• \\nAddress the vanishing gradient problem \\nRNN to improve long-term dependencies.  \\n• \\nExpand TED beyond binary classification to \\ninclude fine-grained categorisation.\\nFatichah \\net al. ( 2020)\\nIncident type \\nprediction\\nCNN \\nCNN + LSTM\\nNot \\nreported—Hold\\xad\\nout Validation\\n98.95% \\n99.09%\\nNA \\nNA\\nNA \\nNA\\nNA \\nNA\\n• \\nEnhance real-time processing for live \\nmonitoring.  \\n• \\nAssess model robustness in varied \\nenvironments. \\n16\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 17}, page_content='Table 5. (Continued) \\nStudy author\\nStudy objective\\nModel\\nDat split (train/ \\nvalidation/test) & \\nvalidation \\nmethod\\nPerformance measures\\nFuture work\\nAccuracy \\n(%)\\nF1-score (%)\\nPrecision (%)\\nRecall \\n(%)\\n• \\nAnalyse data augmentation effects on CNN \\narchitectures.  \\n• \\nIncorporate audio and video for incident \\ndetection.  \\n• \\nExplore advanced DL models like \\ntransformers for classification.\\nDabiri and \\nHeaslip ( 2019)\\nTraffic incident \\ndetection\\nCNN + Word2vec \\nLSTM + Word2vec \\nClSTM + Word2vec \\nCNN + FastText \\nLSTM + FastText \\nClSTM + FastText \\nCNN + Random Word \\nvector \\nLSTM + Random Word \\nvector \\nCLSTM + Random word \\nvector\\nNot reported—5- \\nFold Cross- \\nValidation\\n98.6% \\n98.4% \\n98.5% \\n98.6% \\n98.5% \\n98.6% \\n50.1% \\n50.3% \\n49.9%\\n98.6% \\n98.4% \\n98.5% \\n98.6% \\n98.5% \\n98.6%3939.8% \\n50.2% \\n49.8% \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA\\n• \\nDevelop an efficient geocoder to extract \\ntraffic event locations from tweets.  \\n• \\nImplement a unified Twitter/X-based traffic'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 17}, page_content=\"NA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA\\n• \\nDevelop an efficient geocoder to extract \\ntraffic event locations from tweets.  \\n• \\nImplement a unified Twitter/X-based traffic \\ninformation system across traffic networks.  \\n• \\nEnhance real-time traffic information \\ndissemination to drivers and traffic \\nmanagers.  \\n• \\nExplore integration with transportation \\nagencies and state police for improved \\ntraffic flow management\\nAmbastha and \\nDesarkar ( 2020)\\nTraffic \\ncongestion \\ndetection\\nCNN \\nLSTM \\nULMFiT\\nL-TWITS split (not \\nspecified)—Hol\\xad\\ndout Validation\\nNA \\nNA \\nNA\\n78% \\n82% \\n90%\\n70% \\n76% \\n84%\\n64% \\n74% \\n89%\\n• \\nExplore traffic location detection from tweet \\ncontent.  \\n• \\nExpand the L-TWITS dataset by collecting \\nmore tweets on diverse traffic incidents.  \\n• \\nAddress ULMFiT's prediction failures for \\nsarcastic and irrelevant traffic tweets.\\nAli et al. ( 2021)\\nTraffic Incident \\nDetection and \\nCondition \\nAnalysis\\nOLDA + word2Vec + RNN \\nOLDA + word2Vec + LST\\xad\\nM\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 17}, page_content='sarcastic and irrelevant traffic tweets.\\nAli et al. ( 2021)\\nTraffic Incident \\nDetection and \\nCondition \\nAnalysis\\nOLDA + word2Vec + RNN \\nOLDA + word2Vec + LST\\xad\\nM \\nOLDA + word2Vec + Bi- \\nLSTM \\nOLDA + FastText + RNN \\nOLDA + FastText + LSTM \\nOLDA + FastText + bi- \\nLSTM\\n70/30—Holdout \\nValidation\\n80% \\n85% \\n91% \\n85% \\n92% \\n97%\\n77% \\n84% \\n89% \\n85% \\n92% \\n97%\\n83% \\n83% \\n85% \\n82% \\n94% \\n97%\\n71% \\n86% \\n94% \\n88% \\n92% \\n97%\\n• \\nData Enhancement \\n• \\nIncorporating diverse event types \\n• \\nImproving model performance\\nAlifi and \\nSupangkat ( 2018)\\nTraffic \\ncondition \\nrecognition\\nBi-LSTM + CNN\\n70/15/ \\n15—Holdout \\nValidation\\n78.9%\\n82.1%\\n75%\\n• \\nCombine GANs with other DL models for \\nimproved accuracy. \\n• \\nAdapt the framework for real-time traffic \\ndata processing. \\n• \\nTest on diverse datasets to ensure \\nrobustness. \\n• \\nAddress imbalances in incident types and \\nconditions. \\n• \\nInvestigate CNNs or RNNs to boost GAN \\nperformance.\\n(Continued) \\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n17'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 18}, page_content='Table 5. (Continued) \\nStudy author\\nStudy objective\\nModel\\nDat split (train/ \\nvalidation/test) & \\nvalidation \\nmethod\\nPerformance measures\\nFuture work\\nAccuracy \\n(%)\\nF1-score (%)\\nPrecision (%)\\nRecall \\n(%)\\nLin et al. ( 2020)\\nTraffic incident \\ndetection\\nGAN \\nRF \\nSVM\\nNot \\nreported—10- \\nFold Cross- \\nValidation\\n91.87% \\nNA \\n91.53%\\nNA \\nNA \\nNa\\nNA \\nNA \\nNA\\nNA \\nNA \\nNA\\n• \\nCombine GANs with other DL models for \\nimproved accuracy. \\n• \\nAdapt the framework for real-time traffic \\ndata processing. \\n• \\nTest on diverse datasets to ensure \\nrobustness. \\n• \\nAddress imbalances in incident types and \\nconditions. \\n• \\nInvestigate CNNs or RNNs to boost GAN \\nperformance.\\nNirbhaya and \\nSuadaa ( 2023)\\nTraffic incident \\ndetection\\nSVM \\nNavey Bayes \\nLogistic Regression \\nLSTM \\nIndoBERT\\n80/10/10—5- \\nFold Cross- \\nValidation\\n98.81% \\n93.86% \\n98.68% \\n96.14% \\n99.26%\\n98.54% \\n92.49% \\n98.37% \\n95.16% \\n99.10%\\n98.78% \\n91.91% \\n98.77% \\n95.72% \\n99.20%\\n98.31% \\n93.16% \\n98% \\n94.72% \\n99.02%\\n• \\nEnhanced data collection \\n•'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 18}, page_content='Validation\\n98.81% \\n93.86% \\n98.68% \\n96.14% \\n99.26%\\n98.54% \\n92.49% \\n98.37% \\n95.16% \\n99.10%\\n98.78% \\n91.91% \\n98.77% \\n95.72% \\n99.20%\\n98.31% \\n93.16% \\n98% \\n94.72% \\n99.02%\\n• \\nEnhanced data collection \\n• \\nAdvanced pre-processing techniques \\n• \\nIncorporating multimodal data\\nNote: CNN = Convolutional Neural Network; LSTM = Long Short-Term Memory; Bi-LSTM = Bidirectional LSTM; SVM = Support Vector Machine; MLP = Multi-Layer Perceptron; GRU = Gated Recurrent Unit; \\nGAN = Generative Adversarial Network; ULMFiT = Universal Language Model Fine-tuning for Text Classification; RF = Random Forest; L-TWITS = Labelled-TWeets for Indian Traffic Scenario; MLSMOTE = Multi-Label \\nSynthetic Minority Over-sampling Technique. NA = Not Available. Some studies did not report full metric sets or specify validation splits. \\n18\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 19}, page_content='predictions. However, with imbalanced data, a frequent issue in TED where incident-related tweets are fewer \\nthan non-incident ones, accuracy can be misleading. It often prioritises the majority class while overlooking \\nperformance on minority classes (Johnson and Khoshgoftaar 2019). To mitigate this, the F-score is widely \\nadopted for its balance of precision, which refers to the proportion of correctly identified positives, and recall, \\nwhich refers to the proportion of actual positives correctly captured (Dinga et al. 2019).\\nNonetheless, comparisons across studies reveal inconsistencies. For example, Almassar and Girsang \\n(2022) reported an accuracy of 86.33% using CNN + FastText, but with a lower precision of 81.18%, \\nindicating a potential misclassification of minority-class events. Similarly, Ali et al. (2021) achieved \\naccuracy scores up to 97%, while some F1-scores ranged from 84% to 89%, reflecting the influence of \\nclass imbalance on model reliability.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 19}, page_content=\"accuracy scores up to 97%, while some F1-scores ranged from 84% to 89%, reflecting the influence of \\nclass imbalance on model reliability.\\nMost studies mention data augmentation or resampling to address this issue, but rarely specify the methods \\nused. For instance, Neruda and Winarko (2021) referred to synthetic oversampling, and Jonnalagadda and \\nHashemi (2021) adapted weighted loss functions to emphasise underrepresented labels. Other studies, such as \\nthose by Azhar et al. (2022), Puangnak and Rachsiriwatcharabul (2022), and Ambastha and Desarkar (2020), \\neither referred to augmentation broadly or did not report a specific strategy.\\nClass imbalance can significantly reduce recall and F1-scores for rare events (Johnson and Khoshgoftaar \\n2019; Walsh and Tardy 2022; Jiang et al. 2025). This lack of transparency in the study's methodology \\nmakes it challenging to assess the comparative effectiveness of imbalance-handling techniques. So future\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 19}, page_content='makes it challenging to assess the comparative effectiveness of imbalance-handling techniques. So future \\nwork should clearly report the techniques used and their impact on minority-class performance. Table 6\\npresents common techniques for handling class imbalance in deep learning models (Johnson and \\nKhoshgoftaar 2019; Walsh and Tardy 2022; Jiang et al. 2025).\\n4 Challenges, open issues, and future directions\\nChallenges and limitations arise at each stage of the DL-based Twitter/X TED workflow, culminating in \\nthe final detection stage. Key issues, open challenges, and future directions are discussed below and \\nsummarised in Table 5.\\nFigure 10. Performance metrics (accuracy, precision, recall, and F1-score) reported across studies using various dataset \\nsplit strategies for TED.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n19'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 20}, page_content='4.1 Data quality and labelling\\nTwitter/X’s dynamic nature produces noisy data with misspellings, abbreviations, and informal language that \\nhinder accurate TED. Ambiguity and brevity in tweets further complicate the extraction of meaningful \\ninformation. DL models require substantial labelled data, yet obtaining sufficient traffic-related tweets remains \\ndifficult. Manual labelling is time-consuming and resource-intensive. Two solutions have been suggested:\\na. Adopting semi-supervised learning to leverage a small volume of labelled data alongside a larger pool of \\nunlabelled data for DL model training.  \\nb. Incorporating external sources such as police reports or traffic authority data to automatically label \\nrelevant tweets.\\n4.2 Imbalanced data distribution\\nTraffic-related events that occur more frequently, such as ‘congestion,’ are given more priority than those \\nthat rarely occur, such as ‘accidents,’ which skew the training process. To mitigate this challenge,'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 20}, page_content='that rarely occur, such as ‘accidents,’ which skew the training process. To mitigate this challenge, \\nresearchers can use data augmentation, which can be used to generate new data from the existing dataset \\nand train DL models. Additionally, assigning higher weights to misclassify rare events during training can \\nbe incorporated to ensure attention is paid when modelling.\\n4.3 Ethics and data governance\\nThe application of social media data for identifying traffic events presents ethical and governance challenges. \\nPrivacy concerns arise when users are unaware that their posts are being analysed, necessitating informed \\nconsent and data anonymization (Mredula and Noyon, 2022). Additionally, algorithms trained on such data \\nmay reflect existing social biases, which can potentially lead to unfair outcomes. Addressing these issues requires \\nthe adoption of fairness-aware models (Chen and Wang 2019; Alomari and Mehmood 2023). Data governance'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 20}, page_content='the adoption of fairness-aware models (Chen and Wang 2019; Alomari and Mehmood 2023). Data governance \\nalso plays a key role, involving questions about data ownership and compliance with regulations such as the \\nGeneral Data Protection Regulation, which mandates user consent and control over personal data (Chen, Chen, \\nand Qian 2014; Melhem, Abdi, and Meziane 2024). While technical issues are being managed, the focus should \\nbe on ethical standards and regulatory frameworks. A responsible approach will require collaboration across \\ntechnology, ethics, and policy domains to ensure both effectiveness and respect for user rights.\\n4.4 Multi-modal data integration\\nOngoing research explores the integration of traffic sensor data, speed logs, and accident reports to \\nenhance traffic condition analysis. Multimodal fusion techniques combine inputs from multiple sources, \\nTable 6. Common techniques for handling class imbalance in deep learning models.      \\nTechnique\\nDescription\\nType'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 20}, page_content='Table 6. Common techniques for handling class imbalance in deep learning models.      \\nTechnique\\nDescription\\nType\\nReported effect\\nRandom oversampling/ \\nundersampling\\nDuplicate minority class samples or downsample \\nthe majority class to achieve a balanced \\ndistribution.\\nData-level\\nImproved recall, sometimes at the \\ncost of precision.\\nClass weighting/cost-sensitive \\nlearning\\nAssign higher loss weights to minority classes \\nduring model training.\\nAlgorithm-level\\nBalanced F1-score, reduced bias \\ntowards the majority class.\\nSMOTE (synthetic minority over- \\nsampling technique)\\nGenerate synthetic minority samples by \\ninterpolating the feature space.\\nData-level\\nIncreased sensitivity to rare classes; \\nreduced under-detection.\\nADASYN/MLSMOTE (advanced \\nSMOTE variants)\\nAdaptive generation of minority samples, \\nfocusing on harder-to-learn examples.\\nData-level\\nImproved minority-class recall, but \\nhigher training complexity.\\nGAN-based data augmentation\\nUse Generative Adversarial Networks to'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 20}, page_content='focusing on harder-to-learn examples.\\nData-level\\nImproved minority-class recall, but \\nhigher training complexity.\\nGAN-based data augmentation\\nUse Generative Adversarial Networks to \\nsynthesise realistic minority samples.\\nHybrid\\nBoosted accuracy and F1-scores, \\nespecially in rare-event detection.\\nEnsemble methods\\nCombine classifiers trained on different balanced \\nsubsets.\\nAlgorithm-level\\nImproved robustness, stable \\nperformance across imbalanced \\ndatasets.\\n20\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 21}, page_content='boosting detection accuracy. Recent work also emphasises spatial and temporal modelling to better capture \\nthe evolving nature of traffic events.\\nIn addition to tweets, recent research emphasises the integration of text, audio, video, and IoT sensor \\ndata to enhance TED. These modalities complement Twitter/X’s text-based signals by providing real- \\nworld, multimodal evidence of incidents or congestion. Each source contributes distinct strengths:\\n• Text (tweets) offers real-time, crowd-sourced alerts.  \\n• Audio (sirens, honking) reflects ambient traffic conditions.  \\n• Video (CCTV, dashcams) provides visual confirmation of events.  \\n• IoT sensors (GPS, accelerometers, traffic counters) deliver location-specific, quantitative indicators.\\nTo process these diverse inputs, deep learning models such as late fusion CNN-LSTM architectures and \\nattention-based transformers are employed. These models support cross-modal feature learning, enhancing'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 21}, page_content='attention-based transformers are employed. These models support cross-modal feature learning, enhancing \\ndetection robustness, particularly in noisy or ambiguous textual environments. Transformer-based frameworks, \\nsuch as Multimodal Bi-transformers (MMBT) and Vision-and-Language Transformer (ViLT), are also promis\\xad\\ning for aligning visual, textual, and sensor-based information within a unified pipeline.\\nA practical implementation is demonstrated in Fatichah et al. (2020), who developed a TED system that \\nintegrates Twitter/X text and images. Their approach used CNNs for image analysis and C-LSTM for text \\nprocessing. By multimodal data integration, the system improved detection accuracy compared to text- \\nonly approaches. This shows the potential of multimodal deep learning for enhancing incident detection \\nby leveraging complementary sources of information available on social media platforms. Figure 11'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 21}, page_content='by leveraging complementary sources of information available on social media platforms. Figure 11\\nexpands on this framework, illustrating a conceptual pipeline that incorporates additional Twitter/X \\ndata such as audio, video, and IoT sensor data alongside Twitter/X text for enhanced TED.\\n4.5 Deep learning models hardware constraints\\nThe need for extensive hardware resources when deploying deep learning models, especially Transformer-based \\narchitectures, is considered a challenge(Burhanuddin 2023; Lyu et al. 2022). For example, achieving inference \\nlatency below 500 ms requires GPU acceleration with at least 16 GB of VRAM. For medium- to large-scale                        \\nFigure 11. Conceptual framework for multimodal data fusion in traffic event detection (TED), integrating Twitter/X text, \\naudio, video, and IoT sensor data. This figure is an expanded version of the framework proposed in Fatichah et al. ( 2020),'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 21}, page_content='audio, video, and IoT sensor data. This figure is an expanded version of the framework proposed in Fatichah et al. ( 2020), \\nillustrating how deep learning models can jointly process heterogeneous data streams to improve incident detection \\naccuracy and robustness.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n21'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 22}, page_content='processing, such as handling more than 10,000 tweets/second, TPU integration or distributed GPU clusters are \\nrecommended to maintain throughput (Cao et al. 2025). A number of memory architecture optimisation \\ntechniques were suggested, such as halving the embedding size, parameter pruning, and quantisation (Saai and \\nVijayakumar 2024). In addition, DeepSpeed-Inference offers a multi-GPU and heterogeneous inference solution \\nthat significantly reduces latency and increases throughput (Yazdani et al. 2022). It enables the inference of \\nmodels up to 25 times larger than those supported by GPU-only solutions.\\n4.6 Large language models (LLMs) for TED\\nLLMs, such as ChatGPT and Claude, are not widely adopted in TED research. However, their capabilities \\nin handling informal, multilingual, and ambiguous language show a promising path in future applications. \\nAdditionally, LLMs support zero- and few-shot learning, empowering classification, summarisation, and'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 22}, page_content='Additionally, LLMs support zero- and few-shot learning, empowering classification, summarisation, and \\nsentiment analysis with minimal labelled data (Wandelt et al. 2024). Still, challenges such as high \\ncomputational costs, Limited open access to real-time inference, fine-tuning issues on traffic data, and \\nreproducibility issues due to API-based LLMs impede their adaptability in the TED research (Zhang et al. \\n2024). Hybrid systems combining LLMs for preprocessing with lightweight models for detection may offer \\nan effective path forward. As LLMs evolve, their role in multilingual and multimodal TED systems calls for \\nfurther exploration (Mahmud et al. 2025).\\n4.7 Complexity of DL and the need for explainability and interpretability\\nDL models often function as black boxes, making it essential to improve their interpretability and \\ndebugging for reliable predictions. Researchers are developing AI techniques to understand and refine \\nmodel processes. Common approaches include:'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 22}, page_content='debugging for reliable predictions. Researchers are developing AI techniques to understand and refine \\nmodel processes. Common approaches include:\\n• Attention visualisation, which highlights key parts of a tweet sequence during prediction to assess \\nelement significance.  \\n• Counterfactual explanations, or ‘what-if’ analysis, examining how changes in specific tweet elements \\naffect predictions.  \\n• SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), \\ntwo model-agnostic interpretability techniques that help explain predictions by attributing them to \\nindividual features.\\n4.8 The constant change of language and behaviour of social media users\\nIncremental learning techniques can help models continuously learn from new data while supporting \\nprevious knowledge and adapting to changing language patterns on social media. Model adaptation \\ninvolves training on historical tweets with labelled data and applying this knowledge to current tweets'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 22}, page_content='involves training on historical tweets with labelled data and applying this knowledge to current tweets \\nwhere labelled data is limited, enabling knowledge transfer for improved performance.\\nFuture research should focus on transfer learning by using pre-trained models on large text corpora to \\naddress the limited labelled Twitter/X data challenge and improve model generalisation and performance. \\nMoreover, Deep fusion techniques can also be explored by integrating multiple DL models with different \\narchitectures or data subsets to enhance accuracy and robustness. By addressing these challenges, \\nresearchers can enhance DL models for Twitter/X traffic event detection, resulting in improved traffic \\nmanagement, reduced congestion, and real-time commuter awareness.\\nAuthor contributions\\nCRediT: Danya Qutaishat conducted the literature review, analysed the data, draughted the manuscript, and revised it'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 22}, page_content='Author contributions\\nCRediT: Danya Qutaishat conducted the literature review, analysed the data, draughted the manuscript, and revised it \\nbased on feedback. Songnian Li supervised the study, contributed to the conceptual design, critically reviewed the \\nmanuscript, and provided intellectual guidance.\\n22\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 23}, page_content='Disclosure statement\\nNo potential conflict of interest was reported by the author(s).\\nFunding\\nThis work was funded by the Natural Sciences and Engineering Research Council of Canada, grant number RGPIN- \\n2017-05950.\\nData availability statement\\nThe data supporting the findings of this study come from public datasets, which are described in Section 2.2. Data \\nsharing is not applicable to this article as no new data were created or analysed in this study.\\nReferences\\nAbad, A., O. Alfonso, T. António, M. Carmen García, M.H. Carlos D, P. Fernando, B. Fernando and M. Nuno, eds. \\n2016. Advances in Speech and Language Technologies for Iberian Languages. In Lecture Notes in Computer \\nScience. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-49169-1.\\nAfyouni, I., Z. A. Aghbari, and R. A. Razack. 2022. “Multi-Feature, Multi-Modal, and Multi-Source Social Event Detection:'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 23}, page_content='Afyouni, I., Z. A. Aghbari, and R. A. Razack. 2022. “Multi-Feature, Multi-Modal, and Multi-Source Social Event Detection: \\nA Comprehensive Survey.” Information Fusion 79: 279–308. https://doi.org/10.1016/j.inffus.2021.10.013.\\nAminabadi Yazdani, R., S. Rajbhandari, A.A. Awan, C. Li, Du Li, E. Zheng, O. Ruwase, R. Y. Aminabadi, S. Smith, M. \\nZhang, J. Rasley, and Y. He. 2022. DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at \\nUnprecedented Scale. In SC22: International Conference for High Performance Computing, Networking, Storage \\nand Analysis, 1–15. Dallas, TX, USA: IEEE. https://doi.org/10.1109/SC41404.2022.00051\\nAlam, F., F. Ofli, and M. Imran. 2018. “CrisisMMD: Multimodal Twitter Datasets from Natural Disasters.” Proceedings of \\nthe International AAAI Conference on Web and Social Media 12(1), https://doi.org/10.1609/icwsm.v12i1.14983.\\nAli, F., D. Kwak, P. Khan, S. Islam, K. H. Kim, and K. S. Kwak. 2017. “Fuzzy Ontology-Based Sentiment Analysis of'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 23}, page_content='Ali, F., D. Kwak, P. Khan, S. Islam, K. H. Kim, and K. S. Kwak. 2017. “Fuzzy Ontology-Based Sentiment Analysis of \\nTransportation and City Feature Reviews for Safe Traveling.” Transportation Research Part C: Emerging \\nTechnologies 77: 33–48. https://doi.org/10.1016/j.trc.2017.01.014.\\nAli, F., A. Ali, M. Imran, R. Ali Naqvi, M. H. Siddiqi, and K.-S. Kwak. 2021. “Traffic Accident Detection and \\nCondition Analysis Based on Social Networking Data.” Accident Analysis & Prevention 151: 105973. https:// \\ndoi.org/10.1016/j.aap.2021.105973.\\nAli, F., D. Kwak, P. Khan, S. El-Sappagh, A. Ali, S. Ullah, K. H. Kim, and K.-S. Kwak. 2019. “Transportation Sentiment \\nAnalysis Using Word Embedding and Ontology-Based Topic Modeling.” Knowledge-Based Systems 174: 27–42. \\nhttps://doi.org/10.1016/j.knosys.2019.02.033.\\nAlifi, M. R. and S. H. Supangkat. 2018. Information Extraction of Traffic Condition from Social Media Using'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 23}, page_content='https://doi.org/10.1016/j.knosys.2019.02.033.\\nAlifi, M. R. and S. H. Supangkat. 2018. Information Extraction of Traffic Condition from Social Media Using \\nBidirectional LSTM-CNN. In 2018 International Seminar on Research of Information Technology and Intelligent \\nSystems (ISRITI), 637–640. https://doi.org/10.1109/ISRTI.2018.8864265\\nAlmassar, R. R., and A. S. Girsang. 2022. “Detection of Traffic Congestion Based on Twitter Using Convolutional \\nNeural Network Model.” IAES International Journal of Artificial Intelligence (IJ-AI) 11 (4): 1448–1459. https:// \\ndoi.org/10.11591/ijai.v11.i4.pp1448-1459.\\nAlomari, E., I. Katib, A. Albeshri, T. Yigitcanlar, and R. Mehmood. 2021. “Iktishaf+: A Big Data Tool with Automatic \\nLabeling for Road Traffic Social Sensing and Event Detection Using Distributed Machine Learning.” Sensors 21(9): \\n2993. https://doi.org/10.3390/s21092993.\\nAlomari, E. A., and R. Mehmood. 2023. “Smart Cities, Smarter Roads: A Review of Leveraging Cutting-Edge'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 23}, page_content='2993. https://doi.org/10.3390/s21092993.\\nAlomari, E. A., and R. Mehmood. 2023. “Smart Cities, Smarter Roads: A Review of Leveraging Cutting-Edge \\nTechnologies for Intelligent Event Detection from Social Media.” International Journal of Advanced Computer \\nScience and Applications 14(11): 364–374. https://doi.org/10.14569/ijacsa.2023.014113.\\nAlomari, E., R. Mehmood, and I. Katib. 2019. Road Traffic Event Detection Using Twitter Data, Machine Learning, \\nand Apache Spark. In 2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted \\nComputing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and \\nSmart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), 1888–1895. Leicester, UK: IEEE. \\nhttps://doi.org/10.1109/SmartWorld-UIC-ATC-ScalCom-IOP-SCI.2019.00332\\nAmbastha, P. and M. S. Desarkar. 2020. Incident Detection From Social Media Targeting Indian Traffic Scenario'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 23}, page_content='https://doi.org/10.1109/SmartWorld-UIC-ATC-ScalCom-IOP-SCI.2019.00332\\nAmbastha, P. and M. S. Desarkar. 2020. Incident Detection From Social Media Targeting Indian Traffic Scenario \\nUsing Transfer Learning. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems \\n(ITSC), 1–6. Rhodes, Greece: IEEE. https://doi.org/10.1109/ITSC45102.2020.9294295\\nAnda, C., A. Erath, and P. J. Fourie. 2017. “Transport Modelling in the Age of Big Data.” International Journal of \\nUrban Sciences 21(sup1): 19–42. https://doi.org/10.1080/12265934.2017.1281150.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n23'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content='Asudani, D., N. K. Suresh, P. Nagwani, and Singh. 2023. “Impact of Word Embedding Models on Text Analytics in \\nDeep Learning Environment: A Review.” Artificial Intelligence Review 56(9): 10345–10425. https://doi.org/ \\n10.1007/s10462-023-10419-1.\\nAtefeh, F., and W. Khreich. 2013. “A Survey of Techniques for Event Detection in Twitter.” Computational \\nIntelligence 31(1): 132–164. https://doi.org/10.1111/coin.12017.\\nAzhar, A., S. Rubab, M. M. Khan, Y. A. Bangash, M. D. Alshehri, F. Illahi, and A. K. Bashir. 2022. “Detection and \\nPrediction of Traffic Accidents Using Deep Learning Techniques.” Cluster Computing 26(1): 477–493. https:// \\ndoi.org/10.1007/s10586-021-03502-1.\\nBilgin, M. and I. F. Şentürk. 2017. Sentiment Analysis on Twitter Data with Semi-Supervised Doc2Vec. In 2017 \\nInternational Conference on Computer Science and Engineering (UBMK), 661–666. Antalya, Turkey: \\nIEEE.https://doi.org/10.1109/UBMK.2017.8093492'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content='International Conference on Computer Science and Engineering (UBMK), 661–666. Antalya, Turkey: \\nIEEE.https://doi.org/10.1109/UBMK.2017.8093492\\nBlakey, E. 2024. “The Day Data Transparency Died: How Twitter/X Cut Off Access for Social Research.” Contexts \\n23(2): 30–35. https://doi.org/10.1177/15365042241252125.\\nBonandrini, R., and D. Gatti. 2024. “FastText (Sub)Word Vectors, Reference Module in Social Sciences. https:// \\ndoi.org/10.1016/b978-0-323-95504-1.00032-6.\\nBurhanuddin, M. A. 2023. “Efficient Hardware Acceleration Techniques for Deep Learning on Edge Devices: A \\nComprehensive Performance Analysis.” KHWARIZMIA 2023: 103–112. https://doi.org/10.70470/khwarizmia/2023/010.\\nCao, S., S. Liu, T. Griggs, P. Schafhalter, X. Liu, Y. Sheng, J. E. Gonzalez, M. Zaharia, and I. Stoica. 2025. Moe-Lightning: \\nHigh-Throughput MoE Inference on Memory-Constrained GPUs. In Proceedings of the 30th ACM International'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content=\"High-Throughput MoE Inference on Memory-Constrained GPUs. In Proceedings of the 30th ACM International \\nConference on Architectural Support for Programming Languages and Operating Systems, 715–730.https://doi.org/ \\n10.1145/3669940.3707267\\nChen, Q. and W. Wang. 2019. Multi-Modal Neural Network for Traffic Event Detection. In 2019 IEEE 2nd \\nInternational Conference on Electronics and Communication Engineering (ICECE), 26–30. Xi'an, China: IEEE. \\nhttps://doi.org/10.1109/ICECE48499.2019.9058508\\nChen, P.-T., F. Chen, and Z. Qian. 2014. Road Traffic Congestion Monitoring in Social Media with Hinge-Loss \\nMarkov Random Fields. In 2014 IEEE International Conference on Data Mining.80–89. Shenzhen, China: IEEE. \\nhttps://doi.org/10.1109/ICDM.2014.139\\nChen, Y., Y. Lv, X. Wang, L. Li, and F.-Y. Wang. 2018. Detecting Traffic Information From Social Media Texts With \\nDeep Learning Approaches. In IEEE Transactions on Intelligent Transportation Systems,20: 3049–3058.\"),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content='Deep Learning Approaches. In IEEE Transactions on Intelligent Transportation Systems,20: 3049–3058. \\nIEEE.https://doi.org/10.1109/TITS.2018.2871269\\nCottrill, C., P. Gault, G. Yeboah, J. D. Nelson, J. Anable, and T. Budd. 2017. “Tweeting Transit: An Examination of \\nSocial Media Strategies for Transport Information Management During a Large Event.” Transportation Research \\nPart C: Emerging Technologies 77: 421–432. https://doi.org/10.1016/j.trc.2017.02.008.\\nD’Andrea, E., P. Ducange, B. Lazzerini, and F. Marcelloni. 2015. Real-Time Detection of Traffic From Twitter Stream \\nAnalysis In IEEE Transactions on Intelligent Transportation Systems, 16: 2269–2283. IEEE.https://doi.org/ \\n10.1109/TITS.2015.2404431\\nDabiri, S., and K. Heaslip. 2019. “Developing a Twitter-Based Traffic Event Detection Model Using Deep Learning \\nArchitectures.” Expert Systems with Applications 118: 425–439. https://doi.org/10.1016/j.eswa.2018.10.017.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content='Architectures.” Expert Systems with Applications 118: 425–439. https://doi.org/10.1016/j.eswa.2018.10.017.\\nDeho, B.O., A. William Agangiba, L. Felix Aryeh, and A. Jeffery Ansah. Sentiment Analysis With Word Embedding. \\nIn 2018 IEEE 7th International Conference on Adaptive Science & Technology (ICAST), 1–4.\\nDhiman, A., and D. Toshniwal. 2020. “An Approximate Model for Event Detection From Twitter Data.” IEEE Access \\n8: 122168–122184. https://doi.org/10.1109/ACCESS.2020.3007004.\\nDinesh, L., B. Kuhaneswaran, and N. Ravikumar. 2023. A Multifaceted Machine Learning Approach to Understand Road \\nAccident Dynamics Using Twitter Data. In Handbook of Research on Advancements of Contactless Technology and \\nService Innovation in Library and Information Science, 247–267. https://doi.org/10.4018/978-1-6684-7693-2.ch013\\nDinga, R., P. Brenda W. J. H, V. Dick J, S. Lianne, and M. Andre F. 2019. “Beyond Accuracy: Measures for Assessing'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content='Dinga, R., P. Brenda W. J. H, V. Dick J, S. Lianne, and M. Andre F. 2019. “Beyond Accuracy: Measures for Assessing \\nMachine Learning Models, Pitfalls and Guidelines.” bioRxiv, 743138. https://doi.org/10.1101/743138.\\nDoguc, T. B., and A. A. Ahmet. 2023. “Tweet Toplama, Analiz ve Depolama İçin Platform Tasarımı (TweetCASP).” \\nComputer Science, https://doi.org/10.53070/bbd.1344271.\\nEffrosynidis, D., G. Sylaios, and A. Arampatzis. 2024. “The Effect of Training Data Size on Disaster Classification \\nFrom Twitter.” Information 15(7): 393. https://doi.org/10.3390/info15070393.\\nElSahly, O., and A. Abdelfatah. 2023. “An Incident Detection Model Using Random Forest Classifier.” Smart Cities \\n6(4): 1786–1813. https://doi.org/10.3390/smartcities6040083.\\nEs Swidi, A., S. Ardchir, A. Daif, and M. Azouazi. 2023. “Road Users Detection for Traffic Congestion Classification.” \\nMathematical Modeling and Computing 10(2): 518–523. https://doi.org/10.23939/mmc2023.02.518.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 24}, page_content='Mathematical Modeling and Computing 10(2): 518–523. https://doi.org/10.23939/mmc2023.02.518.\\nFatichah, C., P. D. Sammy Wiyadi, D. Adni Navastara, N. Suciati, and A. Munif. 2020. Incident Detection Based on \\nMultimodal Data From Social Media Using Deep Learning Methods. In 2020 International Conference on ICT for \\nSmart Society (ICISS), 1–6. Bandung, Indonesia: IEEE. https://doi.org/10.1109/ICISS50791.2020.9307555\\nGannina Kumar, A., A. A. Ram, T. M. Jaffarullah, S. M. Reddy, A. S. Subba Reddy, S. Vikas, V. Mathi, and \\nRamalingam. 2024. “A New Approach to Road Incident Detection Leveraging Live Traffic Data: An Empirical \\nInvestigation.” Procedia Computer Science 235: 2288–2296. https://doi.org/10.1016/j.procs.2024.04.217.\\n24\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='Garg, M., and M. Kumar. 2016. “Review on Event Detection Techniques in Social Multimedia.” Online Information \\nReview 40(3): 347–361. https://doi.org/10.1108/OIR-08-2015-0281.\\nGu, Y., Z. (S. Qian, and F. Chen. 2016. “From Twitter to Detector: Real-Time Traffic Incident Detection Using Social Media \\nData.” Transportation Research Part C: Emerging Technologies 67: 321–342. https://doi.org/10.1016/j.trc.2016.02.011.\\nGu, W., A. Tandon, Y.-Y. Ahn, and F. Radicchi. 2021. “Principled Approach to the Selection of the Embedding \\nDimension of Networks.” Nature Communications 12(1): 3772. https://doi.org/10.1038/s41467-021-23795-5.\\nHall, F. L., Y. Shi, and G. Atala. 1993. “On-Line Testing of the McMaster Incident Detection Algorithm Under \\nRecurrent Congestion.” Transportation Research Record 1394: 1–7.\\nHasan, S., and S. V. Ukkusuri. 2014. “Urban Activity Pattern Classification Using Topic Models From Online Geo-Location'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='Recurrent Congestion.” Transportation Research Record 1394: 1–7.\\nHasan, S., and S. V. Ukkusuri. 2014. “Urban Activity Pattern Classification Using Topic Models From Online Geo-Location \\nData.” Transportation Research Part C: Emerging Technologies 44: 363–381. https://doi.org/10.1016/j.trc.2014.04.003.\\nHasnat, M. M., and S. Hasan. 2018. “Identifying Tourists and Analyzing Spatial Patterns of Their Destinations From \\nLocation-Based Social Media Data.” Transportation Research Part C: Emerging Technologies 96: 38–54. https:// \\ndoi.org/10.1016/j.trc.2018.09.006.\\nHasnat, M., A. Mehedi, N. Faghih-Imani, S. Eluru, and Hasan. 2019. “Destination Choice Modeling Using Location- \\nBased Social Media Data.” Journal of Choice Modelling 31: 22–34. https://doi.org/10.1016/j.jocm.2019.03.002.\\nHuang, A., L. Gallegos, and K. Lerman. 2017. “Travel Analytics: Understanding How Destination Choice and Business'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='Huang, A., L. Gallegos, and K. Lerman. 2017. “Travel Analytics: Understanding How Destination Choice and Business \\nClusters Are Connected Based on Social Media Data.” Transportation Research Part C: Emerging Technologies 77: \\n245–256. https://doi.org/10.1016/j.trc.2016.12.019.\\nHussain, N. Y. 2024. “Deep Learning Architectures Enabling Sophisticated Feature Extraction and Representation for \\nComplex Data Analysis.” International Journal of Innovative Science and Research Technology (IJISRT) 9(10): \\n2290–2300. https://doi.org/10.38124/ijisrt/ijisrt24oct1521.\\nImran, M., C. Castillo, F. Diaz, and S. Vieweg. 2015. “Processing Social Media Messages in Mass Emergency.” ACM \\nComputing Surveys 47(4): 1–38. https://doi.org/10.1145/2771588.\\nJain, E., Neeraja, J., Banerjee, B., and Ghosh, P. 2022. A Diagnostic Approach to Assess the Quality of Data Splitting in \\nMachine LearningarXiv preprint arXiv:2206.11721. https://doi.org/10.48550/arXiv.2206.11721'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='Machine LearningarXiv preprint arXiv:2206.11721. https://doi.org/10.48550/arXiv.2206.11721\\nJain, S., R. Pankaj, Z. Sharma, and Fatima. 2023. Traffic Rule Violation and Accident Detection Using CNN. In \\nInternational Conference on Innovative Computing and Communications, 867–878.https://doi.org/10.1007/978- \\n981-99-4071-4_66\\nJiang, H., and H. Deng. 2020. “Traffic Incident Detection Method Based on Factor Analysis and Weighted Random \\nForest.” IEEE Access 8: 168394–168404. https://doi.org/10.1109/ACCESS.2020.3023961.\\nJiang, J., C. Zhang, L. Ke, N. Hayes, Y. Zhu, H. Qiu, B. Zhang, T. Zhou, and G.-W. Wei. 2025. “A Review of Machine \\nLearning Methods for Imbalanced Data Challenges in Chemistry.” Chemical Science 16(18): 7637–7658. https:// \\ndoi.org/10.1039/D5SC00270B.\\nJohnson, J. M., and T. M. Khoshgoftaar. 2019. “Survey on Deep Learning With Class Imbalance.” Journal of Big Data \\n6(1): 27. https://doi.org/10.1186/s40537-019-0192-5.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='doi.org/10.1039/D5SC00270B.\\nJohnson, J. M., and T. M. Khoshgoftaar. 2019. “Survey on Deep Learning With Class Imbalance.” Journal of Big Data \\n6(1): 27. https://doi.org/10.1186/s40537-019-0192-5.\\nJonnalagadda, J. and M. Hashemi. 2021. A Deep Learning-Based Traffic Event Detection From Social Media. In 2021 \\nIEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI), 1–8. Las \\nVegas, NV, USA: IEEE. https://doi.org/10.1109/IRI51335.2021.00007\\nKamkarhaghighi, M., and M. Makrehchi. 2017. “Content Tree Word Embedding for Document Representation.” \\nExpert Systems with Applications 90: 241–249. https://doi.org/10.1016/j.eswa.2017.08.021.\\nKarthik, B., D. Sai, D. NSSVV, M. A. Bharat, B. T. Alam, M. Sai, S. Rana, S. A. Sharma, and Yadav. 2023. Sentimental \\nTechnique Implementation on Textual Data. In 2023 6th International Conference on Contemporary Computing'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='Technique Implementation on Textual Data. In 2023 6th International Conference on Contemporary Computing \\nand Informatics (IC3I). 82–86, Gautam Buddha Nagar, India: IEEE. https://doi.org/10.1109/IC3I59117.2023.10397771\\nKim, Y., S. Song, H. Lee, D. Choi, J. Lim, K. Bok, and J. Yoo. 2023. “Regional Traffic Event Detection Using Data \\nCrowdsourcing.” Applied Sciences 13(16): 9422. https://doi.org/10.3390/app13169422.\\nKisters, P., T. Bauer, W. Posdorfer, and J. Edinger. 2023. Real-Time Traffic Congestion Detection for Driver-Centric \\nApplications. In 2023 IEEE 43rd International Conference on Distributed Computing Systems Workshops \\n(ICDCSW), 163–168. Hong Kong, Hong Kong: IEEE. https://doi.org/10.1109/ICDCSW60045.2023.00029\\nLee, S., S. Lee, J. Noh, J. Kim, and H. Jeong. 2023. “Special Traffic Event Detection: Framework, Dataset Generation, and \\nDeep Neural Network Perspectives.” Sensors (Basel, Switzerland) 23(19): 8129. https://doi.org/10.3390/s23198129.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='Deep Neural Network Perspectives.” Sensors (Basel, Switzerland) 23(19): 8129. https://doi.org/10.3390/s23198129.\\nLi, L., Y. Dou, and J. Zhou. 2023. Traffic Accident Detection Based on Multimodal Knowledge Graphs. In 2023 5th \\nInternational Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI),  \\n644–647. \\nHangzhou, China: IEEE.https://doi.org/10.1109/RICAI60863.2023.10489808\\nLi, R., K. H. Lei, R. Khadiwala, and K. C.-C. Chang. 2012. TEDAS: A Twitter-Based Event Detection and Analysis \\nSystem In 2012 IEEE 28th International Conference on Data Engineering (ICDE), 1273–1276. Arlington, VA, \\nUSA: IEEE.https://doi.org/10.1109/ICDE.2012.125\\nLi, Y., L. Li, H. Jing, B. Ran, and D. Sun. 2020. “Examining imbalanced classification algorithms in predicting real- \\ntime traffic crash risk.” Accident Analysis & Prevention 144: 105610. https://doi.org/10.1016/j.aap.2020.105610.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 25}, page_content='time traffic crash risk.” Accident Analysis & Prevention 144: 105610. https://doi.org/10.1016/j.aap.2020.105610.\\nLin, L., M. Ni, Q. He, J. Gao, and A. W. Sadek. 2015. “Modeling the Impacts of Inclement Weather on Freeway Traffic \\nSpeed: Exploratory Study With Social Media Data.” Transportation Research Record: Journal of the \\nTransportation Research Board 2482(1): 82–89. https://doi.org/10.3141/2482-11.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n25'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='Lin, Yi., L. Li, H. Jing, B. Ran, and D. Sun. 2020. “Automated Traffic Incident Detection with a Smaller Dataset Based \\non Generative Adversarial Networks.” Accident Analysis & Prevention 144: 105610.  https://doi.org/10.1016/ \\nj.aap.2020.105610.\\nLiu, J., T. Li, P. Xie, S. Du, F. Teng, and X. Yang. 2020. “Urban Big Data Fusion Based on Deep Learning: An \\nOverview.” Information Fusion 53: 123–133. https://doi.org/10.1016/j.inffus.2019.06.016.\\nLiu, H., M. Sheng, Z. Sun, Y. Yao, X.-S. Hua, and H.-T. Shen. 2024. “Learning With Imbalanced Noisy Data by \\nPreventing Bias in Sample Selection.” IEEE Transactions on Multimedia 26: 7426–7437. https://doi.org/10.1109/ \\nTMM.2024.3368910.\\nLiu, Y., X. Shen, Y. Zhang, Z. Wang, Y. Tian, J. Dai, and Y. Cao. 2024. A Systematic Review of Machine Learning \\nApproaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases.arXiv preprint \\narXiv:2403.20293. https://doi.org/10.48550/arXiv.2403.20293'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='Approaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases.arXiv preprint \\narXiv:2403.20293. https://doi.org/10.48550/arXiv.2403.20293\\nLu, H., K. Shi, Y. Zhu, Y. Lv, and Z. Niu. 2018. “Sensing Urban Transportation Events from Multi-Channel Social Signals \\nwith the Word2vec Fusion Model.” Sensors (Basel, Switzerland) 18(12): 4093. https://doi.org/10.3390/s18124093.\\nMikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. “Distributed Representations of Words and \\nPhrases and Their Compositionality.” Advances in Neural Information Processing Systems 26: 3111–3119. Red \\nHook, NY: Curran Associates. \\nLyu, B., H. Yuan, L. Lu, and Y. Zhang. 2022. “Resource-Constrained Neural Architecture Search on Edge Devices.” IEEE \\nTransactions on Network Science and Engineering 9(1): 134–142. https://doi.org/10.1109/TNSE.2021.3054583.\\nMaghrebi, M., A. Abbasi, and S. Waller. 2016. Transportation Application of Social Media: Travel Mode Extraction.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='Maghrebi, M., A. Abbasi, and S. Waller. 2016. Transportation Application of Social Media: Travel Mode Extraction. \\nIn 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), 1648–1653. Rio de \\nJaneiro, Brazil: IEEE. https://doi.org/10.1109/ITSC.2016.7795779\\nMahmud, D., H. Hajmohamed, S. Almentheri, S. Alqaydi, L. Aldhaheri, R. A. Khalil, and N. Saeed. 2025. “Integrating \\nLLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions.” IEEE Transactions on Intelligent \\nTransportation Systems 26(5): 5674–5709. https://doi.org/10.1109/TITS.2025.3528116.\\nMannes, J. 2017. Facebook’s fastText Library Is Now Optimized for MobileTechCrunch https://TechCrunch.com/201 \\n7/05/02/facebooks-fasttext-library-is-now-optimized-for-mobile/\\nMelhem, W., A. Abdi, and F. Meziane. 2024. “Traffic Detection and Forecasting from Social Media Data Using a Deep'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='7/05/02/facebooks-fasttext-library-is-now-optimized-for-mobile/\\nMelhem, W., A. Abdi, and F. Meziane. 2024. “Traffic Detection and Forecasting from Social Media Data Using a Deep \\nLearning-Based Model, Linguistic Knowledge, Large Language Models, and Knowledge Graphs.” Proceedings of the \\n16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge \\nManagement. 235–242. https://doi.org/10.5220/0013066900003838.\\nMikolov, T., Quoc V, L., and Ilya, S. 2013. Exploiting Similarities Among Languages for Machine Translation.” arXiv \\npreprint arXiv:1309.4168. https://doi.org/10.48550/arXiv.1309.4168\\nMolloy, J., and R. Moeckel. 2017. “Improving Destination Choice Modeling Using Location-Based Big Data.” ISPRS \\nInternational Journal of Geo-Information 6(9): 291. https://doi.org/10.3390/ijgi6090291.\\nMredula, M.S., C. Noyon, S.R. Md, M. Imtiaz, and C. You-Ze. 2022. “A Review on the Trends in Event Detection by'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='Mredula, M.S., C. Noyon, S.R. Md, M. Imtiaz, and C. You-Ze. 2022. “A Review on the Trends in Event Detection by \\nAnalyzing Social Media Platforms’ Data.” Sensors (Basel, Switzerland) 22(12): 4531. https://doi.org/10.3390/s22124531.\\nMünz, G., L. Sa, and C. Georg. 2007. “Traffic Anomaly Detection Using K-Means Clustering.” GI/ITG Workshop \\nMMBnet 7 (9). https://www.net.in.tum.de/projects/dfg-lupus/files/muenz07k-means.pdf.\\nMurtfeldt, R., Naomi, A., Ihsan, K., and Jevin D, W. 2024. RIP Twitter API: A Eulogy to Its Vast Research \\nContributions.” arXiv preprint arXiv:2404.07340 https://doi.org/10.48550/arXiv.2404.07340\\nNama, N., M. Sampson, N. Barrowman, R. Sandarage, K. Menon, G. Macartney, K. Murto, et al.2019. “Crowdsourcing \\nthe Citation Screening Process for Systematic Reviews: Validation Study.” Journal of Medical Internet Research \\n21(4): e12953. https://doi.org/10.2196/12953.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='the Citation Screening Process for Systematic Reviews: Validation Study.” Journal of Medical Internet Research \\n21(4): e12953. https://doi.org/10.2196/12953.\\nNejjari, F., L. Benhlima, and S. Bah. 2016. Event Traffic Detection Using Heterogeneous Wireless Sensors Network. In \\n2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA), 1–6. https:// \\ndoi.org/10.1109/AICCSA.2016.7945825\\nNeruda, G. A. and E. Winarko. 2021. Traffic Event Detection From Twitter Using a Combination of CNN and BERT. \\nIn 2021 International Conference on Advanced Computer Science and Information Systems (ICACSIS), 1–7. \\nhttps://doi.org/10.1109/ICACSIS53237.2021.9631334\\nNi, M., Q. He, and J. Gao. 2017. “Forecasting the Subway Passenger Flow Under Event Occurrences With Social \\nMedia.” IEEE Transactions on Intelligent Transportation Systems 18(6): 1623–1632. https://doi.org/10.1109/ \\nTITS.2016.2611644.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 26}, page_content='Media.” IEEE Transactions on Intelligent Transportation Systems 18(6): 1623–1632. https://doi.org/10.1109/ \\nTITS.2016.2611644.\\nNirbhaya, M. A. W. and L. H. Suadaa. 2023. Traffic Incident Detection in Jakarta on Twitter Texts Using a Multi- \\nLabel Classification Approach. In 2023 International Conference on Computer, Control, Informatics and Its \\nApplications (IC3INA), 290–295. https://doi.org/10.1109/IC3INA60834.2023.10285731\\nNoori, M. A. R., and R. Mehra. 2020. “Traffic Congestion Detection from Twitter Using Word2vec.” In ICT Analysis \\nand Applications, 527–534. https://doi.org/10.1007/978-981-15-8354-4_52.\\nOlteanu, A., S. Vieweg, and C. Castillo. 2015. What to Expect When the Unexpected Happens: Social Media \\nCommunications Across Crises. In Proceedings of the 18th ACM Conference on Computer Supported \\nCooperative Work & Social Computing, 994–1009. https://doi.org/10.1145/2675133.2675242\\n26\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='Page, M. J., J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C. Hoffmann, C. D. Mulrow, L. Shamseer, et al. 2021. “The \\nPRISMA 2020 Statement: An Updated Guideline for Reporting Systematic Reviews.” International Journal of \\nSurgery 88: 105906. https://doi.org/10.1016/j.ijsu.2021.105906.\\nPage, M. J., J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C. Hoffmann, C. D. Mulrow, L. Shamseer, J. M. Tetzlaff, and \\nM. David. 2020. “Updating Guidance for Reporting Systematic Reviews: Development of the PRISMA 2020 \\nStatement.” OSF Preprints, https://doi.org/10.31222/osf.io/jb4dx.\\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe: Global Vectors for Word Representation In Proceedings \\nof the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. https:// \\ndoi.org/10.3115/v1/D14-1162\\nPita, M. and G. L. Pappa. 2018. Strategies for Short Text Representation in the Word Vector Space. In 2018 7th'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='doi.org/10.3115/v1/D14-1162\\nPita, M. and G. L. Pappa. 2018. Strategies for Short Text Representation in the Word Vector Space. In 2018 7th \\nBrazilian Conference on Intelligent Systems (BRACIS). 266–271. https://doi.org/10.1109/BRACIS.2018.00053\\nplatforms’ data.\" 22(12): 4531.\\nPoudel, A., and T. Weninger. 2024. “Navigating the Post-API Dilemma.” Proceedings of the ACM Web Conference \\n2024, 2476–2484. https://doi.org/10.1145/3589334.3645503.\\nPuangnak, K., and N. Rachsiriwatcharabul. 2022. “Collection of Road Traffic Incidents in Bangkok from Twitter Data \\nBased on Deep Learning Algorithm.” ECTI Transactions on Computer and Information Technology (ECTI-CIT \\n16(3): 268–277. https://doi.org/10.37936/ecticit.2022163.248535.\\nQazi, U., M. Imran, and F. Ofli. 2020. “GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19 \\nTweets With Location Information.” SIGSPATIAL Special 12(1): 6–15. https://doi.org/10.1145/3404820.3404823.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='Tweets With Location Information.” SIGSPATIAL Special 12(1): 6–15. https://doi.org/10.1145/3404820.3404823.\\nQiu, J. 2024. “An Analysis of Model Evaluation With Cross-Validation: Techniques, Applications, and Recent \\nAdvances.” Advances in Economics, Management and Political Sciences 99(1): 69–72. https://doi.org/10.54254/ \\n2754-1169/99/2024ox0213.\\nQutaishat, D., and S. Li. 2025a. “A Transformer-Based Multi-Feature Fusion Method for Detecting Traffic Events \\nUsing Twitter Data.” Big Earth Data. 1–33. https://doi.org/10.1080/20964471.2025.2564525. October.\\nQutaishat, D., and S. Li. 2025b. “Multimodal Spatiotemporal Deep Fusion for Highway Traffic Accident Prediction in \\nToronto: A Case Study and Roadmap.” ISPRS International Journal of Geo-Information 14(no. 11): 434. https:// \\ndoi.org/10.3390/ijgi14110434.\\nRajaraman, A. and J. D. Ullman. 2011. Data Mining. In In Mining of Massive Datasets. 1–17. Cambridge University \\nPress.https://doi.org/10.1017/CBO9781139058452.002'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='doi.org/10.3390/ijgi14110434.\\nRajaraman, A. and J. D. Ullman. 2011. Data Mining. In In Mining of Massive Datasets. 1–17. Cambridge University \\nPress.https://doi.org/10.1017/CBO9781139058452.002\\nRamadhani, A. M. and H. S. Goo. 2017. Twitter Sentiment Analysis Using Deep Learning Methods. In 2017 7th \\nInternational Annual Engineering Seminar (InAES),1–4. Yogyakarta, Indonesia: IEEE. https://doi.org/10.1109/ \\nINAES.2017.8068556\\nRaschka, S. 2018. Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. arXiv preprint \\narXiv:1811.12808. https://doi.org/10.48550/arXiv.1811.12808\\nRaunak, V. 2017. Simple and Effective Dimensionality Reduction for Word Embeddings. arXiv preprint \\narXiv:1708.03629. https://doi.org/10.48550/arXiv.1708.03629\\nRezaeinia, S. M., Ghodsi, A., and Rouhollah, R. 2017. Improving the Accuracy of Pre-Trained Word Embeddings for \\nSentiment Analysis.” arXiv preprint arXiv:1711.08609 https://doi.org/10.48550/arXiv.1711.08609'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='Sentiment Analysis.” arXiv preprint arXiv:1711.08609 https://doi.org/10.48550/arXiv.1711.08609\\nRudkowsky, E., M. Haselmayer, M. Wastian, M. Jenny, &A. Emrich, and M. Sedlmair. 2018. “More Than Bags of \\nWords: Sentiment Analysis With Word Embeddings.” Communication Methods and Measures 12(2–3): 140–157. \\nhttps://doi.org/10.1080/19312458.2018.1455817.\\nSaai, K. P., and Vijayakumar, V. 2024. Resource-Efficient Transformer Architecture: Optimizing Memory and Execution \\nTime for Real-Time Applications.” arXiv preprint arXiv:2501.00042 https://doi.org/10.48550/arXiv.2501.00042\\nSaeed, Z., R. A. Abbasi, O. Maqbool, A. Sadaf, I. Razzak, A. Daud, N. R. Aljohani, and G. Xu. 2019. “What’s \\nHappening Around the World? A Survey and Framework on Event Detection Techniques on Twitter.” Journal of \\nGrid Computing 17(2): 279–312. https://doi.org/10.1007/s10723-019-09482-2.\\nSaeedi, R., M. Sankaranarayanasamy, R. Vishwakarma, P. Singh, and R. Vennelakanti. 2020. Towards Modular'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='Grid Computing 17(2): 279–312. https://doi.org/10.1007/s10723-019-09482-2.\\nSaeedi, R., M. Sankaranarayanasamy, R. Vishwakarma, P. Singh, and R. Vennelakanti. 2020. Towards Modular \\nModeling and Analytic for Multi-Modal Transportation Networks. In 2020 IEEE International Conference on Big \\nData (Big Data), 2426–2432. Atlanta, GA, USA: IEEE. https://doi.org/10.1109/BigData50022.2020.9377806\\nSafitri, A., I. Sekar, S. Wijayanto, and Hadiyoso. 2024. Improving Classification Accuracy with Preprocessing \\nTechniques for Sentiment Analysis. In 2024 International Conference on Data Science and Its Applications \\n(ICoDSA), 487–490. Kuta, Bali, Indonesia: IEEE. https://doi.org/10.1109/ICoDSA62899.2024.10651657\\nSahni, A., and N. Raja. 2018. “Analyzation and Detection of Cyberbullying: A Twitter-Based Indian Case Study.” In \\nData Science and Analytics, 484–497. https://doi.org/10.1007/978-981-10-8527-7_41.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 27}, page_content='Sahni, A., and N. Raja. 2018. “Analyzation and Detection of Cyberbullying: A Twitter-Based Indian Case Study.” In \\nData Science and Analytics, 484–497. https://doi.org/10.1007/978-981-10-8527-7_41.\\nSamant, A., and H. Adeli. 2000. “Feature Extraction for Traffic Incident Detection Using Wavelet Transform and \\nLinear Discriminant Analysis.” Computer‐Aided Civil and Infrastructure Engineering 15(4): 241–250. https:// \\ndoi.org/10.1111/0885-9507.00188.\\nSampath, K. K., and M. Supriya. 2023. “Traffic Prediction in Indian Cities from Twitter Data Using Deep Learning \\nand Word Embedding Models.” In Multi-Disciplinary Trends in Artificial Intelligence, 671–682. https://doi.org/ \\n10.1007/978-3-031-36402-0_62.\\nSavvides, R., and J. Mäkelä Kai %J Statistical Analysis Puolamäki, and Data Mining: The ASA Data Science Journal \\n2023. “Model selection with bootstrap validation.” 16 (2): 162–186. https://doi.org/10.1002/sam.11606.\\nINTERNATIONAL JOURNAL OF DIGITAL EARTH\\n27'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 28}, page_content='Sethi, V., N. Bhandari, F. S. Koppelman, and J. L. Schofer. 1995. “Arterial Incident Detection Using Fixed Detector \\nand Probe Vehicle Data.” Transportation Research Part C: Emerging Technologies 3(no. 2): 99–112. https:// \\ndoi.org/10.1016/0968-090X(94)00017-Y. April.\\nSethurajan, M. R., and N. K. 2023. “An Adept Approach to Ascertain and Elude Probable Social Bots Attacks on \\nTwitter and Twitch Employing Machine Learning Approach.” MethodsX 11 (December), 102430. https://doi.org/ \\n10.1016/j.mex.2023.102430.\\nSuat-Rojas, N., C. Gutierrez-Osorio, and C. Pedraza. 2022. “Extraction and Analysis of Social Networks Data to Detect \\nTraffic Accidents.” Information 13(no. 1): 26. https://doi.org/10.3390/info13010026.\\nTorregrossa, F., R. Allesiardo, V. Claveau, N. Kooli, and G. Gravier. 2021. “A Survey on Training and Evaluation of \\nWord Embeddings.” International Journal of Data Science and Analytics 11(no. 2): 85–103. https://doi.org/ \\n10.1007/s41060-021-00242-8.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 28}, page_content='Word Embeddings.” International Journal of Data Science and Analytics 11(no. 2): 85–103. https://doi.org/ \\n10.1007/s41060-021-00242-8.\\nTsou, M.-H., Zhang, H., and Jung, C.-T. 2017. Identifying Data Noises, User BiasesJung, Ming-Hsiang, and System \\nErrors in Geo-Tagged Twitter Messages (Tweets).” arXiv preprint arXiv:1712.02433 https://doi.org/10.48550/ \\narXiv.1712.02433\\nWalsh, R., and M. Tardy. 2022. “A Comparison of Techniques for Class Imbalance in Deep Learning Classification of \\nBreast Cancer.” Diagnostics 13(no. 1): 67. https://doi.org/10.3390/diagnostics13010067.\\nWandelt, S., C. Zheng, S. Wang, Y. Liu, and X. Sun. 2024. “Large Language Models for Intelligent Transportation: A Review \\nof the State of the Art and Challenges.” Applied Sciences 14(no. 17): 7455. https://doi.org/10.3390/app14177455.\\nWilson, S., W. Magdy, B. McGillivray, K. Garimella, and G. Tyson. 2020. Urban Dictionary Embeddings for Slang'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 28}, page_content='Wilson, S., W. Magdy, B. McGillivray, K. Garimella, and G. Tyson. 2020. Urban Dictionary Embeddings for Slang \\nNLP Applications. In Proceedings of the 12th Language Resources and Evaluation Conference, 4764–4773. \\nEuropean Language Resources Association (ELR). https://aclanthology.org/2020.lrec-1.586\\nXu, S., S. Li, and R. Wen. 2018. “Sensing and Detecting Traffic Events Using Geosocial Media Data: A Review.” Computers, \\nEnvironment and Urban Systems 72(November): 146–160. https://doi.org/10.1016/j.compenvurbsys.2018.06.006.\\nYang, S. U. 2022. Anomaly Traffic Detection Based on LSTM. In 2022 IEEE 10th Joint International Information \\nTechnology and Artificial Intelligence Conference (ITAIC), 667–670. Chongqing, China: IEEE, June 17. https:// \\ndoi.org/10.1109/itaic54216.2022.9836629\\nZhang, Z., Q. He, J. Gao, and M. Ni. 2018. “A Deep Learning Approach for Detecting Traffic Accidents from Social Media'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64); modified using iTextSharp™ 5.5.13.4 ©2000-2024 iText Group NV (AGPL-version)', 'creator': 'PTC Arbortext Layout Developer 12.0.5638/W-x64', 'creationdate': '2026-01-02T16:45:07+05:30', 'source': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\A systematic review and comparative analysis of deep learning models for Twitter X-based traffic event detection.pdf', 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'A systematic review and comparative analysis of deep learning models for Twitter/X-based traffic eve', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-02T16:46:42+05:30', 'trapped': '', 'modDate': \"D:20260102164642+05'30'\", 'creationDate': \"D:20260102164507+05'30'\", 'page': 28}, page_content='doi.org/10.1109/itaic54216.2022.9836629\\nZhang, Z., Q. He, J. Gao, and M. Ni. 2018. “A Deep Learning Approach for Detecting Traffic Accidents from Social Media \\nData.” Transportation Research Part C: Emerging Technologies 86: 580–596. https://doi.org/10.1016/j.trc.2017.11.027.\\nZhang, S., D. Fu, W. Liang, Z. Zhang, B. Yu, P. Cai, and B. Yao. 2024. “TrafficGPT: Viewing, Processing and Interacting with \\nTraffic Foundation Models.” Transport Policy 150: 95–105. https://doi.org/10.1016/j.tranpol.2024.07.004.\\n28\\nD. QUTAISHAT AND S. LI'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='7658\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nDGCN-TES: Dynamic GCN-Based Multitask Model\\nWith Temporal Event Sharing for Rumor Detection\\nShuzhen Wan\\n, Guanghao Yang, Fangmin Dong, and Mengyuan Wang\\nAbstract—The rumor detection task aims to identify unofﬁ-\\ncial and unconﬁrmed information that is spreading on social\\nmedia. At any given moment, different users express their\\nopinions, focusing on some propagation events, and the posts\\nthey make gradually form a social network that expands as\\nit grows. Over time, nodes and edges form a dynamic graph\\nthat presents different states at different moments. However,\\nmost existing research focuses more on the text content, social\\ncontext, propagation mode, etc., and they ignore the factors\\nfrom many aspects and do not consider the dynamic rela-\\ntionships implied in the propagation development of social\\nmedia. To analyze these dynamic properties, this article proposes'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='from many aspects and do not consider the dynamic rela-\\ntionships implied in the propagation development of social\\nmedia. To analyze these dynamic properties, this article proposes\\na dynamic network-based multitask rumor detection method\\ncalled dynamic GCN-based multitask model with temporal\\nevent sharing for rumor detection (DGCN-TES). This method\\ncan effectively capture the dynamic patterns of relationships\\nin propagation events and change them over time to detect\\nrumors. It is mainly divided into three modules: 1) dynamic-\\ngraph convolutional network (GCN) module, which uses dynamic\\ngraph neural network to construct the propagation graph of\\nrumor events at different times, which can better capture the\\ndynamic spatial features that change over time; 2) content-long\\nshort-term memory (LSTM), which uses the LSTM network as\\na benchmark model and has been improved to better capture\\ntime-series text features over time and for multitask shared'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='short-term memory (LSTM), which uses the LSTM network as\\na benchmark model and has been improved to better capture\\ntime-series text features over time and for multitask shared\\ninteractions; and 3) temporal event sharing layer is the sharing\\nlayer, which uses time step as the basic unit of sharing, and\\nrealizes the sharing interaction between dynamic structural\\nfeatures and temporal text features between the ﬁrst two modules.\\nWe tested the method on two real-world rumor detection datasets\\nPHEME and WEIBO, and the ﬁnal results show that the method\\nimproved F1-score by more than 2.63% and 3.91% compared to\\nthe other best baselines baseline.\\nIndex Terms—Dynamic graph neural network, multitask, tem-\\nporal event sharing, time step.\\nI. INTRODUCTION\\nI\\nN the ﬁeld of social psychology, rumor refers to the circula-\\ntion of information that has not been ofﬁcially conﬁrmed\\nor news that is deliberately fabricated by human beings [1].\\nRecently, the Internet and social media platforms have been'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='tion of information that has not been ofﬁcially conﬁrmed\\nor news that is deliberately fabricated by human beings [1].\\nRecently, the Internet and social media platforms have been\\nManuscript received 27 August 2023; revised 16 April 2024 and 12 July\\n2024; accepted 31 July 2024. Date of publication 5 September 2024; date\\nof current version 3 December 2024. This work was supported in part by\\nthe National Nature Science Foundation of China under Grant U1703261 and\\nGrant 61871258. (Corresponding author: Shuzhen Wan.)\\nThe authors are with Hubei Key Laboratory of Intelligent Vision-\\nBased Monitoring for Hydroelectric Engineering, Yichang Key Laboratory\\nof Intelligent Medicine, and the College of Computer and Information\\nTechnology, China Three Gorges University, Yichang 443002, Hubei, China\\n(e-mail:\\nwanshuzhen@163.com;\\nguanghao_y@163.com;\\nfmdong@ctgu.\\nedu.cn; Meng_yuan_Wang@163.com).\\nDigital Object Identiﬁer 10.1109/TCSS.2024.3443275\\nthe main channels through which people obtain news, but the'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='wanshuzhen@163.com;\\nguanghao_y@163.com;\\nfmdong@ctgu.\\nedu.cn; Meng_yuan_Wang@163.com).\\nDigital Object Identiﬁer 10.1109/TCSS.2024.3443275\\nthe main channels through which people obtain news, but the\\nlack of censorship brings many opportunities for rumors to\\nspread. For example, news about COVID-191 [2] is spreading\\nrapidly (e.g., some self-media claim that drinking bleach can\\ncure this disease) and is recognized as a rumor by the World\\nHealth Organization.2 This kind of social opinion is very dan-\\ngerous, and some rumors can cause social panic and serious\\nconsequences if they are not identiﬁed in time. Therefore, cor-\\nrectly distinguishing social media rumor events, maintaining\\npublic order of social opinion, strengthening supervision, and\\neffectively screening rumor events are urgent tasks for today’s\\nsocial networks. With the development of artiﬁcial intelligence\\ntechnology, deep learning has been widely applied to the ﬁeld'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='effectively screening rumor events are urgent tasks for today’s\\nsocial networks. With the development of artiﬁcial intelligence\\ntechnology, deep learning has been widely applied to the ﬁeld\\nof rumor detection. Recent research [3], [4], [5] shows that by\\nmodeling the mode of spread of rumor events as a spreading tree\\nor spread diagram, similarities in structure can be used to dis-\\ntinguish rumor and nonrumor. For example, by modeling news\\nsamples into graphs and sentence classiﬁcation problems into\\ngraph classiﬁcation problems [6], these methods demonstrate\\nthe point that static spatial structures can be used for rumor\\ndetection. The reason that this idea can detect rumors well is\\nits ability to learn rumor event propagation patterns, which is\\nnot possible with methods related to textual content based [7].\\nHowever, this method of constructing communication patterns\\nis static, and it can only reﬂect the communication patterns of'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='not possible with methods related to textual content based [7].\\nHowever, this method of constructing communication patterns\\nis static, and it can only reﬂect the communication patterns of\\nrumor events at the late stage of development or when they have\\nalready ended, whereas the evolution of rumor events in the real\\nworld develops dynamically over time, and this method fails\\nto capture the dynamic characteristics of rumor events in their\\nconstant changes over time.\\nDynamic graph neural networks can be a good solution to the\\nabove problems. Some detection methods based on dynamic\\nideas have been proposed [8], [9], [10], which have better\\nperformance by simulating the propagation process of news\\nevents in the real world and capturing more delicate features\\nthat change over time. A common approach [10] tends to be\\nto construct static graphs of the same propagation event, at\\ndifferent time stages, to form a set of static graph sequences'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='that change over time. A common approach [10] tends to be\\nto construct static graphs of the same propagation event, at\\ndifferent time stages, to form a set of static graph sequences\\nthat are continuous in time. The performance of this approach\\nis limited by the number of time steps, and if the number of time\\nsteps is set too small, a better dynamic effect cannot be realized.\\nTherefore, this article tries to construct a ﬁne-grained dynamic\\n1https://thebulletin.org/2020/01/fake-news-epidemic-coronavirus-breeds-\\nhate-and-disinformation-in-india-and-beyond/\\n2https://m.facebook.com/WHO/photos/fact-drinking-methanol-ethanol-or-\\nbleach-does-not-prevent-or-cure-covid-19-and-c/3040991822612847/\\n2329-924X © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 0}, page_content='2329-924X © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7659\\ngraph neural network based on time steps, whose number of\\ntime steps corresponds to the number of postpropagation of\\nrumor events, to capture a set of transition-dedicated time-\\nbased dynamic structural features, and such dynamic structural\\nfeatures have a certain relationship with the time sequence of\\nrecurrent neural networks in the time dimension.\\nRecurrent neural networks, as a common approach to solving\\nrumor detection, are good at handling sequential data, taking\\ninto account the previous inputs when calculating each time\\nstep, and can capture and utilize the contextual information in\\nsequential data well. Therefore, it has been widely utilized in\\nrecent years. For example, Ma et al. [11] used the recurrent neu-\\nral network (RNN)-based approach to capture contextual infor-\\nmation of related posts in microblog events over time. Ahmad'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='recent years. For example, Ma et al. [11] used the recurrent neu-\\nral network (RNN)-based approach to capture contextual infor-\\nmation of related posts in microblog events over time. Ahmad\\net al. [12] constructed bidirectional long short-term memory\\n(LSTM)-RNN networks for rumor prediction by learning social\\nand content-based features. All of these methods treat textual\\ncontent as sequence data, and there are contextual and temporal\\nrelationships between different texts of a sequence, a feature\\nsimilar to the propagation process of a dynamic graph network,\\ni.e., neighboring contextual tweets correspond to nodes where\\nthe dynamic graph is being propagated in neighboring time\\nsteps. If it were possible to achieve a ﬁne-grained dynamic\\ngraph network based on time steps, then there would be a certain\\nrelationship between the sequence of diffusion of preceding and\\nsubsequent nodes in the dynamic graph, and the time sequence\\norder between the upward and downward propagation of texts in'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='relationship between the sequence of diffusion of preceding and\\nsubsequent nodes in the dynamic graph, and the time sequence\\norder between the upward and downward propagation of texts in\\nthe LSTM network. They would be able to capture the dynamic\\nstructural features of time steps and the temporal sequential\\ntext features, respectively. The above-mentioned detection ap-\\nproaches on sequential data rely only on a single feature such\\nas content, but current social networks are multifaceted and\\nchangeable, and they do not take into account features such\\nas relationships, behaviors, and communication structures in\\nrumor propagation events.\\nFrom the above processing of dynamic graphs as well as\\nLSTM, we are inspired by the fact that it is possible to combine\\nthe advantages of both from a dynamic point of view and\\ncotrain them to learn a representation that combines dynamic\\nstructural features and temporal textual features. Meanwhile,'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='the advantages of both from a dynamic point of view and\\ncotrain them to learn a representation that combines dynamic\\nstructural features and temporal textual features. Meanwhile,\\nthe multitask learning approach can combine the features of\\nmultiple tasks well, and related works [13], [14], [15], [16] often\\ndeﬁne the LSTM layer as a shared network to combine other\\nnetworks, they all use hardware parameter sharing to interact\\nwith features among other tasks, and although this approach\\nachieves parameter sharing to some extent and can improve\\nmodel performance, rich feature sharing among multiple tasks\\nis neglected. Therefore, we choose the LSTM network as a\\nshared layer to combine dynamic graph neural network and text-\\nbased LSTM network through multitasking, and the advantage\\nis that we take the time step as the basic computational unit, and\\nduring the training, multiple dynamic modules start exchanging\\nand learning each other’s features at the same time step, to better'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='is that we take the time step as the basic computational unit, and\\nduring the training, multiple dynamic modules start exchanging\\nand learning each other’s features at the same time step, to better\\nimprove prediction results.\\nIn this article, we simulate the propagation process of news\\nevents in the real world to capture its dynamic features in a\\nmore suitable way, and we design a better sharing way to train\\nthe model to eventually distinguish rumors and nonrumors.\\nThe dynamic graph convolutional network (GCN)-based mul-\\ntitask model with temporal event sharing for rumor detection is\\nproposed. It constructs rumor event propagation graphs corre-\\nsponding to various temporal stages and captures the evolving\\ndynamic structural features of rumor events by dynamic graph\\nneural networks. Simultaneously, the textual content features\\nare captured by LSTM networks. To integrate the two networks,\\nwe designed a novel sharing layer based on temporal steps.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='neural networks. Simultaneously, the textual content features\\nare captured by LSTM networks. To integrate the two networks,\\nwe designed a novel sharing layer based on temporal steps.\\nFinally, an attention mechanism is employed to update output\\nfeatures for rumor detection.\\nThe contributions of this article are as follows.\\n1) A dynamic network-based multitask detection model\\n(DGCN-TES) is proposed, which integrates dynamic\\ngraph neural network and LSTM in a multitask method,\\nand learns information that combines dynamic structural\\nfeatures and temporal textual features, which effectively\\nimproves the performance of the model.\\n2) We propose a ﬁne-grained dynamic graph neural network\\nbased on time steps, to capture dynamic structural fea-\\ntures of rumor events with more delicate transitions to\\nincrease the efﬁciency of multitask sharing and ultimately\\nimprove the detection results.\\n3) A temporal event-based sharing method (TES) is pro-\\nposed for multitasking interaction. Compared with the'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='increase the efﬁciency of multitask sharing and ultimately\\nimprove the detection results.\\n3) A temporal event-based sharing method (TES) is pro-\\nposed for multitasking interaction. Compared with the\\nprevious interaction methods, TES exerts better sharing\\nperformance, which interacts with the information of\\nmultitasks at each time step and effectively improves the\\nprediction.\\n4) We conducted a series of experiments on two real-world\\ndatasets, and the experimental results demonstrate that\\nour proposed dynamic GCN-based multitask model with\\ntemporal event sharing for rumor detection (DGCN-TES)\\nis effective and outperforms other approaches.\\nII. RELATED WORK\\nIn this section, the work related to the proposed model is\\nreviewed, and this research focuses on the following topics: text\\ncontent-based detection methods, propagation structure-based\\ndetection methods, and multitask-based detection methods.\\nA. Text Content-Based Detection Methods'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='content-based detection methods, propagation structure-based\\ndetection methods, and multitask-based detection methods.\\nA. Text Content-Based Detection Methods\\nIn the event of rumor spreading, it is often dependent on\\nspeciﬁc forms of social media, such as text, images, and video\\nscreens. These media-speciﬁc forms mainly include textual fea-\\ntures [7] and visual features. This literature mainly discusses\\nthe description of textual content from the linguistic level, such\\nas sentences, vocabulary, and semantics. After these contents\\nare cleaned, deduplicated, and other operations, the potential\\ntext features can be obtained using text embedding techniques\\nto represent sentences as vectors, which are used as inputs to\\nneural networks. For example, Singh et al. [16] proposed an\\nattention-based LSTM network that uses LSTM to model tex-\\ntual content and subsequently uses the text with several different\\nlanguage and user features to distinguish rumors. Ma et al. [11]'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 1}, page_content='attention-based LSTM network that uses LSTM to model tex-\\ntual content and subsequently uses the text with several different\\nlanguage and user features to distinguish rumors. Ma et al. [11]\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='7660\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nused data from recurrent neural time series for modeling, using\\nan RNN network that can learn the feature of the sequential\\ndata to capture the features of rumor events over time. Ma et al.\\n[17] proposed a generative adversarial network (GAN)-based\\nmodel to obtain a feature representation of fake news, which\\nis based on a GRU-based generator to generate controversial\\ninstances, and in turn, designed an RNN-based discriminator to\\nidentify instances. Cheng et al. [18] mainly proposed a GAN-\\nbased hierarchical framework for text-level rumor detection and\\nprovided solutions for interpretation and gene classiﬁcation.\\nHowever, most content-based detection methods rely only on a\\nsingle feature, or content, and do not take into account the rela-\\ntionships and behaviors in a rumor-spreading event. Therefore,\\nthis article considers the integration of text-related work with'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='single feature, or content, and do not take into account the rela-\\ntionships and behaviors in a rumor-spreading event. Therefore,\\nthis article considers the integration of text-related work with\\nother work in the form of multitasking to learn multifaceted\\nfeatures for prediction.\\nB. Propagation Structure-Based Detection Methods\\nIn a large number of deep learning tasks, rumor propagation\\nevents often imply temporal and structural features. The pro-\\ncess of rumor events is not static; it is a historical collection\\nof information that is constantly changing over time, and the\\nstructure of propagation is also constantly changing. There is a\\nslight difference between the propagation pattern of true news\\nand the propagation process of false news: for example, true\\nnews tends to propagate more slowly [19], and the content of\\ntrue news is richer and usually related but not identical content\\ncan appear in multiple posts; while the propagation speed of'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='news tends to propagate more slowly [19], and the content of\\ntrue news is richer and usually related but not identical content\\ncan appear in multiple posts; while the propagation speed of\\nfalse news tends to be exploded growth in a short time, and the\\npropagation content tends to be a few limited and roughly the\\nsame words and pictures, with a more scanty degree of content.\\nThe richness of the content is relatively scarce. Based on the\\nabove features, rumors can be analyzed from the perspectives of\\ntime and dissemination structure and then distinguished. Some\\napproaches have explored studies based on the structural fea-\\ntures (e.g., propagation features) of social networks [20]. Zhang\\net al. [21] proposed a lightweight propagation path aggregation\\nneural network, where they modeled the propagation structure\\nof each rumor as a set of independent propagation paths for\\nrumor embedding and classiﬁcation. For early detection of ru-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='neural network, where they modeled the propagation structure\\nof each rumor as a set of independent propagation paths for\\nrumor embedding and classiﬁcation. For early detection of ru-\\nmor events, Silva et al. [22] proposed training an autoencoder to\\nlearn the embedding of the entire propagation network, which\\nthey demonstrated would give better results for early rumor\\ndetection. Vaibhav et al. [6] modeled each article in the dataset\\nas a graph and formulated the fake news detection task as a\\ngraph classiﬁcation task, where the nodes of the graph come to\\nrepresent the sentences of the article and the edges between the\\nnodes represent the semantic similarity between the sentences.\\nZhang et al. [23] constructed a propagation graph by tracking\\nthe propagation structure of posts and proposed an algorithm\\nbased on gated graph neural networks to generate a robust rep-\\nresentation for each node for rumor detection. Yang et al. [24]\\nexplored the rumor problem from an adversarial perspective'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='based on gated graph neural networks to generate a robust rep-\\nresentation for each node for rumor detection. Yang et al. [24]\\nexplored the rumor problem from an adversarial perspective\\nbased on graphs to extract more unique structural features for\\nbetter rumor detection by dynamically generating perturbations\\non heterogeneous social graphs with domain constraints. Bian\\net al. [3] proposed a novel bidirectional graphical model (Bi-\\nGCN) that explores these two features by running on top–down\\nand bottom–up propagation of rumors.\\nThe graph neural network effectively captures the global\\nstructural features learned during the propagation of a rumor\\nevent; however, the propagation graph of many applications\\nchanges over time during the development of real social media.\\nTraditional graph neural networks have limited attention to tem-\\nporal features, and numerous studies on dynamic graph neural\\nnetworks have emerged to be able to capture both temporal and'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='Traditional graph neural networks have limited attention to tem-\\nporal features, and numerous studies on dynamic graph neural\\nnetworks have emerged to be able to capture both temporal and\\nspatial structural features. Instead of learning with a static net-\\nwork, Song et al. [8] proposed a dynamic graph-based detection\\nframework to simulate the event evolution of real-world news.\\nSun et al. [10] investigated a dynamic propagation graph-based\\nnews detection problem, where they utilized a structure-aware\\nmodule and an event-aware module to capture temporal and\\nnetwork structure information, respectively. Huang et al. [25],\\nalthough they did not construct a graph structure to deal with the\\nproblem, proposed a spatiotemporal structured neural network,\\nwhich considers the spatial and temporal structures as a whole,\\nto model the news propagation for rumor detection, taking full\\nadvantage of the temporal features that cannot be captured'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='which considers the spatial and temporal structures as a whole,\\nto model the news propagation for rumor detection, taking full\\nadvantage of the temporal features that cannot be captured\\nby traditional networks. A dual dynamic graph convolutional\\nnetwork was investigated in the literature [10], modeling spatial,\\ntemporal, and textual features in a single architecture, passing\\nmultiple fused messages to subsequent network units in a se-\\nquential manner.\\nNumerous studies have shown that dynamic graph neural\\nnetworks possess more powerful capabilities compared to static\\ngraph networks, which not only capture the relational features\\nof the propagated structure but also the dynamic structural\\nfeatures of the temporal sequence by constructing static graphs\\nin the form of different temporal sequences.\\nTherefore, in this article, we choose a dynamic graph neural\\nnetwork as one of the baseline models. We try to construct a\\ntime-step-based ﬁne-grained dynamic graph neural network.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='Therefore, in this article, we choose a dynamic graph neural\\nnetwork as one of the baseline models. We try to construct a\\ntime-step-based ﬁne-grained dynamic graph neural network.\\nC. Multitask-Based Detection Methods\\nMultitask learning refers to joint learning behaviors that can\\nshare information about multiple related but not identical tasks\\n[26]. Auxiliary tasks can support the primary task in learning\\ncertain features that are easily overlooked or difﬁcult to learn\\nby the primary task itself, which makes multitask learning par-\\nticularly important when certain potential, but more valuable,\\nfeatures are not well utilized by the primary task. Collobert and\\nWeston [27] described a single convolutional neural network\\narchitecture that, given a sentence outputting multiple differ-\\nent predictions, shares the parameters of multitasks across the\\nnetwork for joint training. Kochkina et al. [15] constructed a\\nmultitask learning architecture consisting of three tasks dealing'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 2}, page_content='ent predictions, shares the parameters of multitasks across the\\nnetwork for joint training. Kochkina et al. [15] constructed a\\nmultitask learning architecture consisting of three tasks dealing\\nwith truthfulness classiﬁcation, stance classiﬁcation, and rumor\\ndetection tasks, and in a concrete implementation, only one\\nset of hidden layers was deﬁned, and the same hidden layers\\nwere used for different tasks, allowing them to achieve hard\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 3}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7661\\nFig. 1.\\nDynamic network-based multitask detection framework.\\nparameter sharing. Wan et al. [14] jointly performed rumor\\ndetection and stance classiﬁcation by using two RNN-based\\nnetwork architectures built with shared layers. Wu et al. [28]\\nexplored shared layers with gating and attention mechanisms\\nthat can selectively capture valuable shared features for fake\\nnews detection and stance classiﬁcation. Bai et al. [29] proposed\\na multitask attention tree neural network (MATNN) speciﬁcally\\ndesigned to provide a structured representation of rumor conver-\\nsations and exploited the attention mechanism to jointly classify\\nstances and detect rumor veracity. Cheng et al. [30] proposed a\\nmultitask variational autoencoder-assisted rumor classiﬁcation\\nsystem consisting of four components: rumor detection, rumor\\ntracking, stance classiﬁcation, and veracity classiﬁcation, where'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 3}, page_content='multitask variational autoencoder-assisted rumor classiﬁcation\\nsystem consisting of four components: rumor detection, rumor\\ntracking, stance classiﬁcation, and veracity classiﬁcation, where\\na more appropriate classiﬁcation technique and training engine\\nimproves the performance and generalization of the model.\\nHowever, most of the existing multitask learning methods\\nrealize sharing only by deﬁning a set of RNNs as a shared\\nlayer and using hard parameter sharing to interact with other\\nintertask features, which, although it realizes parameter sharing\\nto a certain extent and can improve the model performance, the\\nfeature sharing among multitasks is neglected.\\nTherefore, in this article, based on the multitask sharing\\nlayer, we constructed a shared network that can share multitask\\nfeatures as well as hard parameters simultaneously at the time\\nof training.\\nIII. DGCN-TES MODEL\\nIn this article, we propose a dynamic network-based multi-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 3}, page_content='features as well as hard parameters simultaneously at the time\\nof training.\\nIII. DGCN-TES MODEL\\nIn this article, we propose a dynamic network-based multi-\\ntask detection model (DGCN-TES), which performs multitask-\\ning interaction for rumor detection in a dynamic perspective. It\\nis mainly divided into four modules: dynamic-GCN, content-\\nLSTM, temporal event sharing layer (hereinafter referred to\\nas TES), and fusion and classiﬁcation module, as shown in\\nFig. 1. First, the propagation graphs of rumor events at different\\nmoments are constructed, and they are modeled by using the\\ndynamic graph convolutional network to capture the rumor\\nevents’ dynamic structural features; second, the temporal share\\nlayer is used as a shared network combining dynamic-GCN and\\ncontent-LSTM to interact with features at each identical time\\nstep; and ﬁnally, the attention mechanism is used to focus on\\nthe information that has an important impact on different tasks'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 3}, page_content='content-LSTM to interact with features at each identical time\\nstep; and ﬁnally, the attention mechanism is used to focus on\\nthe information that has an important impact on different tasks\\nand between different moments, which is used as the input to\\nthe classiﬁer to make predictions.\\nCompared to other related work [8], [10] on dynamic graphs,\\nalthough they also consider dynamic graph neural networks to\\ndeal with the dynamic correlation patterns of rumor propagation\\nevents, the former does not yet adequately consider temporal\\ngranularity, and the latter neglects the dynamic interaction of\\nmultitasks. In contrast, the computational process of DGCN-\\nTES proposed in this article follows a temporal event-sharing\\nstrategy based on time-step ﬁne granularity, and this design cre-\\nates a different snapshot of the dynamic graph at each moment,\\nwhich not only improves the efﬁciency of multitask sharing\\nbut also makes the dynamic graphs smoother in terms of the\\ntemporal dimensions of the excess.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 3}, page_content='which not only improves the efﬁciency of multitask sharing\\nbut also makes the dynamic graphs smoother in terms of the\\ntemporal dimensions of the excess.\\nA. Problems and Symbolic Deﬁnitions\\nThe rumor detection task is deﬁned as a binary classiﬁcation\\nproblem. Let ε = ε1, ..., εn be a set of instances of rumor detec-\\ntion events, εi is the ith event, and n is the number of propaga-\\ntion events to be detected in the dataset. Denote the set of posts\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='7662\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nTABLE I\\nIMPORTANT NOTATIONS AND DESCRIPTION\\nNotations\\nDescriptions\\nεi\\nThe ith event\\nεc\\ni\\nThe set of tweets in the ith event\\nγ\\nThe time step of different tweets in the ith event\\nVir, Eir\\nNodes and edges in the ith event at the moment γ\\nGir =< Vir, Eir >\\nPropagation graph for the γ moment in the ith event\\nFig. 2.\\nCoarse-grained dynamic graphs based on time steps.\\nand comment contents as εc\\ni = {si, ci1, ..., cimi−1}, si is the\\nsource tweet, cij is the jth response text, and mi is the number\\nof posts in the event εi. Considering the source tweet as the 0th\\ntweet for presentation simply. Then, εc\\ni = {si, ci1, ..., cimi−1}\\ncan be expressed as εc\\ni = {ci0, ci1, ..., cimi−1}. By expressing\\nthe propagation order of each post in a propagation event εi in\\nterms of time as εt\\ni = {ti0, ti1, ..., timi−1}, with t denoting the\\ntime, the ﬁnal rumor propagation event εi can be expressed as'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='the propagation order of each post in a propagation event εi in\\nterms of time as εt\\ni = {ti0, ti1, ..., timi−1}, with t denoting the\\ntime, the ﬁnal rumor propagation event εi can be expressed as\\nεi = {(ci0, ti0), (ci1, ti1), ..., (cimi−1, timi−1)}.\\nDivide each post εi in a propagation event into γ time steps\\nalong the time dimension, γ is determined by the number of\\nposts mi in each propagation event, γϵ{1, 2, ..., γ}, for a prop-\\nagation event εi different propagation states at different time\\nsteps γ. In the ﬁrst time step εi0 = {(ci0, ti0)}, in the second\\ntime step εi0 = {(ci0, ti0), (ci1, ti1)}, so that the event εi is\\nﬁnally represented at all time steps as εi = {εi0, εi1, ..., εiγ}.\\nFor the same propagation event εi at different time steps,\\ndifferent propagation structure graphs Gir =< Vir, Eir > are\\nconstructed. For easy understanding, the important mathemat-\\nical notations are listed in Table I.\\nB. Dynamic-GCN Module\\n1) Construction of Dynamic Graphs: The idea of construct-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='constructed. For easy understanding, the important mathemat-\\nical notations are listed in Table I.\\nB. Dynamic-GCN Module\\n1) Construction of Dynamic Graphs: The idea of construct-\\ning the dynamic graph is inspired by Sun et al. [10], and we\\nconstruct a time-step-based rumor propagation graph, such as\\nthe dynamic-GCN part in Fig. 1. The propagation of rumor\\nevents develops dynamically according to time, and for the\\nsame propagation event, each time step is a network of rumor\\npropagation. With the development of time, the number of these\\nnetworks increases dynamically, eventually forming a dynamic\\npropagation network. According to the rule that the propaga-\\ntion network changes dynamically over time, a corresponding\\npropagation graph is constructed at each time step, where nodes\\ndenote different posts and edges denote the forwarding and\\nreplying relationships between these posts.\\nThis section gives the deﬁnition of a ﬁne-grained dynamic'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='denote different posts and edges denote the forwarding and\\nreplying relationships between these posts.\\nThis section gives the deﬁnition of a ﬁne-grained dynamic\\ngraph based on time steps: a time step that propagates a ﬁne-\\ngrained dynamic pattern of only one node. This is done as\\nfollows.\\nFig. 3.\\nFine-grained dynamic graphs based on time steps.\\nAs shown in Fig. 2, for the propagation event εi: at the\\nmoment T0, only one source post exists (node 0); at moment\\nT1, two response tweets interact with the source post (nodes 1\\nand 2); at moment T2, only one tweet responds to the source\\npost (node 3); and at moment T3, nodes 4 and 5 respond to\\nnode 1 again simultaneously. A coarse-grained dynamic graph\\nstate, as shown in Fig. 2, was initially obtained based on this\\nrelationship, but some of these moments propagated more than\\none node at the same time, and a ﬁne-grained segmentation is\\nnow required.\\nIn the subsequent computation, in order for the dynamic'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='relationship, but some of these moments propagated more than\\none node at the same time, and a ﬁne-grained segmentation is\\nnow required.\\nIn the subsequent computation, in order for the dynamic\\ngraph network to be able to interact with features based on\\ntime steps in the TES, multiple nodes propagated in the same\\nmoment are sorted, resulting in a moment where only one\\nnode’s state is propagated, as shown in Fig. 3. For nodes 1 and\\n2, which are propagated at the same time in the moment of t1,\\nalthough their creation times are the same, they are sorted based\\non the numbering order of each node. Finally, based on the\\nabove node propagation rules and their response and forwarding\\nnode numbers, the respective static graphs are constructed at\\ndifferent time steps, and a ﬁne-grained dynamic graph sequence\\nbased on the time steps is constructed for the dynamic graph\\nconvolution operation in the next section.\\n2) Dynamic-GCN: This section describes in detail the com-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='based on the time steps is constructed for the dynamic graph\\nconvolution operation in the next section.\\n2) Dynamic-GCN: This section describes in detail the com-\\nputation of ﬁne-grained dynamic graphs based on time steps.\\nFor the propagation event, εi = {εi0, εi1, ..., εiγ} construct\\nsnapshots of the propagation graph at different moments Gir =\\nGi0, ..., Gir. Hp\\nir ∈Rmir∗F is the feature matrix containing all\\nthe nodes of the propagation graph Gir, mir is the number of\\nnodes of the propagation graph at that moment, and F denotes\\nthe features vector of the node. Its adjacency matrix is deﬁned\\nas Air ∈Rmir∗F . As in (1) and (2), the dynamic graph convo-\\nlution layer is deﬁned as\\nHl+1\\nir\\n= σ( ˆD−1\\n2 ˆA ˆD−1\\n2 Hl\\nirW l)\\n(1)\\nwhere ˆD is the matrix, ˆA is the adjacency matrix, Hl\\nir is the\\nfeature matrix of all nodes of the previous layer of the propa-\\ngation graph Gir, W l is the weight matrix, and σ is the nonlinear\\nactivation function.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 4}, page_content='ir is the\\nfeature matrix of all nodes of the previous layer of the propa-\\ngation graph Gir, W l is the weight matrix, and σ is the nonlinear\\nactivation function.\\nThe operation in (1) is performed on the propagation graph\\nGir = {Gi0, Gi1, ..., Gir} for all time steps to obtain the spatial\\nfeatures of all time steps\\nHir = {Hi0, Hi1, ..., Hir}.\\n(2)\\nAt this point, the propagation structure feature of the same\\npropagation event at each moment has been obtained, form-\\ning a dynamic feature sequence, which is taken as the input\\nto the temporal event sharing layer (described in detail in\\nSection III-D), at the same time, the dynamic structure fea-\\ntures Hir of the last moment is taken as the ﬁnal output of\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7663\\nthe dynamic-GCN module, which is used as the input to the\\nclassiﬁcation network (described in detail in Section III-E).\\nC. Content-LSTM Module\\nIn the previous section, after the ﬁne-grained time-step-based\\ndynamic graph convolution computation, the dynamic struc-\\ntural feature {Hi0, Hi1, ..., Hir}, which is a time series-based\\nembedding of structural features, the difference between neigh-\\nboring time-steps lies in the generation of a certain node (post),\\nwhich are arranged in a strict temporal order. To make this\\ndynamic feature of dynamic graphs related to content-LSTM\\nin the development of the time step to meet the requirement of\\ncombining them based on the time step in TES. In this section,\\nthe node diffusion order in Section III-B is used as the arrange-\\nment order of the textual content of the corresponding rumor\\nevents, and the textual content of the same order is modeled'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='the node diffusion order in Section III-B is used as the arrange-\\nment order of the textual content of the corresponding rumor\\nevents, and the textual content of the same order is modeled\\nto form a set of time-step-based textual feature sequences. It is\\nused for subsequent computation steps.\\nInspired by Wan et al. [14], we use an LSTM network as a\\nbaseline model. For the propagation event εi, the entire tweets\\ncontained are denoted as Xi_content = {Xi_0, Xi_1, ..., Xi_n} ∈\\nRn∗300, which is used as the input of content-LSTM, as in the\\nfollowing equations:\\nit = σ(WiiXi_content_t + bit + Whih(t−t) + bhi)\\n(3)\\nft = σ(WifXi_content_t + bif + Whfh(t−1) + bhf)\\n(4)\\ngt = tanh(WigXi_content_t + big + Whgh(t−1) + bhg)\\n(5)\\not = σ(WioXi_content_t + bio + Whoh(t−1) + bho)\\n(6)\\nct = ft ∗ct−+ it ∗gt\\n(7)\\nhcontent −t = ot ∗tanh(ct)\\n(8)\\nwhere Xi_content_t is the input feature vector at a given moment,\\nh(t−1) is the hidden state at the previous moment, and W∗and'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='(6)\\nct = ft ∗ct−+ it ∗gt\\n(7)\\nhcontent −t = ot ∗tanh(ct)\\n(8)\\nwhere Xi_content_t is the input feature vector at a given moment,\\nh(t−1) is the hidden state at the previous moment, and W∗and\\nb∗are the corresponding weight matrix and bias.\\nAt this point, the same propagation event at each moment of\\nthe text feature sequence, formed a temporal feature sequence\\nHcontent, as in (9), notated as\\nhcontent = {hcontent−0, hcontent−1, ..., hcontent−t}.\\n(9)\\nNote that the time series feature corresponds one-to-one with\\nthe dynamic structure sequence feature Hi in (2) in terms of the\\norder of occurrence of the propagation events. This time series\\nfeature is taken as the input of the temporal event sharing layer\\n(described in detail in Section III-D); meanwhile, the sequence\\nfeature Hcontent of the last moment is taken as the ﬁnal output of\\ncontent-LSTM, which is used as the input of the classiﬁcation\\nnetwork (described in detail in Section III-E).\\nD. Temporal Event Share Layer'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='content-LSTM, which is used as the input of the classiﬁcation\\nnetwork (described in detail in Section III-E).\\nD. Temporal Event Share Layer\\nIn the above two sections, the dynamic structural feature Hi\\nand the time-series feature Hcontent are obtained based on the\\nrule that the propagation network develops dynamically over\\ntime, as well as the temporal order in which the nodes are\\nFig. 4.\\nStructure of TES.\\ngenerated, respectively. They are related in the development\\nsequence of propagation events; in other words, for a certain\\npropagation event at the same time stage, we capture its tem-\\nporal and spatial feature sequences, and their temporality and\\ndynamics are consistent in the postdevelopment process.\\nIn this section, as shown in the middle part of Fig. 1,\\na temporal event-sharing layer is proposed, which combines\\ndynamic-GCN and content-LSTM with time step as the basic\\ncomputational unit and shares their features at the same time'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='a temporal event-sharing layer is proposed, which combines\\ndynamic-GCN and content-LSTM with time step as the basic\\ncomputational unit and shares their features at the same time\\nstage interactively and trains them together. Compared to other\\nmultitask learning using hardware parameter sharing [15] as\\nan approach, we additionally implement temporal event shar-\\ning, which improves the sharing frequency and efﬁciency of\\ndynamic-GCN and content-LSTM. The speciﬁc computation\\nsteps are as follows.\\nStep 1: As in Fig. 4, at the moment t0, the feature Hi0−v ∈\\nR1∗F of the node(Node_0) corresponding to the one that\\nis being propagated in the dynamic structural feature Hi0 ∈\\nRnodes∗F in Section III-B is taken, and then the time-series\\nfeature Hcontent−0 in Section III-C is taken. Now, the propaga-\\ntion structure features and content features of the corresponding\\ntweets with the number 0 are taken at the same time.\\nStep 2: Splice them and go through the linear layer mapping'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='tion structure features and content features of the corresponding\\ntweets with the number 0 are taken at the same time.\\nStep 2: Splice them and go through the linear layer mapping\\nas the input of the cell block at the moment t0 in the TES, and\\ngo through the gating calculation to get the output of the ﬁrst\\ntime step, as in the following equations:\\nXshare−0 = Linear(Concat(Hi0, Hcontent−0))\\n(10)\\nit = σ(WiiXshare−0 + bit + Whth(t−1) + bhi)\\n(11)\\nft = σ(WifXshare−0 + bif + Whfh(t−1) + bhf)\\n(12)\\ngt = tanh(WigXshare−0 + big + Whgh(t−1) + bhg) (13)\\not = σ(WioXshare−0 + bio + Whoh(t−1) + bho)\\n(14)\\nct = ft ∗ct−1 + it ∗gt\\n(15)\\nHshare−0 = ot ∗tanh(ct).\\n(16)\\nIn the above equation, Xshare−0 is the input of the temporal\\nevent sharing layer at the moment t0, W∗and b∗are the weight\\nmatrix and bias, and h(t−1) is the hidden layer vector of the\\nprevious moment, which is randomly initialized and generated\\nat the moment t0, and Hshare−0 is the output.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 5}, page_content='matrix and bias, and h(t−1) is the hidden layer vector of the\\nprevious moment, which is randomly initialized and generated\\nat the moment t0, and Hshare−0 is the output.\\nAt this point, a similar operation is carried out in the ﬁrst cell\\nblock of the content-LSTM, with the difference that the input\\nis a representation of the text content at the moment t0, and the\\noutput is the time series feature hcontent−0 at the moment t0.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 6}, page_content='7664\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nStep 3: Splice hcontent−0 and Hshare−0 and undergo linear\\nmapping to update the hidden state of content-LSTM at the\\nmoment of t0 as the hidden state of the content-LSTM cell block\\nat the next moment, as in\\nhcontent−0 = Linear(Concat(hcontent−0, Hshare−0)).\\n(17)\\nThe computation of the content-LSTM for the next moment\\nis referred to in (3)–(8).\\nStep 4: Repeat the ﬁrst step to the third step until all the time\\nsteps have been looped to ﬁnally get the output of each moment\\nof TES, as in\\nHshare = {Hshare−0, Hshare−1, ..., Hshare−t}.\\n(18)\\nNote that the TES is continuously interacting with the infor-\\nmation in dynamic-GCN and content-LSTM in chronological\\norder during the computation process, thus realizing temporal\\nevent sharing.\\nE. Attentional Mechanisms and Classiﬁer\\nIn the previous section, the computation process has been\\ncompleted and the feature representation after feature shar-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 6}, page_content='event sharing.\\nE. Attentional Mechanisms and Classiﬁer\\nIn the previous section, the computation process has been\\ncompleted and the feature representation after feature shar-\\ning has been obtained through TES, which are dynamic-GCN,\\ncontent-LSTM, and TES layers: Hir, Hcontent, and Hshare.\\nTo understand which features are more important for pre-\\ndiction across different time series and across different tasks,\\nthis section updates them using a multihead attention mecha-\\nnism [31]. Rumor detection efforts in LSTM networks typically\\nuse the latent feature representation of the ﬁnal time series as\\nthe predicted feature vector [14], since the sequence data are\\ncomputed through a gate mechanism, and the output of the last\\ngate also contains all of the previous gating computed hidden\\nstates. Therefore, we also use the features of the last time series\\nfor computation. As in the right part of Fig. 1, we obtain the\\nconvolutional features Hit of the dynamic graph convolutional'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 6}, page_content='states. Therefore, we also use the features of the last time series\\nfor computation. As in the right part of Fig. 1, we obtain the\\nconvolutional features Hit of the dynamic graph convolutional\\nnetwork at the ﬁnal moment for subsequent computation. The\\ndetails of the implementation are as follows.\\nIn the ﬁrst step, Hshare−t is spliced with Hcontent−t and Hi−t,\\nrespectively, and mapped by a linear layer to obtain the inputs\\nX1 and X2 of the attention layer as in the following equations:\\nX1 = Linear(Concat(Hcontent−t, Hshare−t))\\n(19)\\nX2 = Linear(Concat(Hi−t, Hshare−t)).\\n(20)\\nIn the second step, X1 and X2 are linearly transformed into\\nthree projections with the same dimensions as itself (query Q;\\nkey K; and value V), which are used as inputs to the attention\\nlayer, as in the following equations:\\nMultiHead(Q, K, V ) = Concat(head1, ..., headn)W o\\n(21)\\nheadi = Attention(XW Q\\ni , XW K\\ni , XW V\\ni ) (22)\\nAttention(O, K, V ) = softmax\\n\\x02QKT\\n√dk\\nV\\n\\x03\\n.\\n(23)'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 6}, page_content='layer, as in the following equations:\\nMultiHead(Q, K, V ) = Concat(head1, ..., headn)W o\\n(21)\\nheadi = Attention(XW Q\\ni , XW K\\ni , XW V\\ni ) (22)\\nAttention(O, K, V ) = softmax\\n\\x02QKT\\n√dk\\nV\\n\\x03\\n.\\n(23)\\nMultiHead(Q, K, V ) is the output of the attention layer, Q,\\nK, and V denote XW Q\\ni , XW K\\ni , XW V\\ni , and W o is the weight\\nTABLE II\\nSTATISTICS OF DATASETS\\nStatistic\\nWEIBO\\nPHEME\\nPosts/events\\n4663\\n5473\\nNon −rumors\\n2312\\n1996\\nRumors\\n2351\\n3477\\nAvg posts/event\\n452\\n20\\nMax posts/event\\n44 764\\n346\\nMin posts/event\\n2\\n3\\nmatrix, Wi is the initialization of the different multimatrices,\\ndk is the dimension of K, scaling the output when K is larger.\\nIn the third step, the outputs of the attention layer are spliced\\nwith Hshare−t and Hi−t, and go through a linear mapping, which\\nis used to update them, as in the following equations:\\nHcontent−t = Linear(Concat(Hshare−t, Hcontent−t))\\n(24)\\nHi−t = Linear(Concat(Hshare−t, Hi−t)).\\n(25)\\nIn the fourth step, the updated, ﬁnal event feature represen-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 6}, page_content='Hcontent−t = Linear(Concat(Hshare−t, Hcontent−t))\\n(24)\\nHi−t = Linear(Concat(Hshare−t, Hi−t)).\\n(25)\\nIn the fourth step, the updated, ﬁnal event feature represen-\\ntations Hcontent−t,Hi−t for prediction are obtained, and the cor-\\nresponding fully connected and activation layers are designed\\nfor them, respectively, as in the following equations:\\nYc = softmax(WcHcontent−t + bc)\\n(26)\\nYDG = softmax(WDGHi−t + bDG).\\n(27)\\nW∗and b∗are their respective corresponding weights and bias\\nparameters, and ﬁnally, the cross entropy loss function is used\\nas the classiﬁcation loss as in the following equations:\\nLc = −\\n\\x04\\ni\\nYilogYC\\n(28)\\nLDG = −\\n\\x04\\ni\\nYilogYDG.\\n(29)\\nYi is the true label of the ith event. For training, the two loss\\nfunctions are assigned corresponding coefﬁcients and summed\\nto ﬁnd the total loss function for backpropagation as in (30).\\nW1 and W2 represent the parameters of the two tasks\\nLoss = W1 ∗LC + W2 ∗LDG.\\n(30)\\nIV. EXPERIMENTS\\nThis section evaluates the superiority of our method by com-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 6}, page_content='W1 and W2 represent the parameters of the two tasks\\nLoss = W1 ∗LC + W2 ∗LDG.\\n(30)\\nIV. EXPERIMENTS\\nThis section evaluates the superiority of our method by com-\\nparing the explored method with other methods based on several\\npublicly available datasets. In addition, ablation experiments\\nand other complementary experiments are performed to illus-\\ntrate the effectiveness of the various components of our method.\\nA. Datasets\\nTo investigate the content and propagation patterns of fake\\nnews, the WEIBO dataset [5] and PHEME dataset [15] are\\nselected. After removing some samples, the dataset is shown\\nin Table II, which contains both false and true information\\nfrom both sites. Each event contains several response tweets\\nwith comments and retweets, and all have a label, rumor, or\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 7}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7665\\nnonrumor. For the WEIBO dataset, due to the large average\\nnumber of posts per rumor event, the WEIBO dataset is divided\\ninto training, validation, and testing sets in the ratio of 6:2:2,\\nand for the PHEME dataset in the ratio of 8:1:1, taking into\\naccount the balance of the sample distribution. To compare\\nthe detection results with other models to validate the effec-\\ntiveness of the model, the experiments used four evaluation\\nmetrics, namely, accuracy, precision, recall, and F1 score. The\\nF1 score was used to evaluate the performance of the model.\\nThe calculation formulas are shown in the following equations,\\nrespectively, where TP denotes a true-positive rate, TN denotes\\na true-negative rate, FP denotes a false-positive rate, and FN\\ndenotes a false-negative rate:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN\\n(31)\\nPrecision =\\nTP\\nTP + FP\\n(32)\\nRecall =\\nTP\\nTP + FN\\n(33)'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 7}, page_content='a true-negative rate, FP denotes a false-positive rate, and FN\\ndenotes a false-negative rate:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN\\n(31)\\nPrecision =\\nTP\\nTP + FP\\n(32)\\nRecall =\\nTP\\nTP + FN\\n(33)\\nF1 = 2 ∗Precision ∗Recall\\nPrecision + Recall .\\n(34)\\nB. Comparison Methods\\nWe compare with the following baselines.\\nDTC [32]: a rumor detection method using Decision Tree\\nclassiﬁer with hand-crafted features.\\nSVM-RBF [33]: an SVM model with RBF kernel using\\nhandcrafted features based on the overall statistics of the posts\\nfor classiﬁcation.\\nSVM-TS [34]: A time-series structure-based SVM classiﬁer\\nthat extracts features by modeling the content, users, and prop-\\nagation patterns of rumors as well as their impact on society.\\nRvNN [5]: a method that uses a tree-structured recursive\\nneural network to extract the high-level representation from\\nboth text contents and propagation structures, which reinforces\\nor weakens the stance of the node by judging whether to respond\\nto the node.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 7}, page_content='both text contents and propagation structures, which reinforces\\nor weakens the stance of the node by judging whether to respond\\nto the node.\\nTGNF [9]: a temporally evolving graph neural network\\nmodel for fake news detection fusing interactive and temporal\\nfeatures.\\nBi-GCN [3]: a GCN-based rumor detection model utilizing\\nthe bidirectional propagation structure to obtain the propagation\\nand dispersion features.\\nSTS-NN [25]: a spatial–temporal structure neural network to\\ntreat the spatial structure and the temporal structure as a whole\\nto model the message propagation for rumor detection.\\nDDGCN [10]: a dual-dynamic graph convolutional network\\nto model the spatial structure, temporal structure, external\\nknowledge, and text information in one uniﬁed framework.\\nC. Parameter Settings\\nFor all comparison methods, the default settings from the\\ncorresponding articles were used. All hyperparameters are spec-\\niﬁed in Table III. For the methods in this study, the parameters\\nTABLE III'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 7}, page_content='For all comparison methods, the default settings from the\\ncorresponding articles were used. All hyperparameters are spec-\\niﬁed in Table III. For the methods in this study, the parameters\\nTABLE III\\nMAIN HYPERPARAMETER SETTINGS\\nDatasets\\nHyperparameter\\nPHEME\\nAverage subgraph nodes\\n20\\nMax subgraph nodes\\n346\\nTime steps\\n20\\nHidden Dim\\n128\\nBatch size\\n128\\nWEIBO\\nAverage subgraph nodes\\n40\\nMax subgraph nodes\\n44 764\\nTime steps\\n452\\nHidden Dim\\n128\\nBatch size\\n32\\nTABLE IV\\nRUMOR DETECTION RESULTS ON THE WEIBO DATASET\\nMethod\\nAcc\\nPre\\nRecall\\nF1\\nDTC\\n0.858\\n0.834\\n0.822\\n0.857\\nSVM-RBF*\\n0.899\\n0.938\\n0.846\\n0.889\\nSVM-TS\\n0.885\\n0.950\\n0.932\\n0.938\\nRvNNBU(ACL18)\\n0.908\\n0.912\\n0.897\\n0.905\\nSTS-NN\\n0.913\\n0.902\\n0.898\\n0.900\\nBi-GCN(AAAI20)\\n0.919\\n0.918\\n0.916\\n0.913\\nTGNF(IPM21)*\\n0.968\\n0.974\\n0.960\\n0.967\\nDDGCN(AAAI22)*\\n0.948\\n0.953\\n0.948\\n0.950\\nours\\n0.974\\n0.978\\n0.970\\n0.975\\nNote: The bold entries are the relative best values.\\nTABLE V\\nRUMOR DETECTION RESULTS ON THE PHEME DATASET\\nMethod\\nAcc\\nPre\\nRecall\\nF1\\nDTC\\n0.581\\n0.659\\n0.652\\n0.656'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 7}, page_content='0.948\\n0.950\\nours\\n0.974\\n0.978\\n0.970\\n0.975\\nNote: The bold entries are the relative best values.\\nTABLE V\\nRUMOR DETECTION RESULTS ON THE PHEME DATASET\\nMethod\\nAcc\\nPre\\nRecall\\nF1\\nDTC\\n0.581\\n0.659\\n0.652\\n0.656\\nSVM-RBF\\n0.602\\n0.875\\n0.431\\n0.577\\nSVM-TS\\n0.651\\n0.642\\n0.686\\n0.663\\nRvNNBU(ACL18)\\n0.789\\n0788\\n0.788\\n0.789\\nSTS-NN\\n0.819\\n0.816\\n0.791\\n0.800\\nBi-GCN(AAAI20)\\n0.847\\n0.840\\n0.834\\n0.835\\nTGNF(IPM21)*\\n0.848\\n0.892\\n0.861\\n0.877\\nDDGCN(AAAI22)*\\n0.855\\n0.846\\n0.841\\n0.844\\nours\\n0.869\\n0.878\\n0.877\\n0.877\\nNote: The bold entries are the relative best values.\\nwere optimized using the Adma algorithm. The learning rate is\\n1e-3, and the batch sizes are 32 and 128. For the embedding\\nrepresentation, this article uses the spaCY open source AIP\\npretraining model TRANSFORMER to process each sentence\\nto get the embedding representation with 300 dimensions.\\nD. Experimental Results and Analyses\\nThe proposed model is compared with the baseline method\\nmentioned in Section IV-B, Tables IV and V show the per-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 7}, page_content='D. Experimental Results and Analyses\\nThe proposed model is compared with the baseline method\\nmentioned in Section IV-B, Tables IV and V show the per-\\nformance of the compared methods, where the ﬁrst group is\\nthe machine learning method and the second group is the\\ndeep learning approach. Fig. 5 also shows more visually the\\ncomparison of the proposed method in this article with other\\nbaselines using F1 scores. We ran ﬁve runs based on the default\\nsettings in the original paper and reported the average results\\nhere, “*” Experimental results were obtained directly from the\\noriginal paper.\\nFrom the above table, the following analysis can be obtained.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='7666\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nFig. 5.\\nComparison of F1-score in different baseline models.\\nThe machine learning methods (DTC, SVM-RBF, and SVM-\\nTS) have the worst performance scalar, none of which is bet-\\nter than the second group. This is due to the fact that the\\ninformation extracted by feature engineering is often a latent\\nrepresentation of the data and does not capture deeper feature\\ninformation, whereas deep learning is able to learn high-level\\nsemantic information about the data. However, a comparison\\nof the ﬁrst group of methods with each other shows that SVM-\\nTS outperforms the other two methods due to the development\\nof a dynamic series of time-structured models that are able\\nto explore the patterns of various social contextual features\\nover time, and feature design that incorporates social contextual\\ninformation.\\nIn the second group, the RvNN that utilizes the propaga-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='to explore the patterns of various social contextual features\\nover time, and feature design that incorporates social contextual\\ninformation.\\nIn the second group, the RvNN that utilizes the propaga-\\ntion tree model performs better than SVM-TS, the accuracy is\\n2.59% and 21.19% higher on WEIBO and PHEME datasets,\\nrespectively, this is due to the fact that it models the rumor\\nevents as a propagation tree, the leaf nodes will inﬂuence and\\nlearn each other’s information; however, the gap on the WEIBO\\ndataset is relatively close, which may be due to the fact that\\nthe WEIBO dataset is caused by the higher average number of\\nposts when the average number of posts of events is higher, the\\npropagation tree constructed is richer, and the structural features\\ncaptured are more obvious. STS-NN proposes a spatiotemporal\\nstructural neural network, which models the spatial and tem-\\nporal structure as a whole for message propagation for rumor'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='captured are more obvious. STS-NN proposes a spatiotemporal\\nstructural neural network, which models the spatial and tem-\\nporal structure as a whole for message propagation for rumor\\ndetection and does not construct a graph structure to capture the\\nspatial features, so its accuracy is 0.5% higher than that of the\\nRvNN by 0.4%, 3.8%, respectively, which is between RvNN\\nand Bi-GCN.\\nBi-GCN that utilizes the propagation graph structure has bet-\\nter performance, it outperforms RvNN and STS-NN by 1.2%,\\n7.35%, and 0.65%, 3.41% on the two datasets, respectively,\\nthanks to its use of static graph related algorithms. It learns the\\nembedding representation of graphs from a bidirectional point\\nof view, designing a bidirectional convolutional algorithm to\\nlearn better features. But it does not use dynamic ideas.\\nTABLE VI\\nRESULTS OF COMPARISON AMONG DIFFERENT VARIANTS\\nDatasets\\nMethod\\nacc\\npre\\nrecall\\nF1\\nWEIBO\\nW/o GCN\\n0.906\\n0.902\\n0.902\\n0.902\\nW/o TES\\n0.952\\n0.961\\n0.962\\n0.961\\nW/o dynamic\\n0.961\\n0.961\\n0.969'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='TABLE VI\\nRESULTS OF COMPARISON AMONG DIFFERENT VARIANTS\\nDatasets\\nMethod\\nacc\\npre\\nrecall\\nF1\\nWEIBO\\nW/o GCN\\n0.906\\n0.902\\n0.902\\n0.902\\nW/o TES\\n0.952\\n0.961\\n0.962\\n0.961\\nW/o dynamic\\n0.961\\n0.961\\n0.969\\n0.965\\nDGCN-TES\\n0.974\\n0.978\\n0.970\\n0.975\\nPHEME\\nW/o GCN\\n0.812\\n0.812\\n0.812\\n0.813\\nW/o TES\\n0.828\\n0.823\\n0.826\\n0.824\\nW/o dynamic\\n0.838\\n0.829\\n0.840\\n0.834\\nDGCN-TES\\n0.869\\n0.878\\n0.877\\n0.877\\nNote: The bold entries are the relative best values.\\nAlgorithms using dynamic ideas will have better results,\\nTGNF models the temporal interaction events of nodes from\\nthe dynamic evolution perspective, and the accuracy is 5.33%\\nand 0.1% higher than Bi-GCN, respectively. DDGCN is 0.82%\\nmore accurate than TGNF on the PHEME dataset due to a kind\\nof dynamic graph convolutional network.\\nOverall, DGCN-TES has the best performance in most of the\\nmetrics, and the ﬁne-grained time-step-based dynamic graph\\nnetwork we designed is able to capture dynamic sequence\\nfeatures with more delicate transitions, get stronger feature'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='metrics, and the ﬁne-grained time-step-based dynamic graph\\nnetwork we designed is able to capture dynamic sequence\\nfeatures with more delicate transitions, get stronger feature\\nrepresentations from interaction training in TES, and utilize\\nthe attention mechanism, which is able to act as ﬁltering and\\naccelerate model convergence, and thus outperforms Bi-GCN\\nby 5.98% and 2.59%, respectively, and DDGCN by 2.74%\\nand 1.63%, respectively, than Bi-GCN using the static graph\\nalgorithm.\\nThe method DGCN-TES proposed in this study achieves\\nbetter performance, thanks to the fact that we model and in-\\nteract with information about the dynamic spatial structure and\\ntemporal text content. Speciﬁcally, there are the following ad-\\nvantages that other methods do not do:\\n1) In this article, we constructed a time-step level, ﬁne-\\ngrained dynamic graph convolutional layer based on the\\ntime-step level, where a snapshot of the dynamic graph\\nis created for each extension of a node. More delicate'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='grained dynamic graph convolutional layer based on the\\ntime-step level, where a snapshot of the dynamic graph\\nis created for each extension of a node. More delicate\\nand ﬁne-grained features are captured in the dynamic\\nrelationship model.\\n2) Using the form of multitasking to combine the sequence\\ndata with the dynamic relationship, in the design, it is\\ndone that the time in the sequence data corresponds to\\nthe time in the dynamic relationship, dynamic-GCN and\\ncontent-LSTM are combined at the time-step level, which\\nmakes both of them more powerful and more effective in\\nsharing interaction.\\nE. Ablation Experiment\\nTo further investigate the impact of the individual parts in this\\nmethod, this section conducts ablation experiments using the\\ncomparison strategy shown in Table VI. The model is compared\\nto the network with each part removed.\\nThe following combinations are deﬁned to experimentally\\ndemonstrate the effect of each of our proposed components.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 8}, page_content='to the network with each part removed.\\nThe following combinations are deﬁned to experimentally\\ndemonstrate the effect of each of our proposed components.\\n1) W/o TES: When the model does not use TES, the overall\\narchitecture is content-LSTM and dynamic-GCN as well\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7667\\nas classiﬁers. Despite the lack of TES, the overall form\\nof multitasking remains.\\n2) W/o dynamic: When the model does not use DGNN,\\nthe overall architecture is content-LSTM, GCN, TES,\\nAttention mechanism, and Classiﬁer.\\n3) W/o GCN: When the model does not use a graph-\\nconvolutional network, the TES cannot be shared due\\nto the lack of a graph-convolutional layer, leaving the\\nnetwork with only the content-LSTM and the classiﬁer.\\nThe following analysis can be obtained from Table VI.\\nW/o GCN: This effect is the worst, due to the fact that\\nthe network loses objects that can be shared, leaving only the\\ncontent-LSTM layer for detection, and is unable to capture\\nspatial features as well as dynamic features.\\nW/o TES: At this time, the network is still in the mode of\\nmultitasking, but due to the loss of TES, the sharing strategy is'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='spatial features as well as dynamic features.\\nW/o TES: At this time, the network is still in the mode of\\nmultitasking, but due to the loss of TES, the sharing strategy is\\nchanged, and relying on backpropagation alone for parameter\\nsharing is limited and insufﬁcient to achieve the best perfor-\\nmance, but the accuracy is 5.07% and 1.97% higher w/o GCN\\nstrategy, respectively, which shows the important impact of the\\ntime-step-based TES.\\nW/o dynamic: This strategy is the best in addition to DGCN-\\nTES and the accuracy is higher than w/o TES by 0.94% and\\n1.2%, respectively, mainly because this approach basically re-\\nalizes multitasking combined with temporal event sharing. The\\ntwo tasks are able to capture temporal features, spatial features\\ntogether, and at the same time, because of the participation of\\nthe attention mechanism, the performance is better than other\\nstrategies. However, due to the use of static graphs, it ignores\\nthe dynamics of rumor events in the evolution process.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='the attention mechanism, the performance is better than other\\nstrategies. However, due to the use of static graphs, it ignores\\nthe dynamics of rumor events in the evolution process.\\nBy comparing with other strategies, DGCN-TES is the op-\\ntimal method for each combined strategy. The model can fully\\ncapture the content features of sequence data, the structural\\nfeatures of rumor propagation events, and the dynamic feature\\nof the propagation process, combining them in a multitask\\nfashion and designing a time-series event-sharing layer so that\\nthe two tasks can better learn from each other through ﬁne-\\ngrained time steps. Finally, the attention mechanism is utilized\\nto focus on the features that have an important effect on pre-\\ndiction between different tasks and different time steps and\\nignore those less important information so as to improve the\\nmodel effect.\\nIn summary, the following conclusion is drawn: the DGCN-\\nTES is the best performer, which proves that the idea of combin-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='ignore those less important information so as to improve the\\nmodel effect.\\nIn summary, the following conclusion is drawn: the DGCN-\\nTES is the best performer, which proves that the idea of combin-\\ning textual content information in the form of multitasking with\\na dynamic dissemination structure, and then utilizing temporal\\nevent sharing to enhance the interaction between the multitasks,\\nis effective and has important implications.\\nF. Compare and Contrast Different Time Granularity\\nDynamics Graph\\nAt the early stage of the experiment, we had a conjecture:\\nhow does the coarseness of the time-step division granularity\\nand the number of static graph snapshots constructed affect the\\nexperiment? We believe that the ﬁner the granularity and the\\nmore static graph snapshots are constructed, the more delicate\\nFig. 6.\\nDifferent time particle size comparison experiments.\\nand more effective the transition of the generated dynamic\\ngraph features will be, which will lead to a better overall per-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='Fig. 6.\\nDifferent time particle size comparison experiments.\\nand more effective the transition of the generated dynamic\\ngraph features will be, which will lead to a better overall per-\\nformance of the model.\\nIn this section, to investigate how dynamic graphs with differ-\\nent granularities affect network performance, this issue is inves-\\ntigated by actively controlling the time interval for generating\\nsnapshots of static graphs. Experiments were conducted on the\\nPHEME dataset and the WEIBO dataset, following several sets\\nof different time granularity strategies. The results are shown\\nin Fig. 6, where the horizontal axis indicates the number of set\\ntime steps relative to the average event propagation length, and\\nthe vertical axis indicates the accuracy.\\nFrom Fig. 6, it is easy to ﬁnd that when the time interval is\\nsmaller, a static graph snapshot with a smoother transition will\\nbe constructed; and thus, the captured structural features are'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='From Fig. 6, it is easy to ﬁnd that when the time interval is\\nsmaller, a static graph snapshot with a smoother transition will\\nbe constructed; and thus, the captured structural features are\\nmore delicate and more orderly. More importantly, the smaller\\nthe interval, the better the interaction between the dynamic\\ngraph network layer and the timing event sharing layer, the\\ncloser the information interaction between multiple tasks, and\\nthe better the performance of the model.\\nCoarser granularity and worst results when the time step is\\nsplit into three (1/3). However, comparing the (1/6) time steps,\\nthe accuracy of the WEIBO and PHEME datasets is improved\\nby 4.71% and 0.48%, respectively, and this gap is not very\\nobvious in the PHEME dataset, which may be due to the fact\\nthat the average number of posts of the propagation events in\\nthe PHEME dataset is less, and the transition difference of the\\ndynamical graph features of the different time steps is closer to'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 9}, page_content='that the average number of posts of the propagation events in\\nthe PHEME dataset is less, and the transition difference of the\\ndynamical graph features of the different time steps is closer to\\neach other, which ultimately leads to the results of the different\\ntime granularity is closer, and the accuracy increases steadily\\nas the time granularity increases.\\nTherefore, it was concluded as follows.\\n1) The model performs better when the temporal granularity\\nis ﬁner, the transitions of the captured dynamical graph\\nfeatures are more delicate, and the interactions in the TES\\nare closer.\\n2) The average number of posts of a propagation event will\\ndirectly determine the number of time steps, and when the\\naverage number of posts of a propagation event is small,\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 10}, page_content='7668\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\nFig. 7.\\nEarly detection on the PHEME dataset.\\nFig. 8.\\nEarly detection on the WEIBO dataset.\\nthe gap between DGCN-TES at different time granulari-\\nties is closer.\\nG. Early Detection\\nEarly rumor detection is one of the important goals of ru-\\nmor detection, a task aimed at identifying rumor events in the\\nearly stages of their propagation, and this means of assessing\\nthe quality of rumor detection methods is another important\\nreference point.\\nTo construct an early rumor detection task based on DGCN-\\nTES, this experiment on one hand controls the number of nodes\\ngenerated by inputting the rumor propagation graph into the\\ndynamic graph convolutional layer, and on the other hand,\\nintercepts the text sequence data corresponding to this time,\\nand represents the different periods of rumor events by re-\\nconstructing the rumor propagation events. Speciﬁcally, this'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 10}, page_content='intercepts the text sequence data corresponding to this time,\\nand represents the different periods of rumor events by re-\\nconstructing the rumor propagation events. Speciﬁcally, this\\nexperiment deletes the corresponding tweets posted after a ﬁxed\\ntime step to reconstruct the propagation event dataset of the\\nmodel, and after that, in the same way, we take the sequential\\ntext content corresponding to the time period and construct\\nthe dynamic propagation graph, and then evaluate the perfor-\\nmance of the rumor detection model in different periods. As in\\nFigs. 7 and 8, the differences between DGCN-TES and other\\napproaches on the PHEME dataset and WEIBO dataset are\\nshown respectively.\\nIn terms of the dataset, the WEIBO dataset performs better\\nthan the other methods in any period of time, and the accuracy\\nsteadily increases as the early detection phase develops. This\\nmay be due to the fact that the WEIBO dataset propagates events\\nwith a larger average number of posts, the dynamic excess of'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 10}, page_content='steadily increases as the early detection phase develops. This\\nmay be due to the fact that the WEIBO dataset propagates events\\nwith a larger average number of posts, the dynamic excess of\\nthe time step is very delicate, and the learned dynamic structural\\nfeatures are able to interact with the temporal features at a\\nhigher frequency. From this perspective, it can explain why\\nthe performance of the PHEME dataset is more similar to the\\nDDGCN method in the early detection phase from 20% to 60%,\\nand then pulls away from 80% to 100%.\\nFrom the results, the proposed method DGCN-TES achieves\\nrelatively high accuracy in the early stage of rumor event prop-\\nagation, in addition, STS-NN is the worst in early detection,\\nthis is because all other networks adopt graph network related\\noperation, which suggests that the graph network is more con-\\nducive to deal with early detection. As for the methods using\\ngraph networks, the best results are achieved by using dynamic'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 10}, page_content='operation, which suggests that the graph network is more con-\\nducive to deal with early detection. As for the methods using\\ngraph networks, the best results are achieved by using dynamic\\ngraph networks, and our curves are more similar to DDGCN in\\nthe early stage of rumor propagation and superior to DDGCN in\\nthe subsequent stage of rumor propagation, which is due to the\\nfact that our dynamic temporal granularity is more ﬁne-grained,\\nand the more delicate dynamic graph structure demonstrates a\\nmore superior performance in both early detection and long-\\nterm rumor detection.\\nH. Exploring the Impact of Different Variants of the Model\\nIn the above-mentioned work, this article combines dynamic-\\nGCN and content-LSTM in the form of multitasking, although\\nthe two base components are obtained by modifying the GCN\\nand LSTM networks, the most basic original model is used\\nand no related variant network is used. However, in the ﬁeld\\nof NLP research, several studies have carried out richer and'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 10}, page_content='and LSTM networks, the most basic original model is used\\nand no related variant network is used. However, in the ﬁeld\\nof NLP research, several studies have carried out richer and\\nmore effective work using variant networks, e.g., Pandey et al.\\n[35] used an LSTM network based on an attention mechanism\\nto recognize sarcastic statements, and Pandey and Singh [36]\\nproposed a BERT-based bidirectional encoder model that stacks\\nLSTM networks to perform recognition prediction.\\nRelevant research has all shown that the variant model ob-\\ntained by improving on the original baseline model has better\\nperformance, therefore, to better explore how much different\\nvariant models affect DGCN-TES, in this section, this article\\nreplaces the baseline components in DGCN-TES and carries\\nout experiments. Speciﬁcally, two variant network experimental\\nscenarios are carried out in this section as follows.\\n1) Using attention-based LSTM network instead of content-\\nLSTM component in DGCN-TES.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 10}, page_content='scenarios are carried out in this section as follows.\\n1) Using attention-based LSTM network instead of content-\\nLSTM component in DGCN-TES.\\n2) Using bi-LSTM network instead of TES component in\\nDGCN-TES\\nThe experimental results are shown in Figs. 9 and 10.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 11}, page_content='WAN et al.: DGCN-TES: DYNAMIC GCN-BASED MULTITASK MODEL WITH TEMPORAL EVENT SHARING FOR RUMOR DETECTION\\n7669\\nFig. 9.\\nComparison of F1-score in different variants of networks on WEIBO\\ndataset.\\nFig. 10.\\nComparison of F1-score in different variants of networks on\\nPHEME dataset.\\nIt is clearly visible through Fig. 9 and Fig. 10 that the\\nexperimental results of using the attention mechanism-based\\nLSTM instead of content-LSTM are better than DGCN-TES,\\nand by comparing the F1 scores on the WEIBO and PHEME\\ndatasets, it can be found that the DGCN-TES (Att-LSTM) ex-\\nceeds the DGCN-TES by 0.21% and 0.23%, which is mainly\\nbecause when dealing with sequential text data, the attention\\nmechanism is able to focus on feature information in the con-\\ntext that is more beneﬁcial for rumor detection, thus learning\\nbetter feature representations, which ultimately enhances the\\noverall performance of the model. The variant network scheme\\nusing Bi-LSTM instead of content-LSTM is more effective,'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 11}, page_content='better feature representations, which ultimately enhances the\\noverall performance of the model. The variant network scheme\\nusing Bi-LSTM instead of content-LSTM is more effective,\\nexceeding 0.82% and 0.46% on WEIBO and PHEME datasets,\\nwhich is mainly due to the fact that bi-LSTM uses bidirectional\\nsequential structure, which is not only focuses on sequential\\nfeatures from top to the bottom but also focuses on the effective\\ninformation from the bottom to the top when dealing with the\\ntemporal sequential text information. This way of processing is\\nmore dynamic and also makes the dynamic feature interaction\\nbetween the “previous time step” and “next time step” of the\\ndynamic graph in the dynamic-GCN component more effective,\\nthus showing better performance.\\nMore in-depth exploration using variant networks can further\\nimprove the performance of the model, but the focus of this\\narticle is on exploring dynamic patterns in the rumor propaga-'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 11}, page_content='More in-depth exploration using variant networks can further\\nimprove the performance of the model, but the focus of this\\narticle is on exploring dynamic patterns in the rumor propaga-\\ntion process, not in this context (e.g., using the GAT network\\ninstead of the GCN network in dynamic-GCN; and using the\\nBERT instead of content-LSTM), and therefore no more in-\\ndepth experiments are conducted using other variant networks\\nin this section.\\nV. CONCLUSION\\nIn this article, we propose a network named DGCN-TES,\\nwhich models dynamic spatial structure, temporal structure,\\nand textual content information under a uniﬁed architecture.\\nDGCN-TES contains a ﬁne-grained dynamic graph convo-\\nlutional layer based on temporal sequences and a textual\\ncontent-based LSTM layer for processing spatial and content\\ninformation over time. In addition, we designed a temporal\\nevent-sharing module for sharing the interaction of features of\\ndifferent temporal in the two tasks to learn from each other'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 11}, page_content='information over time. In addition, we designed a temporal\\nevent-sharing module for sharing the interaction of features of\\ndifferent temporal in the two tasks to learn from each other\\nduring the training process. Finally, the attention mechanism\\nis utilized to focus on the features that are beneﬁcial for pre-\\ndiction between different tasks and time series to improve the\\nnetwork’s effectiveness. Our experiments get better feedback\\non two public datasets, and DGCN-TES outperforms other\\nmethods. Finally, we did additional supplementary experiments\\nto explore the effect of different temporal granularities on the\\nperformance of dynamic networks and found that more delicate\\ntemporal granularities are more suitable for dynamic networks.\\nAn early detection study was also conducted, and the DDGCN-\\nTES performance was also superior in comparison with other\\nmethods. An extended research is also done on the effect of\\ndifferent variant networks on the model, and by using different'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 11}, page_content='TES performance was also superior in comparison with other\\nmethods. An extended research is also done on the effect of\\ndifferent variant networks on the model, and by using different\\nvariant networks for experiments, it is found that a more suitable\\nvariant network can bring better performance improvement.\\nIn the future, we will further explore the properties of rumor-\\nspreading events in dynamic networks, and process the multi-\\nmodal information in rumor-spreading events for better network\\nperformance.\\nREFERENCES\\n[1] N. DiFonzo and P. Bordia, “Rumor psychology: Social and organiza-\\ntional approaches.” Amer. Psychol. Assoc., 2007.\\n[2] J. Cohen and D. Normile, “New SARS-like virus in China triggers\\nalarm,” Amer. Assoc. Adv. Sci., vol. 367, no. 6475, pp. 234–235, 2020.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='7670\\nIEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 11, NO. 6, DECEMBER 2024\\n[3] T. Bian et al., “Rumor detection on social media with bi-directional\\ngraph convolutional networks,” in Proc. AAAI Conf. Artif. Intell.,\\nvol. 34, no. 1, 2020, pp. 549–556.\\n[4] N. Bai, F. Meng, X. Rui, and Z. Wang, “Rumor detection based on a\\nsource-replies conversation tree convolutional neural net,” Computing,\\nvol. 104, pp. 1–17, Jan. 2022.\\n[5] J. Ma, W. Gao, and K.-F. Wong, “Rumor detection on Twitter with\\ntree-structured recursive neural networks,” in Proc. 56th Ann. Meeting\\nAssociation for Comp. Linguistics, Association for Computational Lin-\\nguistics, 2018, pp. 1980–1989.\\n[6] V. Vaibhav, R. M. Annasamy, and E. Hovy, “Do sentence interactions\\nmatter? Leveraging sentence level representations for fake news classi-\\nﬁcation,” 2019, arXiv:1910.12203.\\n[7] X. Zhou and R. Zafarani, “A survey of fake news: Fundamental theories,\\ndetection methods, and opportunities,” ACM Comput. Surv., vol. 53,'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='ﬁcation,” 2019, arXiv:1910.12203.\\n[7] X. Zhou and R. Zafarani, “A survey of fake news: Fundamental theories,\\ndetection methods, and opportunities,” ACM Comput. Surv., vol. 53,\\nno. 5, pp. 1–40, 2020.\\n[8] C. Song, Y. Teng, Y. Zhu, S. Wei, and B. Wu, “Dynamic graph neural\\nnetwork for fake news detection,” Neurocomputing, vol. 505, pp. 362–\\n374, Sep. 2022.\\n[9] C. Song, K. Shu, and B. Wu, “Temporally evolving graph neural network\\nfor fake news detection,” Inf. Process. Manage., vol. 58, no. 6, 2021,\\nArt. no. 102712.\\n[10] M. Sun, X. Zhang, J. Zheng, and G. Ma, “DDGCN: Dual dynamic graph\\nconvolutional networks for rumor detection on social media,” in Proc.\\nAAAI Conf. Artif. Intell., vol. 36, no. 4, 2022, pp. 4611–4619.\\n[11] J. Ma, W. Gao, P. Mitra, S. Kwon, and B. J. Jansen, “Detecting rumors\\nfrom microblogs with recurrent neural networks,” in Proc. 25th Int. Joint\\nConf. Artif. Intell., ser. IJCAI’16, 2016, pp. 3818–3824.\\n[12] T. Ahmad, M. S. Faisal, A. Rizwan, R. Alkanhel, P. W. Khan, and'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='Conf. Artif. Intell., ser. IJCAI’16, 2016, pp. 3818–3824.\\n[12] T. Ahmad, M. S. Faisal, A. Rizwan, R. Alkanhel, P. W. Khan, and\\nA. Muthanna, “Efﬁcient fake news detection mechanism using enhanced\\ndeep learning model,” Appl. Sci., vol. 12, no. 3, 2022, Art. no. 1743.\\n[13] J. Ma, W. Gao, and K.-F. Wong, “Detect rumor and stance jointly\\nby neural multi-task learning,” in Proc. Companion Web Conf., 2018,\\npp. 585–593.\\n[14] S. Wan, B. Tang, F. Dong, M. Wang, and G. Yang, “A writing style-based\\nmulti-task model with the hierarchical attention for rumor detection,” Int.\\nJ. Mach. Learn. Cybern., vol. 14, no. 11, 2023, pp. 1–16.\\n[15] E. Kochkina, M. Liakata, and A. Zubiaga, “All-in-one: Multi-task\\nlearning for rumour veriﬁcation,” 2018, arXiv:1806.03713.\\n[16] J. P. Singh, A. Kumar, N. P. Rana, and Y. K. Dwivedi, “Attention-\\nbased LSTM network for rumor veracity estimation of tweets,” Inf. Syst.\\nFrontiers, vol. 24, pp. 1–16, Aug. 2020.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='[16] J. P. Singh, A. Kumar, N. P. Rana, and Y. K. Dwivedi, “Attention-\\nbased LSTM network for rumor veracity estimation of tweets,” Inf. Syst.\\nFrontiers, vol. 24, pp. 1–16, Aug. 2020.\\n[17] J. Ma, W. Gao, and K.-F. Wong, “Detect rumors on Twitter by promoting\\ninformation campaigns with generative adversarial learning,” in Proc.\\nWorld Wide Web Conf., 2019, pp. 3049–3055.\\n[18] M. Cheng, Y. Li, S. Nazarian, and P. Bogdan, “From rumor to genetic\\nmutation detection with explanations: A GAN approach,” Sci. Rep.,\\nvol. 11, no. 1, 2021, Art. no. 5861.\\n[19] S. Vosoughi, D. Roy, and S. Aral, “The spread of true and false news\\nonline,” Science, vol. 359, no. 6380, pp. 1146–1151, 2018.\\n[20] F. Jin, E. Dougherty, P. Saraf, Y. Cao, and N. Ramakrishnan, “Epidemio-\\nlogical modeling of news and rumors on Twitter,” in Proc. 7th Workshop\\nSocial Netw. Mining Anal., 2013, pp. 1–9.\\n[21] P. Zhang, H. Ran, C. Jia, X. Li, and X. Han, “A lightweight propagation'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='logical modeling of news and rumors on Twitter,” in Proc. 7th Workshop\\nSocial Netw. Mining Anal., 2013, pp. 1–9.\\n[21] P. Zhang, H. Ran, C. Jia, X. Li, and X. Han, “A lightweight propagation\\npath aggregating network with neural topic model for rumor detection,”\\nNeurocomputing, vol. 458, pp. 468–477, Oct. 2021.\\n[22] A. Silva, Y. Han, L. Luo, S. Karunasekera, and C. Leckie, “Propaga-\\ntion2Vec: Embedding partial propagation networks for explainable fake\\nnews early detection,” Inf. Process. & Manage., vol. 58, no. 5, 2021,\\nArt. no. 102618.\\n[23] Z. Wu, D. Pi, J. Chen, M. Xie, and J. Cao, “Rumor detection based on\\npropagation graph neural network with attention mechanism,” Expert\\nSyst. Appl., vol. 158, 2020, Art. no. 113595.\\n[24] X. Yang, Y. Lyu, T. Tian, Y. Liu, Y. Liu, and X. Zhang, “Rumor detection\\non social media with graph structured adversarial learning,” in Proc. 29th\\nInt. Conf. Int. Joint Conf. Artif. Intell., 2021, pp. 1417–1423.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='on social media with graph structured adversarial learning,” in Proc. 29th\\nInt. Conf. Int. Joint Conf. Artif. Intell., 2021, pp. 1417–1423.\\n[25] Q. Huang, C. Zhou, J. Wu, L. Liu, and B. Wang, “Deep spatial–temporal\\nstructure learning for rumor detection on Twitter,” Neural Comput. Appl.,\\nvol. 35, no. 18, pp. 1–11, 2020.\\n[26] Y. Zhang and Q. Yang, “A survey on multi-task learning,” IEEE Trans.\\nKnowl. Data Eng., vol. 34, no. 12, pp. 5586–5609, Dec. 2022.\\n[27] R. Collobert and J. Weston, “A uniﬁed architecture for natural language\\nprocessing: Deep neural networks with multitask learning,” in Proc. 25th\\nInt. Conf. Mach. Learn., 2008, pp. 160–167.\\n[28] L. Wu, Y. Rao, H. Jin, A. Nazir, and L. Sun, “Different absorption from\\nthe same sharing: Sifted multi-task learning for fake news detection,”\\n2019, arXiv:1909.01720.\\n[29] N. Bai, F. Meng, X. Rui, and Z. Wang, “A multi-task attention tree\\nneural net for stance classiﬁcation and rumor veracity detection,” Appl.'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='2019, arXiv:1909.01720.\\n[29] N. Bai, F. Meng, X. Rui, and Z. Wang, “A multi-task attention tree\\nneural net for stance classiﬁcation and rumor veracity detection,” Appl.\\nIntell., vol. 53, no. 9, pp. 10715–10725, 2023.\\n[30] M. Cheng, S. Nazarian, and P. Bogdan, “VRoC: Variational autoencoder-\\naided multi-task rumor classiﬁer based on text,” in Proc. Web Conf.,\\n2020, pp. 2892–2898.\\n[31] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 30, no. 1, pp. 261–272, 2017.\\n[32] C. Castillo, M. Mendoza, and B. Poblete, “Information credibil-\\nity on Twitter,” in Proc. 20th Int. Conf. World Wide Web, 2011,\\npp. 675–684.\\n[33] F. Yang, Y. Liu, X. Yu, and M. Yang, “Automatic detection of rumor on\\nSina Weibo,” in Proc. ACM SIGKDD Workshop Mining Data Semantics,\\n2012, pp. 1–7.\\n[34] J. Ma, W. Gao, Z. Wei, Y. Lu, and K.-F. Wong, “Detect rumors using\\ntime series of social context information on microblogging websites,” in'),\n",
       " Document(metadata={'producer': 'Adobe LiveCycle PDF Generator; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-19T19:11:24+05:30', 'source': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DGCN-TES_Dynamic_GCN-Based_Multitask_Model_With_Temporal_Event_Sharing_for_Rumor_Detection.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'DGCN-TES: Dynamic GCN-Based Multitask Model With Temporal Event Sharing for Rumor Detection', 'author': '', 'subject': 'IEEE Transactions on Computational Social Systems;2024;11;6;10.1109/TCSS.2024.3443275', 'keywords': '', 'moddate': '2024-12-01T18:26:57-05:00', 'trapped': '', 'modDate': \"D:20241201182657-05'00'\", 'creationDate': \"D:20241119191124+05'30'\", 'page': 12}, page_content='2012, pp. 1–7.\\n[34] J. Ma, W. Gao, Z. Wei, Y. Lu, and K.-F. Wong, “Detect rumors using\\ntime series of social context information on microblogging websites,” in\\nProc. 24th ACM Int. Conf. Inf. Knowl. Manage., 2015, pp. 1751–1754.\\n[35] R. Pandey, A. Kumar, J. P. Singh, and S. Tripathi, “Hybrid attention-\\nbased long short-term memory network for sarcasm identiﬁcation,” Appl.\\nSoft Comput., vol. 106, 2021, Art. no. 107348.\\n[36] R. Pandey and J. P. Singh, “BERT-LSTM model for sarcasm detection\\nin code-mixed social media post,” J. Intell. Inf. Syst., vol. 60, no. 1,\\npp. 235–254, 2023.\\nAuthorized licensed use limited to: SRM University Amaravathi. Downloaded on January 09,2026 at 11:19:35 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='Abstract\\nIn this paper, we present a novel deep Siamese network with a multi-scale hybrid feature extraction architecture, named DSN-STC\\n(Deep Siamese Network for Short Text Clustering), that significantly improves the clustering of short text. A key innovation of our\\napproach is a specialized transformation mechanism that maps pre-trained word embeddings into cluster-aware text\\nrepresentations. In this new latent space, the proposed model minimizes the overall overlapping between clusters while improving\\nthe cohesion within each cluster. This results in considerable improvements in clustering performance. Since short texts inherently\\ncontain both sequential context and localized patterns within their limited context, in this paper a hybrid approach is used by\\ncombining both recurrent layers and multi-scale convolutional neural networks to maximize the extractable feature sets from their'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='combining both recurrent layers and multi-scale convolutional neural networks to maximize the extractable feature sets from their\\nlimited context. This architecture allows us to capture the sequential features and local dependencies by recurrent layer and\\nconvolutional layers respectively which leads to generating a more accurate and rich representation for each short text. To evaluate\\nour architecture and because our main focus is on clustering Persian short text, several experiments are conducted in which the\\nresults show that the DSN-STC outperforms other approaches in clustering accuracy (ACC) and normalized mutual information\\n(NMI) metrics. Also to further test the proposed architecture’s generalizability and adaptability in other languages, DSN-STC is\\nevaluated on 2 English benchmark datasets where it consistently outperformed previous approaches in both metrics. These results'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='evaluated on 2 English benchmark datasets where it consistently outperformed previous approaches in both metrics. These results\\nhighlight the model’s ability to learn robust and cluster-aware feature representations that are highly useful for effective short text\\nclustering.\\nCitation: Molaei M, Feizi-Derakhshi M-R, Balafar M-A, Tanha J (2026) DSN-STC: Leveraging Siamese networks for\\noptimized short text clustering. PLoS One 21(1): e0335709. https://doi.org/10.1371/journal.pone.0335709\\nEditor: Qionghao Huang, Zhejiang Normal University, CHINA\\nReceived: June 12, 2025; Accepted: October 14, 2025; Published: January 2, 2026\\nCopyright: © 2026 Molaei et al. This is an open access article distributed under the terms of the Creative Commons\\nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author\\nand source are credited.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author\\nand source are credited.\\nData Availability: Data Availability Statement: The dataset generated during the current study is available in the GitHub\\nrepository, https://github.com/m-molaei/DSN-STC.\\nFunding: The author(s) received no specific funding for this work.\\nCompeting interests: The authors have declared that no competing interests exist.\\nIntroduction\\nThe exponential growth of textual documents available on the Internet in recent years has significantly increased, especially with\\nthe advent of social media platforms. With mobile devices and Internet technologies advancing quickly, users have become\\nmotivated to search for information, communicate with their peers, and share opinions and thoughts on social media platforms like\\nTwitter, Instagram, Facebook and search engines such as Google. The sheer amount of short text generated daily from these'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='Twitter, Instagram, Facebook and search engines such as Google. The sheer amount of short text generated daily from these\\nplatforms leads to a large volume of overall unstructured data.\\nShort texts are characterized by their brevity, often lacking sufficient context, making knowledge extraction challenging. Despite\\ntheir brevity, short texts are rich in information and play a crucial role in numerous applications, including information retrieval,\\nsentiment analysis, and topic detection. However, their unstructured nature presents significant challenges for data analysis,\\nparticularly in clustering, where the goal is to automatically identify valuable patterns within large collections of text [1].\\nAmong the various data-mining techniques, clustering stands out as an essential method for analyzing short text corpora.\\nClustering short texts can be highly beneficial across diverse fields like information retrieval. For example, clustering can group'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='Clustering short texts can be highly beneficial across diverse fields like information retrieval. For example, clustering can group\\nsimilar short texts in an unsupervised manner, facilitating tasks such as topic detection in short news texts. This automation reduces\\nthe need for human intervention, saving time, cost, and resources, which are typically required for manual labeling. However,\\nclustering short texts is particularly challenging due to their chaotic nature, often containing noise, slang, emojis, misspellings,\\nabbreviations, and grammatical errors [1]. Additionally, the short length of these texts exacerbates issues related to data sparsity,\\nlimited context, and high-dimensional representation. Standard clustering techniques, such as k-means [2] or DBSCAN [3], often\\nstruggle to accurately group short texts because these methods rely heavily on measuring similarity or distance between data'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 0}, page_content='struggle to accurately group short texts because these methods rely heavily on measuring similarity or distance between data\\npoints and depend on accurate text representations [4]. When applied directly to short text corpora, traditional techniques tend to\\nDSN-STC: Leveraging Siamese networks for optimized short\\ntext clustering\\nPublished: January 2, 2026\\nhttps://doi.org/10.1371/journal.pone.0335709\\nMahdi Molaei, Mohammad-Reza Feizi-Derakhshi \\n, Mohammad-Ali Balafar, Jafar Tanha\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n1/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='perform poorly, as the sparse and high-dimensional feature vectors generated by standard text representation methods like term\\nfrequency-inverse document frequency (TF-IDF) or bag of words (BoW) [5] are less effective in capturing meaningful distances\\nbetween data points [6].\\nTo address these challenges, dimensionality reduction is often employed as an essential step in the short text clustering (STC)\\nprocess. One notable advancement in this area was the introduction of Deep Embedded Clustering (DEC) [7], which utilized an\\nautoencoder (AE) network for dimensionality reduction before applying k-means clustering. This approach marked a significant step\\nforward in tackling the high-dimensionality problem inherent in short text representation. However, despite these advancements,\\nsignificant challenges persist. One notable issue is that the dimensionality reduction achieved through AE networks can'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='significant challenges persist. One notable issue is that the dimensionality reduction achieved through AE networks can\\ninadvertently increase overlap between clusters, potentially degrading the overall clustering performance.\\nRecognizing the limitations of existing single‐branch and purely unsupervised schemes, this paper introduces a sophisticated\\nSiamese‐based architecture that jointly tackles both dimensionality reduction and cluster separability within a unified training\\nframework. Siamese networks, first introduced in 1994, are typically used for tasks involving similarity detection [8]. These networks\\nconsist of two identical subnetworks with shared parameters, which are trained simultaneously to generate outputs for two input\\ninstances. The network then updates its weights based on whether the two inputs belong to the same class, aiming to reduce the\\ndistance between embeddings of similar texts and increase the separation between those of different classes by a margin m. To'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='distance between embeddings of similar texts and increase the separation between those of different classes by a margin m. To\\nadvance this paradigm, our proposed DSN-STC integrates a hybrid feature extraction framework that jointly combines two\\ncomplementary branches: a recurrent neural network to model long-range contextual dependencies and a multi-scale convolutional\\nmodule to capture local n-gram patterns across varying granularities. The extractable features from the limited context of short text\\nare maximized by this architecture.\\nBeyond its architectural innovations, DSN-STC learns a mapping from word embeddings into text representations \\n that\\nprojects high-dimensional input vectors into a dense, cluster-aware latent space. The contrastive loss not only ensures tight intra-\\ncluster cohesion and wide inter-cluster gaps, but also implicitly encourages the network to emphasize features with strong cluster-'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='cluster cohesion and wide inter-cluster gaps, but also implicitly encourages the network to emphasize features with strong cluster-\\nlabel correlations. Theoretically, this encourages the network to identify and amplify key lexical or syntactic features such as\\ndomain-specific keywords or phrase structures, that carry the highest mutual information with respect to cluster labels. In this way,\\nDSN‐STC not only reduces representational dimensionality but also systematically mitigates cluster overlap thereby resolving two\\nof the principal challenges identified in prior short-text clustering research.\\nIn addition, we observed that texts with fewer than 10 tokens, after preprocessing, often lack sufficient information to be encoded\\ninto meaningful representations for clustering purposes. As a result, the dataset is filtered to include only texts with a length\\nbetween 10 and 30 tokens. This range was chosen to focus on short texts while still retaining enough content for effective'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='between 10 and 30 tokens. This range was chosen to focus on short texts while still retaining enough content for effective\\nclustering. By filtering out extremely short texts, it is ensured that our method could produce more meaningful representations,\\nleading to more accurate clustering results. Furthermore, although this study concentrates on Persian short texts, the DSN-STC\\nwas also evaluated on English corpora and found to yield similarly strong gains. This cross‐linguistic evaluation confirms that our\\narchitecture is not tied to a single language’s characteristics but generalizes robustly across different linguistic settings.\\nOur extensive evaluations demonstrate that the proposed approach significantly outperforms previous methods, achieving superior\\nclustering performance.\\nThe main contributions of this paper are:\\n1. Contrastive Siamese Pre-trained Word Embeddings Transformer for Learning Cluster-Aware Text Representations: A novel Siamese‐network'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='The main contributions of this paper are:\\n1. Contrastive Siamese Pre-trained Word Embeddings Transformer for Learning Cluster-Aware Text Representations: A novel Siamese‐network\\narchitecture has been formulated to learn dense, cluster‐aware representations for short texts. By jointly optimizing a contrastive loss, the model provably\\nminimizes intra‐cluster cohesion while maximizing inter‐cluster margins in the new latent space. This theoretical design ensures that high‐dimensional, sparse\\ntext inputs are mapped into a lower‐dimensional latent space where cluster overlap is rigorously controlled, leading to more separable and robust clusters.\\nEvaluation results confirm that the learned margin satisfies bounds on cluster cohesion and separation, extending prior work on contrastive representation\\nlearning.\\n2. Multi-Scale Recurrent–Convolutional Fusion for Rich Text Representations: A novel hybrid architecture is presented that fuses bidirectional sequential'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='learning.\\n2. Multi-Scale Recurrent–Convolutional Fusion for Rich Text Representations: A novel hybrid architecture is presented that fuses bidirectional sequential\\nfeatures with multi-scale n-gram dependencies through parallel processing paths. In this design, one path employs recurrent units to model long-range\\ncontextual interactions, while the other leverages convolutional filters of varying kernel sizes to extract localized n-gram patterns at multiple granularities. This\\nhigher-dimensional basis enables the model to implicitly identify and amplify those token sequences or phrase structures that carry maximal mutual\\ninformation and can reflect the subject of text better. In addition these rich representations are further refined via contrastive training, the resulting the new\\nlatent space becomes both dense and highly discriminative. Empirically, this synergy between long-range dependency modeling and localized pattern'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='latent space becomes both dense and highly discriminative. Empirically, this synergy between long-range dependency modeling and localized pattern\\ndetection yields a more informative representation space and leads to significant improvements in clustering accuracy.\\n3. Focus on Effective Text Length for Clustering: An optimal token‐count window of 10–30 was determined and applied to ensure that each document\\ncontains enough semantic information while excluding outliers, excessively short texts lacking context or overly long passages that may introduce noise. Texts\\noutside this range were removed, resulting in a corpus of concise yet contextually rich inputs. This filtering strategy produced more informative representation\\nand yielded consistent gains in clustering accuracy and cluster coherence which is shown in experiments.\\nThe remainder of this paper is organized as follows. The Related Work section surveys prior work in short-text clustering, grouping'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='The remainder of this paper is organized as follows. The Related Work section surveys prior work in short-text clustering, grouping\\nmethods into several categories and highlights the gaps that our DSN-STC addresses. The Methods section details the proposed\\nDSN-STC architecture, including its Siamese network design, multi-scale hybrid feature extraction architecture, and contrastive-\\nloss training procedure. The Experiments section describes our experimental setup including datasets, preprocessing,\\nhyperparameters (Table 1), evaluation metrics, and present results across empirical evaluations, ablation studies, statistical\\nvalidation, and cross-linguistic tests. The Discussion section discusses key findings, strengths, and limitations of DSN-STC. Finally,\\nthe Conclusions and Future Works section concludes the paper and outlines directions for future work.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 1}, page_content='1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n2/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='Table 1. Hyperparameter settings for DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t001\\nRelated work\\nThe field of short‐text clustering has evolved rapidly, encompassing a variety of deep‐learning architectures and representation\\nstrategies. This section reviews four key categories of prior work, including deep embedded clustering, contrastive and Siamese\\napproaches, transformer‐based clustering, and hybrid recurrent–convolutional models, and then highlights how DSN-STC\\nadvances beyond each.\\nDeep Embedded Clustering (DEC) was one of the pioneering approaches to introduce a deep learning-based method in the field of\\nclustering [7]. DEC uses an autoencoder (AE) network to extract feature representations, which are crucial for overcoming the\\nlimitations of traditional clustering techniques that struggle with high-dimensional and sparse data, such as short texts. In this'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='limitations of traditional clustering techniques that struggle with high-dimensional and sparse data, such as short texts. In this\\napproach, feature representation learning and clustering are performed jointly by combining the autoencoder with the k-means\\nalgorithm. This approach demonstrated superior performance, outperforming previous methods in both image and textual data\\nclustering tasks. Following the introduction of Deep Embedded Clustering (DEC), many classic methods have been proposed for\\nthe advancement of the deep learning-based text clustering field. Examples include Improved Deep Embedded Clustering (IDEC)\\n[9], Short Text Clustering with SIF Embeddings (STC) [10], Deep Clustering Network (DCN) [11], and DEC with Data Augmentation\\n(DEC-DA) [12], each of which introduce their own methodology for improving the original method, focusing on different components'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='(DEC-DA) [12], each of which introduce their own methodology for improving the original method, focusing on different components\\nof the clustering method. These approaches share a common thread of jointly learning feature representations and cluster\\nassignments, but they differ in their specific implementations and each of these methods aimed to address specific challenges in\\nclustering tasks, such as better handling of data sparsity, improving feature representation, or enhancing the separation between\\nclusters. These models consistently demonstrated improved performance across various datasets by introducing more\\nsophisticated techniques for joint learning of feature representations and cluster assignments. They showed notable success in\\nclustering both image and textual data, further advancing the field of deep learning-based clustering methods. Self-Taught'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='clustering both image and textual data, further advancing the field of deep learning-based clustering methods. Self-Taught\\nConvolutional Neural Networks for Short Text Clustering also addresses the sparsity of short‐text by first compressing raw features\\ninto binary codes, then training a CNN to fit those codes while learning semantic representations, and finally applying K-means on\\nthe learned features [4]. This framework demonstrated that unsupervised CNNs, guided by auxiliary codes, can effectively capture\\nboth local n-gram patterns and global semantic structure in brief texts.\\nIn the realm of deep learning-based clustering, a notable advancement came with the introduction of a novel approach that\\nleverages deep neural networks to simultaneously learn feature representations and suitable embeddings [13]. This method builds\\nupon the foundation laid by earlier techniques like DEC [7]. At its core, the approach utilizes an autoencoder for dimensionality'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='upon the foundation laid by earlier techniques like DEC [7]. At its core, the approach utilizes an autoencoder for dimensionality\\nreduction, followed by a specialized representation network connected to the encoder’s output. The key innovation lies in its\\nobjective to maximize inter-cluster distances by minimizing cross-entropy between pairwise similarity distributions in the\\nautoencoder’s latent space and the representation network’s embedding space. This strategy effectively encourages maximum\\nseparation between clusters, addressing a common challenge in clustering tasks. Evaluated on both textual and image datasets,\\nthe method demonstrated significant improvements over its predecessors, marking a step forward in the field of unsupervised\\nlearning and clustering. The paper [14] tackles the challenges of clustering sparse and high-dimensional short texts. The authors'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='learning and clustering. The paper [14] tackles the challenges of clustering sparse and high-dimensional short texts. The authors\\npropose two methods using unsupervised autoencoders to enhance text representation. The first, Structural Text Network Graph\\nAutoencoder (STN-GAE), combines text network structure with pre-trained features using graph convolutional networks. The\\nsecond, Soft Cluster Assignment Autoencoder (SCA-AE), adds a soft clustering constraint in the latent space to improve clustering-\\naware representations. Experiments on seven datasets show significant improvements over traditional models, with the SCA-AE\\nachieving up to 14% better accuracy compared to BERT. In [15] Guan et al. introduce a novel framework that overcomes the\\nlimitations of traditional text clustering methods. Recognizing the shortcomings of bag-of-words models in handling high\\ndimensionality, sparsity, and sequential information, the authors propose a deep feature-based approach. The DFTC framework'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='dimensionality, sparsity, and sequential information, the authors propose a deep feature-based approach. The DFTC framework\\nleverages pre-trained text encoders to capture rich semantic representations, reducing the reliance on supervised learning often\\nassociated with deep learning-based clustering. Empirical evaluations demonstrate DFTC’s superior performance over traditional\\nmethods and state-of-the-art models like BERT across diverse datasets. To enhance interpretability, the paper presents the Text\\nClustering Results Explanation (TCRE) model, which provides insights into the semantics of the formed clusters. This contribution\\nsignificantly advances the field of text data analysis by offering both improved clustering accuracy and meaningful explanations.\\nDing and Mei, in [16] proposed a framework that incorporates semantic fusion into the BiLSTM-CNN architecture, which improves'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='Ding and Mei, in [16] proposed a framework that incorporates semantic fusion into the BiLSTM-CNN architecture, which improves\\nshort text classification via a number of local and contextual features. Using the Skip-gram model to embed words, local features\\nare captured through the CNN, while global context is handled by BiLSTM, which improves upon the limitations of more classical\\nmodels. Multiple tests conducted on several datasets demonstrate method’s superior performance relative to other classification\\nmodels and present a robust method that reclines upon a new method of feature extraction, which enhances performance when\\nclassifying short text. The paper [17] presents a novel method for clustering short texts, particularly from social media. The authors\\naddress challenges like data sparsity and non-standard language by integrating BERT, which captures contextual semantics, with'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 2}, page_content='address challenges like data sparsity and non-standard language by integrating BERT, which captures contextual semantics, with\\nthe Biterm Topic Model (BTM) to analyze word co-occurrences and extract topics. Using the DBSCAN algorithm, the proposed\\napproach demonstrates high clustering accuracy and improved text processing quality. This research offers a valuable contribution\\nto short text analysis, especially in environments with sparse and variable data. Paper [18], tackle the issues of feature sparsity and\\nsemantic ambiguity often found in short texts. They introduce the DCAN model, which combines convolutional neural networks\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n3/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='(CNN) with an attention mechanism and dynamic routing to improve the extraction and fusion of features. The model first uses CNN\\nto capture features at various levels of granularity, enriching the text’s semantic representation. It then applies an attention\\nmechanism to prioritize relevant features based on context, followed by dynamic routing to optimize information flow between\\nlayers. Tested on datasets like AG News and SST-2, DCAN demonstrated superior accuracy over existing methods, offering a\\nvaluable advancement in short text classification and natural language processing. Further advancements have focused on\\nimproving the robustness of these models by explicitly addressing noise and outliers in embedding-based clustering using a\\ncombination of Frobenius-based reconstruction with sparsity-promoting or elastic penalties and by incorporating graph-based\\nregularizes to preserve local geometry [19,20]. These approaches improve robustness to Laplacian/outlier noise and enhance the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='regularizes to preserve local geometry [19,20]. These approaches improve robustness to Laplacian/outlier noise and enhance the\\ntopology of learned representations, which can be beneficial for sparse or noisy short-text corpora.\\nOther methods have explored hybrid deep–probabilistic models, such as combining autoencoder representations with Gaussian\\nMixture Models (GMMs) [21]. The authors design a framework that enables the joint optimization of both data representations and\\nGMM parameters, which helps achieve more compact clusters and better separation between them. The process starts with an\\nautoencoder extracting features from unlabeled data, which are then modeled by a GMM. What sets this method apart is its\\nadaptive mechanism, where the GMM parameters are continuously refined based on the learned features, allowing the model to\\nalign more accurately with the true data structure. This twofold optimization not only captures the distribution more precisely but'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='align more accurately with the true data structure. This twofold optimization not only captures the distribution more precisely but\\nalso enhances the clustering by ensuring the Gaussian components distinctly represent separate clusters. Tests on eight datasets\\nshow that the method surpasses several existing advanced clustering techniques, making it particularly effective for unsupervised\\nlearning. This contribution offers a robust solution that combines deep learning with probabilistic modeling to advance clustering\\ntechniques.\\nSiamese networks have ability to learn meaningful representations by analyzing the relationships between document pairs. These\\nnetworks are particularly effective at improving clustering performance, especially when dealing with high-dimensional, sparse data.\\nBy focusing on the similarities and differences between texts, Siamese architectures allow for more nuanced and context-aware'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='By focusing on the similarities and differences between texts, Siamese architectures allow for more nuanced and context-aware\\nclustering results. In the paper [22], authors propose a deep Siamese neural network that addresses the shortcomings of traditional\\nhigh-dimensional text representation methods. This model learns low-dimensional document embeddings by focusing on semantic\\nsimilarities between documents, which improves text classification tasks. Using two sub-networks based on multi-layer perceptrons\\n(MLPs), the network is trained to boost similarity scores for documents within the same category and reduce them for those in\\ndifferent categories. Tested on the BBC news dataset, the method significantly outperforms conventional approaches, offering\\nimportant insights into improving text categorization with advanced neural architectures. The paper [23] proposed a model based on'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='important insights into improving text categorization with advanced neural architectures. The paper [23] proposed a model based on\\nthe Siamese Neural Network (SNN) which is used for calculating semantic similarities between different languages and domains.\\nThey improved classic SNNs using the ReLU activation function and lexical feature sets, leading to improvements in measuring\\nsemantics between short text pairs. The proposed method is evaluated on both English and Portuguese datasets. The results\\noutperformed baseline models and showed proposed method’s effectiveness in cross-lingual and cross-domain text similarity tasks.\\nThis research offers a valuable approach to improving semantic similarity with minimal training data. A novel methodology to boost\\ntextual clustering by integrating semi-supervised learning approaches is proposed in [24]. The authors use pairwise constraints to'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='textual clustering by integrating semi-supervised learning approaches is proposed in [24]. The authors use pairwise constraints to\\nguide the clustering process. These constraints specify whether pairs of documents should be grouped together (‘must-link’) or kept\\napart (‘cannot-link’). Pairwise clustering performance improved significantly. The method introduced is based on Convolutional\\nSiamese Network (CSN) that can learn a low-dimensional representation of the documents, which captures semantic similarities\\nbetween the documents. The low-dimensional representations are learned and optimized with the pairwise constraints, leading to\\nimprovement in the quality of the clusters. After the representations are learned, a K-Means algorithm is used to cluster the\\ndocuments. The authors demonstrate performance on 8 data sets, and the proposed method shows consistent improved'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='documents. The authors demonstrate performance on 8 data sets, and the proposed method shows consistent improved\\nperformance against alternative clustering methods, i.e., MPC-KMeans [25] and standard K-Means preferences. This paradigm has\\nbeen most powerfully realized in recent years through the adoption of transformer architectures. Models such as Sentence-BERT\\n(SBERT) have set a new standard by fine-tuning pre-trained language models on sentence-pair objectives, thereby producing\\nembeddings that capture nuanced semantic relationships with unprecedented accuracy [26].\\nMany architectures for text clustering have been proposed, but relatively few have focused specifically on short texts. A\\nfundamental challenge in these approaches is the generation of cluster-aware, low-dimensional representations that exactly\\nencode the salient features of the original text while improving effective clustering. Although existing methods successfully integrate'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='encode the salient features of the original text while improving effective clustering. Although existing methods successfully integrate\\nrepresentation learning with clustering, they frequently employ single-branch networks or rely on unsupervised coding schemes that\\nmay inadequately capture cluster structure in short-text contexts. Transformer-based clustering solutions (e.g., BERT\\u2009+\\u2009K-means)\\nprovide strong baselines but incur substantial computational overhead. In contrast, the proposed method, DSN-STC, is constructed\\nas a semi-supervised, multi-scale hybrid Siamese network combining recurrent and convolutional feature extractors which is trained\\nwith a contrastive loss to map initial embeddings into a cluster-aware space where representations are both lower in dimensionality\\nand more distinctly separated. Consequently, inter-cluster distances are maximized and intra-cluster distances minimized which'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='and more distinctly separated. Consequently, inter-cluster distances are maximized and intra-cluster distances minimized which\\ncauses minimizing clusters’ overlap and more accurate clustering outcomes. This design has the potential to transform large-scale\\nshort-text analysis by producing embeddings that are inherently optimized for clustering tasks.\\nProblem definition\\nLet \\n be an input pair, where \\n and \\n are the word embeddings of short text that are generated from the pre-trained word\\nembedding model. Both \\n and \\n are fed into a shared neural network , parameterized by θ, and then two feature vectors \\nand \\n are generated at the output:\\n(1)\\nFor contrastive supervision we constructed an exhaustive pairwise training set using the available class labels. Concretely, given \\nsentences \\n with ground-truth labels \\n, we built the binary affinity matrix \\n with:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 3}, page_content='sentences \\n with ground-truth labels \\n, we built the binary affinity matrix \\n with:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n4/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 4}, page_content='(2)\\nThis procedure generates a complete training set of \\n labeled pairs, where every ordered pair \\n receives a\\ncorresponding label \\n. We use these labeled pairs as the supervised signal for the Siamese contrastive objective: positive pairs\\nare driven to small embedding distance while negative pairs are pushed to lie beyond the margin m. The exhaustive construction\\nmaximizes the available pairwise supervision and provides a dense, stable training signal for contrastive learning in our semi-\\nsupervised setup.\\nInstead of Euclidean distance, we measure similarity via the cosine distance:\\n(3)\\nwhich lies in the interval [0,2] since \\n. In practice (explained later in the experimental setup section) encoder outputs\\nare L2-normalized before computing cosine distance.\\nProposed method\\nThis study did not involve human participants, identifiable human data, or animals; therefore, ethical approval and informed consent\\nwere not required.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 4}, page_content='Proposed method\\nThis study did not involve human participants, identifiable human data, or animals; therefore, ethical approval and informed consent\\nwere not required.\\nThe overall architecture of DSN-STC is depicted in Fig 1. Each Siamese branch begins by feeding a sequence of word embeddings\\nfor two input texts. These embeddings are then processed by the hybrid feature extraction architecture, which consists of three\\nparallel modules: a recurrent layer, three convolutional layers with kernel sizes 3, 5, and 7, and a stack of fully connected layers to\\nextract multi-scale, cluster-aware features. Specifically:\\nFig 1. Proposed architecture of DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.g001\\n1. Recurrent Path:\\nA recurrent layer models long-range dependencies and produces hidden states {ℎ}. These states are projected through two dense\\nlayers to yield a dense feature vector \\n.\\n2. Convolutional Path:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 4}, page_content='A recurrent layer models long-range dependencies and produces hidden states {ℎ}. These states are projected through two dense\\nlayers to yield a dense feature vector \\n.\\n2. Convolutional Path:\\nThree 1D convolutions capture local n-gram patterns at varying granularities. Their outputs are concatenated and passed through a\\ndense layer to form \\n.\\n3. Fusion and Projection:\\nThe vectors r and c are concatenated and mapped by a last dense layer into the final representation \\n. A contrastive-\\nloss head then enforces pulling together similar pairs and pushing apart others by margin m.\\nTheoretically, this design learns a mapping from word embeddings into text representations \\n into a cluster‐aware latent\\nspace, where representations are both lower in dimension and more separable. Each sentence contains both local and long-range\\ndependencies among its words. For example, consider this sentence: “Alice, who had traveled the world, cherishes her childhood'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 4}, page_content='dependencies among its words. For example, consider this sentence: “Alice, who had traveled the world, cherishes her childhood\\nmemories.” Capturing and extracting local patterns (e.g., “childhood memories” in this example) requires convolutional layers, while\\nmodeling the relation between “Alice” and “cherishes” demands long-range context which can be extracted by the recurrent layer.\\nThis combination of convolutional and recurrent features equips the network with a multi-faceted latent space in which the\\ncontrastive loss can more effectively discern and reinforce cluster structure. Furthermore, by emphasizing dimensions that capture\\nboth salient local patterns and long-range relational context, intra-cluster cohesion is strengthened, and inter-cluster separation is\\nenlarged. Consequently, these representations become inherently optimized for downstream clustering tasks. Detailed\\nconfigurations for each module follow in the subsections below.\\nThe Siamese network\\nt\\n1/8/26, 1:04 PM'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 4}, page_content='configurations for each module follow in the subsections below.\\nThe Siamese network\\nt\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n5/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 5}, page_content='A Siamese neural network consists of two identical subnetworks that share the same weights, architecture, and parameters. These\\nsubnetworks process two inputs in parallel, which are then compared to determine their similarity or distance. Then the outputs,\\nwhich are two final representations of input short text, are evaluated using the contrastive loss function that measures the distance\\nbetween these two representations \\n and tries to maximize or minimize the existing distance between them based on their\\nlabels that show they are in the same cluster or not. The contrastive loss is defined as follows in (4):\\n(4)\\nwhere \\n is the cosine distance (Eq 3) measured between the embeddings, \\n is a binary label which is taken from affinity matrix\\n, m is a margin that defines the minimum distance between dissimilar pairs, ensuring that negative pairs are pushed apart by\\ndistance of m.\\nThe overall loss'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 5}, page_content=', m is a margin that defines the minimum distance between dissimilar pairs, ensuring that negative pairs are pushed apart by\\ndistance of m.\\nThe overall loss \\n minimized for a training mini-batch B is the mean of the pairwise loss over all |B| pairs in the batch:\\n(5)\\nProposition 1: Margin-Separation Guarantee in Theory\\nAssume encoder outputs are L2-normalized (as done in training). If training reaches perfect convergence on the training set (i.e.,\\nwhen L\\u2009=\\u20090 for all pairs), the model guarantees:\\n1. Cluster Cohesion: Every positive pair \\n is mapped to the same point, resulting in a cosine distance of zero:\\nTherefore, the maximum intra-cluster distance is \\n.\\n2. Cluster Separation: Every negative pair \\n is separated by a margin of at least m:\\nTherefore, the minimum inter-cluster distance is at least m.\\nIn other words, this is not an assumption but a direct result of convergence. With \\n and the minimum negative-pair distance'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 5}, page_content='Therefore, the minimum inter-cluster distance is at least m.\\nIn other words, this is not an assumption but a direct result of convergence. With \\n and the minimum negative-pair distance\\nbeing m, we have a guaranteed inter-cluster gap of at least m in cosine distance.\\nProof Sketch:\\nThe proof follows from the definition of the contrastive loss function. At the point of training convergence, the total loss L is zero.\\nSince the loss function is a sum of non-negative terms, the loss contribution for every individual training pair must also be zero. We\\nanalyze the two cases:\\n1. Positive pairs \\nFor a positive pair, the corresponding loss term is \\n. For the total loss to be zero, this term must be zero:\\nThis demonstrates that all positive pairs are mapped to the same point in the embedding space and have a cosine distance of zero.\\n2. Negative pairs \\nFor a negative pair, the loss term is \\n. For this term to be zero, the argument of the squared max\\nfunction must itself be zero:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 5}, page_content='2. Negative pairs \\nFor a negative pair, the loss term is \\n. For this term to be zero, the argument of the squared max\\nfunction must itself be zero:\\nThis equality holds only if the term inside is non-positive, which means \\n. This directly implies:\\nTheoretical Bounds in the Non-Asymptotic Case:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n6/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='Beyond the idealized convergence guarantee presented in Proposition 1, the contrastive loss function offers a deeper theoretical\\nstrength by providing explicit bounds on cluster quality in the practical, non-asymptotic case. While the analysis above describes\\nthe ideal convergence scenario, in practice, the model converges to a state where the loss for any given pair is bounded by a small\\nresidual value, \\n. In this more realistic scenario, the structure of the loss function allows us to derive formal guarantees on the\\nfinal cluster structure:\\nThis analysis demonstrates that the quality of the final clustering, both its compactness and its separation margin, is directly and\\nmathematically tied to the model’s ability to minimize the training loss, providing a strong theoretical justification for our approach.\\nThe primary goal is to learn a latent space where distances directly reflect text‐pair similarity, transforming pre-trained word'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='The primary goal is to learn a latent space where distances directly reflect text‐pair similarity, transforming pre-trained word\\nembeddings into a cluster-aware text representations. Furthermore, by maximizing inter‐cluster distances and minimizing intra‐\\ncluster distances, this space mitigates overlap and enhances clustering performance, especially for short texts with limited context.\\nThis is particularly useful for extracting rich features and generating cluster-aware representations for short text that has limited\\ncontextual information.\\nIn the following sections, we will explain about details of the internal structure of the proposed DSN-STC, which is designed to\\nextract a rich feature pool and generate text representations based on these extracted features and their corresponding clusters.\\nRecurrent layer: Sequential features and long-term dependencies.\\nRecurrent layers are the sort of neural network that can help with sequential data and are therefore more geared towards these'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='Recurrent layer: Sequential features and long-term dependencies.\\nRecurrent layers are the sort of neural network that can help with sequential data and are therefore more geared towards these\\ncontext-heavy domains of use, such as natural language processing and time series analysis. Unlike traditional neural networks,\\nwhich treat inputs independently of each other, recurrent layers have loop connections that allow thoughts and memories to pass\\nfrom one step to another. This preserved data from input helps the model to remember existing orders between sequences and\\nworks well in NLP fields like language modeling, speech recognition, and sequence prediction. But still, the vanilla recurrent layer\\nhas some problems which the most important of them is that as the sequence gets longer, the model cannot remember previous\\ninformation. To overcome the shortcomings of vanilla recurrent layer, researchers developed more advanced models like Long'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='information. To overcome the shortcomings of vanilla recurrent layer, researchers developed more advanced models like Long\\nShort-Term Memory (LSTM) [27] and Gated Recurrent Units (GRUs) [28]. These networks have modified structure that help them\\nremember information for longer sequences and make them better suited for handling complex tasks.\\nIn the proposed Siamese network, a recurrent layer is used to effectively capture the complex long-range dependencies between\\nwords and model sequential features. As discussed earlier, recurrent layers excel at handling sequential data, making them\\nparticularly well-suited for language tasks. By using a recurrent layer, the flow of text is modeled, capturing not only individual word\\nmeanings, but also how they interact and existing relations in sequence. By using a recurrent layer in the proposed network, it is\\nensured that the model can dynamically learn from the sequential flow of the text and improve its ability to represent complex'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='ensured that the model can dynamically learn from the sequential flow of the text and improve its ability to represent complex\\npatterns and relationships. In better words, we use a recurrent layer to extract sequential data and generate an informative\\nrepresentation at first as our first set of features in this network. As there are various types of recurrent layers, each was\\nimplemented and evaluated to determine which extracts the richest features for clustering. The evaluation results will be discussed\\nin the next section.\\nConvolutional layers: Local features and N-grams dependencies.\\nConvolutional neural networks have gained significant success in the field of NLP because of their ability to process and analyze\\ntextual data effectively. While they were originally proposed and designed for image processing, they have been adapted for text by\\nconsidering words or n-grams as spatial features, allowing these networks to extract and learn hierarchical features. This adaption'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='considering words or n-grams as spatial features, allowing these networks to extract and learn hierarchical features. This adaption\\nenables convolutional layers to extract local features and relationships that lead to have good performance in NLP tasks such as\\ntext classification and categorization. As aforementioned earlier, one of the key advantages of using convolutional layers for\\nprocessing textual data is the ability of these networks to model and extract existing local patterns between n-grams by applying\\nconvolutional filters on them. Extracting these sets of features can help to model the overall meaning of the text. This helps the\\nmodel understand the meaning of the text in a way that traditional methods, which simply count words without considering their\\norder or relationships, cannot [29]. In addition, convolutional layers can handle dimensionality challenges by using Pooling layers to'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='order or relationships, cannot [29]. In addition, convolutional layers can handle dimensionality challenges by using Pooling layers to\\nreduce dimensionality in a way that the main and essential features preserves. This is especially useful for NLP tasks that involve\\ninput sequences of varying lengths [30].\\nThree convolutional layers were used as the second component of the hybrid feature extraction architecture. The core of this\\ncomponent consists of three parallel convolutional layers that each of them uses a distinct kernel size of 3, 5, and 7, respectively.\\nThe main purpose of this multi-scale approach is rooted in the linguistic diversity of short texts. Key information can be encoded in\\nphrases of varying lengths, and a single kernel size would inevitably overlook critical patterns. Using linguistic examples from\\nPersian, we can illustrate the necessity of this approach:\\nBound on Cluster Cohesion: For any positive pair \\n, the loss term is \\n. If we assume \\n, it follows that'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 6}, page_content='Persian, we can illustrate the necessity of this approach:\\nBound on Cluster Cohesion: For any positive pair \\n, the loss term is \\n. If we assume \\n, it follows that \\n, which\\nprovides an upper bound on the intra-cluster distance:\\n\\uf054\\nBound on Cluster Separation: Similarly, for any negative pair \\n, the loss is \\n. Bounding this loss by  implies that\\n. Since the max term must be non-negative, this simplifies to \\n, which provides a lower bound on the\\ninter-cluster distance:\\n\\uf054\\nA small kernel (size 3) is effective at capturing tight, meaningful collocations that act as a single semantic unit, such as «بورس اوراق بهادار» (‘stock exchange’).\\n\\uf054\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n7/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='These general examples are representative of the n-gram structures found within used dataset [31], where key topics are often\\ndistinguished by such phrases of varying lengths. By using these kernels in parallel, the model can simultaneously detect these\\ndifferent types of features-from atomic entities to complete phrasal events- and create a richer and more robust representation for\\neach short text. After each convolutional layer, a Pooling layer is used. These layers have two main responsibilities:\\n1). Dimensionality reduction that helps to computational cost efficiency\\n2). Translation invariance, enhancing the model’s robustness to variations in word positions. In other words, Pooling layers reduce the spatial dimensions of\\nthe feature maps generated by convolutional layers which causes the model to focus on the most salient features which not considering the minor variations in'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='the feature maps generated by convolutional layers which causes the model to focus on the most salient features which not considering the minor variations in\\ninput. This characteristic is particularly beneficial in NLP, where the same meaning can be conveyed through different word arrangements [30].\\nIn the last step, the feature vectors generated by each network are concatenated together. This concatenation process leads to\\ngenerating a single comprehensive vector for each input short text, that represent a multi-scale encapsulation of features from\\ndifferent contextual windows.\\nThis feature extracting process is design to complement the recurrent component of the proposed architecture which is described in\\nRecurrent Path subsection. In other words, while the recurrent component is used for extracting sequential dependencies, the\\nconvolutional component focuses on extracting local features and patterns between n-gram. This approach will allow us to create a'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='convolutional component focuses on extracting local features and patterns between n-gram. This approach will allow us to create a\\nrich feature set for generating the final representations.\\nFully connected layers: Feature concatenation and dimensionality reduction.\\nThe third component of the proposed hybrid feature extraction architecture consists of several fully connected (FC) layers. These\\nlayers have an important role in efficiently integrating and refining features. The main characteristics of these layers can be\\nexplained in the following points:\\n1). Concatenation of feature vectors: These FC layers serve to fuse the rich feature sets that are extracted in previous components. This fused representation\\nintegrates sequential-based features extracted by recurrent component and local n-gram dependencies extracted by convolutional component.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='integrates sequential-based features extracted by recurrent component and local n-gram dependencies extracted by convolutional component.\\n2). Efficient dimensionality reduction: During feature fusion, these layers progressively reduce the dimensionality of the fused vector. This process is performed\\nin a learned manner as opposed to a direct way which can cause the loss of important features. In other words, by learning to fuse and compress features\\nrather than applying a direct reduction, the model can preserve the most salient information while yielding a dense final representation.\\nClustering text\\nUpon completion of training, the Text Embedding Encoder learns a cluster‐aware mapping from word embeddings into text\\nrepresentation and becomes a robust representation generator for input texts. It can then be applied to any short text to produce\\nhigh‐quality, cluster‐discriminative representations. By emphasizing features that distinguish each cluster, the encoder enhances'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='high‐quality, cluster‐discriminative representations. By emphasizing features that distinguish each cluster, the encoder enhances\\nintra‐cluster cohesion and enlarges inter‐cluster separation, thereby improving overall clustering performance. In the subsequent\\nclustering phase, these representations are supplied to a clustering algorithm (which is K-means in our implementation) to partition\\nthe texts into clusters.\\nExperiments\\nThis section presents the empirical evaluation of the proposed DSN-STC model. We first describe the experimental setup\\n(datasets, pre-processing, and evaluation metrics) and the implementation details, then report the results of several experiments\\ndesigned to evaluate model performance, justify architectural choices, and compare DSN-STC against state-of-the-art baselines.\\nExperimental setup\\nAll experiments were implemented in Python using the TensorFlow framework [32]. The DSN-STC model was trained using the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='Experimental setup\\nAll experiments were implemented in Python using the TensorFlow framework [32]. The DSN-STC model was trained using the\\nhyperparameter settings detailed in Table 1, which were kept consistent across all relevant experiments unless otherwise specified.\\nThe final clustering of the learned embeddings was performed using the K-means algorithm, as implemented in the Scikit-learn\\nlibrary. For evaluation purposes, the number of clusters, K, was set to the number of ground-truth classes in each dataset. While\\nthe standard K-means algorithm minimizes squared Euclidean distance, our model’s contrastive loss function operates on cosine\\ndistance. To ensure theoretical consistency between the training objective and the clustering metric, a critical normalization step\\nwas employed. During training, the cosine similarity calculation itself involves L2-normalizing the feature vectors (ensuring each has'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='was employed. During training, the cosine similarity calculation itself involves L2-normalizing the feature vectors (ensuring each has\\na Euclidean norm of 1) before comparison. Correspondingly, after the embeddings were generated from the trained model, we\\napplied an explicit L2 normalization step to all embedding vectors (ensuring each vector has a Euclidean norm of 1) before feeding\\nthem to the K-means algorithm. This projects all embeddings onto a unit hypersphere, a space where Euclidean distance becomes\\na monotonic function of cosine distance. Consequently, the rank-ordering of distances is preserved, and minimizing one metric is\\nequivalent to minimizing the other, thus formally aligning the clustering process with the learning objective.\\nDataset\\nAs our main goal in this paper is short text clustering in the Persian language we used the Sep_TD_Tel01 dataset [31] which is a'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 7}, page_content='Dataset\\nAs our main goal in this paper is short text clustering in the Persian language we used the Sep_TD_Tel01 dataset [31] which is a\\ncomprehensive Persian text collected from Telegram. This dataset was collected without specific restrictions such as keyword\\nfiltering that makes it a suitable sample of natural data stream from social media. This dataset has been used in other studies on\\nA medium kernel (size 5) can encompass a more complete event or short phrasal topic, like «افزایش قیمت سکه در بازار» (‘increase in coin price in the market’).\\n\\uf054\\nA larger kernel (size 7) is necessary to capture longer-range dependencies within a single clause, where the key relationship spans several words, such as\\nin «بانکی را اعالم کرد بانک مرکزی نرخ بهره بین» (‘The Central Bank announced the interbank interest rate’).\\n\\uf054\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n8/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 8}, page_content='the Persian language including NER [33], event detection [34], text clustering, and topic detection [35,36] which shows its usage\\nand importance in Persian as a low-resource language.\\nIn this dataset, for collecting data, a message collector system was developed at ComInSys (Computerized Intelligence Systems)\\nlab that gathers text from all channels and groups. Approximately 23% of the messages were assigned topic labels, originally\\ncovering 75 distinct clusters which these data were used. After applying constraints that limited texts to 10–30 tokens, the number\\nof clusters was reduced to 44. The Sep_TD_Tel01 dataset was randomly split into train, validation, and test (the ratio is reported at\\nTable 1) using random_seed\\u2009=\\u200973. This seed controlled both data shuffling and model initialization for all experiments. In addition,\\nmore detailed information about the dataset can be found in Table 2.\\nTable 2. Summary of Dataset [31].\\nhttps://doi.org/10.1371/journal.pone.0335709.t002'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 8}, page_content='more detailed information about the dataset can be found in Table 2.\\nTable 2. Summary of Dataset [31].\\nhttps://doi.org/10.1371/journal.pone.0335709.t002\\nData pre-processing.\\nFor pre-processing text in the dataset, as they contain a variety of noise like typos, emojis, URLs, and other artifacts, several pre-\\nprocessing techniques [37] were used to remove these unnecessary elements from text and clean them while preserving\\ninformative content. The specific steps included text normalization, such as converting any embedded English/Latin characters to\\nlowercase, and noise removal, where URLs, HTML tags, user mentions, and emojis were removed. To preserve the original signal,\\nno stop-word removal or spell correction was performed. Also for tokenization, we used a custom tokenizer developed at the\\nComInSys lab, which is specifically designed for the morphological nuances of the Persian language. This pre-processing helps to'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 8}, page_content='ComInSys lab, which is specifically designed for the morphological nuances of the Persian language. This pre-processing helps to\\npreserve informative content. As explained earlier, token-length constraints are applied to text that leads to deleting those that have\\nless than 10 or more than 30 tokens. Figs 2 and 3 show the number of tweets and their respective after preprocessing and\\nnormalization steps. After pre-processing, the resulting clean texts served as the basis for generating the different input\\nrepresentations used in this study. For the experiments utilizing TF-IDF, vectors were generated using a vectorizer configured with\\nsublinear term frequency scaling (sublinear_tf\\u2009=\\u2009True) and a vocabulary limited to the top 300 most frequent features (max_features\\u2009\\n=\\u2009300). Min-max normalization is then performed on these embeddings to reduce the variance. This normalization helps accelerate'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 8}, page_content='=\\u2009300). Min-max normalization is then performed on these embeddings to reduce the variance. This normalization helps accelerate\\ntraining and improves convergence by scaling the data to a common range. The resulting embeddings are fed into the model as\\ninput.\\nFig 2. Tweet counts per cluster before pre-processing.\\nhttps://doi.org/10.1371/journal.pone.0335709.g002\\nFig 3. Tweet counts per cluster after pre-processing.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n9/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 9}, page_content='https://doi.org/10.1371/journal.pone.0335709.g003\\nEvaluation metrics\\nTwo metrics are used to evaluate the proposed method and compare it with other approaches in the text clustering field:\\nUnsupervised Clustering Accuracy (ACC) and Normalized Mutual Information (NMI).\\n1. Unsupervised clustering accuracy (ACC): This metric measures the correspondence between assigned clusters and ground-truth cluster labels. The ACC\\nformally is defined as follows:\\n(6)\\nWhere:\\n2. Normalized mutual information (NMI): For label set T and cluster set C, NMI is defined as:\\n(7)\\nWhere:\\nExperimental results\\nExperiment1: Performance of different recurrent layers in the DSN-STC.\\nIn this experiment, we aim to determine the most effective layer for the recurrent component of the proposed architecture. To this\\nend, different types of recurrent layers including Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 9}, page_content='end, different types of recurrent layers including Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional\\nLSTM (Bi-LSTM), and Bidirectional GRU (Bi-GRU) were implemented and to determine which of these layers can perform better in\\nour short text clustering context. The comparative results are presented in Table 3:\\nTable 3. Comparative Results of Various Recurrent Layers in the Recurrent Component of the DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t003\\nThe results presented in Table 3 indicate that the Bi-LSTM layer is the most suitable choice for our recurrent component. By\\nprocessing each input sequence in both forward and reverse directions, the Bi-LSTM effectively doubles the contextual window\\navailable at every time step, enabling the hidden state to incorporate information from both preceding and succeeding tokens. From\\na theoretical standpoint, this bidirectionality enhances the representational capacity of the recurrent subspace: under the framework'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 9}, page_content='a theoretical standpoint, this bidirectionality enhances the representational capacity of the recurrent subspace: under the framework\\nof sequence modeling, combining forward and backward state vectors increases the expressive power of the network, permitting it\\nto approximate a broader class of sequence‐to‐representation functions. Furthermore, the gated architecture of the LSTM mitigates\\nvanishing‐gradient issues, ensuring that long‐range dependencies (which is critical for disambiguating short texts) are retained in\\nthe learned state dynamics. These properties collectively yield richer, more nuanced feature sets, as necessary to resolve the\\ninherent brevity and lexical ambiguity of short-text inputs. Accordingly, we adopt the Bi-LSTM as the recurrent module in our final\\nDSN-STC configuration.\\nExperiment2: Effect of contrastive‐loss margin.\\nIn Experiment 2, the margin hyperparameter m in Eq 2 was varied from 0.0 to 2.0 in steps of 0.1. For each margin value, DSN-STC'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 9}, page_content='DSN-STC configuration.\\nExperiment2: Effect of contrastive‐loss margin.\\nIn Experiment 2, the margin hyperparameter m in Eq 2 was varied from 0.0 to 2.0 in steps of 0.1. For each margin value, DSN-STC\\nwas trained for 200 epochs with early stopping on validation loss (patience\\u2009=\\u200910). The resulting ACC and NMI were computed on\\nboth training and test splits and the final results are presented in Table 4.\\nN is the total number of data points\\n\\uf054\\ny  is the true cluster label of the i-th data point.\\n\\uf054\\ni\\n is the assigned cluster to the i-th data point\\n\\uf054\\nδ(x,y) is an indicator function equaling 1 if x\\u2009=\\u2009y and 0 otherwise\\n\\uf054\\n illustrates a permutation function that maps each assigned cluster label \\n to the equivalent true cluster label using the Hungarian algorithm [38].\\n\\uf054\\nMI(K, P) is the mutual information among K and P\\n\\uf054\\nE(K) and E(P) is the entropies of K and P respectively.\\n\\uf054\\n is used for normalizing the MI(K, P) to be in range [0, 1]\\n\\uf054\\n1/8/26, 1:04 PM'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 9}, page_content='\\uf054\\nMI(K, P) is the mutual information among K and P\\n\\uf054\\nE(K) and E(P) is the entropies of K and P respectively.\\n\\uf054\\n is used for normalizing the MI(K, P) to be in range [0, 1]\\n\\uf054\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n10/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 10}, page_content='Table 4. Comparative Results of Different Margin Levels in Contrastive Loss Function for Training the DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t004\\nA clear peak in performance was found when the margin m was set to 1.7–1.8 (train ACC\\u2009=\\u20090.7681; test ACC\\u2009=\\u20090.7669; train NMI\\u2009=\\u2009\\n0.9208; test NMI\\u2009=\\u20090.9207). Performance degrades substantially for margins at the extremes: when m\\u2009≤\\u20090.5, clusters remain\\ninsufficiently separated, yielding low ACC and NMI (under-separation), and when m\\u2009≥\\u20091.9, the enforced separation becomes too\\nstrict, causing gradients to vanish and performance to drop (over-separation). Margins in the intermediate range 0.6\\u2009≤\\u2009m\\u2009≤\\u20091.6\\nproduce only moderate gains, indicating partial cluster separation. In the contrastive loss framework [39], setting the margin m too\\nsmall results in insufficient separation between dissimilar pairs, as only distances within m incur a penalty. Conversely, an'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 10}, page_content='small results in insufficient separation between dissimilar pairs, as only distances within m incur a penalty. Conversely, an\\nexcessively large margin causes most dissimilar pairs to lie beyond m, ceasing to provide gradient updates for optimization.\\nTherefore for all other experiments, we used m\\u2009=\\u20091.7. In addition, Fig 4 plots the effect of the contrastive margin m on clustering\\nperformance (ACC and NMI) and complements Table 4 by visualizing the margin sweep. It highlights the region around m\\u2009=\\u20091.7\\nwhere both ACC and NMI peak and supports our selection of m\\u2009=\\u20091.7 for subsequent experiments.\\nFig 4. Evaluation of ACC and NMI as functions of the contrastive‐loss margin.\\nhttps://doi.org/10.1371/journal.pone.0335709.g004\\nExperiment3: Comparative clustering performance on the persian Sep_TD_Tel01 dataset.\\nTo establish a robust set of state-of-the-art baselines for our comparative analysis, we selected several leading Sentence'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 10}, page_content='To establish a robust set of state-of-the-art baselines for our comparative analysis, we selected several leading Sentence\\nTransformer (SBERT) models. Given our focus on the Persian language, we specifically chose widely-adopted multilingual variants\\nto ensure broad language coverage and provide a challenging benchmark. All models were sourced from the HuggingFace\\nrepository which is a comprehensive resource for standardized NLP models. The specific architectures, underlying pre-trained\\nlanguage models, and output embedding dimensions for each of the evaluated SBERT variants are detailed in Table 5.\\nTable 5. Details of sentence transformer models.\\nhttps://doi.org/10.1371/journal.pone.0335709.t005\\nA comprehensive evaluation of the proposed DSN-STC model was conducted on the Persian Sep_TD_Tel01 dataset. Following\\ntoken-length filtering during preprocessing, DSN-STC (with margin\\u2009=\\u20091.7) was compared against both standard and new clustering'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 10}, page_content='token-length filtering during preprocessing, DSN-STC (with margin\\u2009=\\u20091.7) was compared against both standard and new clustering\\ntechniques. For the standard baselines, each was evaluated using four word embedding schemes (TF-IDF, GloVe [40], FastText\\n[41], and ParsBert [42]), while recent deep-clustering algorithms were assessed exclusively using ParsBert embeddings, which are\\npre-trained on Persian text. Clustering performance was quantified via ACC and NMI and evaluated both before and after token-\\nlength filtering to show the effect of the proposed constraint. The results of these comparisons are reported in Table 6.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n11/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 11}, page_content='Table 6. Clustering Performance (ACC, NMI) of Various Implemented Methods on the Sep_TD_Tel01 Dataset, Before and After Token-Length\\nFiltering.\\nhttps://doi.org/10.1371/journal.pone.0335709.t006\\nAs shown in Table 6, imposing the token‐length constraint yielded substantial improvements across all methods, with average\\nabsolute gains of approximately 0.23 in ACC and 0.22 in NMI. One of our key objectives from this experiment is to see if this\\nconstraint can improve the clustering performance or not. Although DSN-STC\\u2009+\\u2009ParsBert already outperformed competing\\napproaches even before filtering, this experiment confirms our first hypothesis: by excluding outlier texts that lack adequate context,\\nclustering coherence is enhanced and overlaps are reduced, leading to marked performance gains for every methods tested.\\nFurthermore, our second hypothesis involved enabling the model to learn a transformation of pre-trained word embeddings into'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 11}, page_content='Furthermore, our second hypothesis involved enabling the model to learn a transformation of pre-trained word embeddings into\\ncluster-aware text representation, such that representations in this new latent space can be clustered more easily and accurately\\nthan in the original space. Indeed, after applying our Siamese constraint, DSN-STC\\u2009+\\u2009ParsBert achieved the best scores (ACC\\u2009=\\u2009\\n0.7669; NMI\\u2009=\\u20090.9207). In other words, by optimizing representations for both cluster membership and pairwise similarity,\\nembeddings are mapped into a cluster-aware latent space where detection is significantly more accurate.\\nIn addition to further contextualize the performance of DSN-STC, we established a strong state-of-the-art baseline by clustering\\nsentence embeddings generated from several pre-trained multilingual SBERT models. The best-performing variant, use-cmlm-\\nmultilingual\\u2009+\\u2009K-means, achieved a test Accuracy of 0.60149 and a test NMI of 0.81583 (Table 6). This comparison is highly'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 11}, page_content='multilingual\\u2009+\\u2009K-means, achieved a test Accuracy of 0.60149 and a test NMI of 0.81583 (Table 6). This comparison is highly\\ninstructive and reveals a dual advantage of our proposed architecture. First, while SBERT embeddings are powerful, they are\\ndesigned for general-purpose semantic representation and are not inherently optimized for the specific cluster structure of a\\ndownstream task. Second, and critically for a low-resource language, these multilingual models exhibit a known weakness on\\nmorphologically rich languages like Persian. Their shared BPE vocabularies frequently split Persian morphemes across subword\\nboundaries, which can obscure root forms and impede the learning of coherent representations for nuanced constructs, a challenge\\nnoted in prior work [45]. In contrast, DSN-STC is architected to address both deficits. The substantial performance gap between the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 11}, page_content='noted in prior work [45]. In contrast, DSN-STC is architected to address both deficits. The substantial performance gap between the\\nSBERT baseline and our DSN-STC\\u2009+\\u2009ParsBert model empirically demonstrates the primary contribution of our work. It confirms that\\nour architecture’s advancement stems not only from its contrastive objective successfully learning a cluster-aware latent space, but\\nalso from its ability to leverage a language-specialized encoder (ParsBert) effectively and overcome the tokenization and\\nrepresentational challenges that general-purpose multilingual models face.\\nThe effectiveness of the token-length constraint and the overall superiority of our model are demonstrated in the results from\\nExperiment 3, which are visualized in Figs 5 and 6. These figures display paired bar charts of ACC and NMI for each method,\\nillustrating the results before and after applying the constraint. Consistent improvements in both metrics were observed across all'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 11}, page_content='illustrating the results before and after applying the constraint. Consistent improvements in both metrics were observed across all\\nembedding techniques following filtering constraint. Notably, DSN-STC\\u2009+\\u2009ParsBert achieved the largest improvements, with ACC\\nincreasing from 0.4604 to 0.7669 and NMI from 0.6581 to 0.9207. Also, these figures also allow for a direct comparison against\\nstrong transformer-based baselines. Even the best-performing SBERT model (use-cmlm-multilingual) achieved a final test accuracy\\nof only 0.60149, substantially underperforming all variants of our DSN-STC model.\\nFig 5. Comparison of ACC Metric Before and After Applying Constraints Across Different Methods.\\nhttps://doi.org/10.1371/journal.pone.0335709.g005\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n12/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 12}, page_content='Fig 6. Comparison of NMI Metric Before and After Applying Constraints Across Different Methods.\\nhttps://doi.org/10.1371/journal.pone.0335709.g006\\nImportantly, DSN-STC outperformed competing methods within each embedding technique, underscoring its robustness in learning\\neffective mapping from word embeddings into text representations. However, the intrinsic quality of those embeddings still\\ninfluences the final latent space: higher-quality original embeddings yield more powerful cluster-aware representations. The charts\\nprovide a concise visual summary of each method’s comparative performance before and after applying filtering constraint.\\nMoreover, DSN-STC already led the field even before filtering, confirming that (1) token-length constraints enhance clustering by\\nremoving uninformative texts and (2) the DSN-STC architecture’s ability to learn cluster-aware representation facilitates more\\naccurate cluster detection.\\nExperiment4: Cross-linguistic generalizability.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 12}, page_content='accurate cluster detection.\\nExperiment4: Cross-linguistic generalizability.\\nAlthough our main focus of this study is to propose a clustering architecture for the Persian language, we also evaluate the\\nrobustness of the proposed method on two English benchmark datasets that are commonly used in clustering approaches,\\nincluding 20newsgroups [46] and AGnews. This experiment allows us to test and evaluate the generalizability and adaptability of\\nour approach in other languages and prepare good comparative results with other models. Table 7 summarizes these results:\\nTable 7. Comparison of DSN-STC Performance (ACC, NMI) Against Various Reported Methods on Standard English Benchmark Datasets.\\nhttps://doi.org/10.1371/journal.pone.0335709.t007\\nAs shown in Table 7, the proposed method consistently outperforms other clustering approaches on English datasets. This not only'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 12}, page_content='https://doi.org/10.1371/journal.pone.0335709.t007\\nAs shown in Table 7, the proposed method consistently outperforms other clustering approaches on English datasets. This not only\\nhighlights the strength and flexibility of the proposed method but also shows the potential of it to improve clustering in other\\nlanguage contexts. The improvements gained can confirm this experiment’s hypothesis that our method can perform better on\\nclustering text not only in Persian but also in other languages. The reason for this is the ability of our method to extract and create\\nrich feature sets from the limited context of the text, which leads to constructing richer cluster-aware representations for text.\\nExperiment5: Component ablation study.\\nTo understand the individual contributions of each architectural component within DSN-STC, we conducted an ablation study that\\nsystematically removes one component at a time and measures its impact on clustering performance. Table 8 reports each variant'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 12}, page_content='systematically removes one component at a time and measures its impact on clustering performance. Table 8 reports each variant\\nof DSN-STC’s ACC and NMI alongside their absolute decreases (ΔACC, ΔNMI) relative to the full DSN-STC model. In this ablation\\nstudy we sought to answer the following research questions:\\nTable 8. Impact of Component Ablation on DSN-STC Performance (Sep_TD_Tel01).\\nhttps://doi.org/10.1371/journal.pone.0335709.t008\\n1. RQ1: Which feature-extraction branch (recurrent vs. convolutional) contributes more to overall clustering performance?\\n2. RQ2: How critical is the multi-scale nature of the convolutional branch (kernel sizes [3, 5, 7] vs. a single size)?\\n3. RQ3: To what extent does the contrastive-loss objective improve cluster separation compared to a standard binary-cross-entropy loss?\\nAs it can be seen in Table 8, every removal causes a clear drop in performance:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 12}, page_content='As it can be seen in Table 8, every removal causes a clear drop in performance:\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n13/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='1. RQ1 finding: Removing the recurrent branch yields ΔACC\\u2009=\\u2009–0.1642 and ΔNMI\\u2009=\\u2009–0.1614, while removing the Conv1D branch yields ΔACC\\u2009=\\u2009–0.1556 and\\nΔNMI\\u2009=\\u2009–0.1504. Both pathways contribute nearly equally, indicating that sequence modeling and local pattern extraction each provide necessary information.\\nThe slightly larger impact of dropping the recurrent branch suggests that long-range dependencies carry marginally more weight in distinguishing clusters of\\nshort text.\\n2. RQ2 finding: Restricting the convolutional branch to a single kernel size incurs the largest drop (ΔACC\\u2009=\\u2009–0.2017; ΔNMI\\u2009=\\u2009–0.1784). This result empirically\\nconfirms our architectural motivation. Multi-scale convolutions capture patterns at varying n-gram windows, from short phrases to longer collocations.\\nRemoving this diversity forces the model to overlook certain granularities of meaning; for instance, a model with only a small kernel might identify key entities'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='Removing this diversity forces the model to overlook certain granularities of meaning; for instance, a model with only a small kernel might identify key entities\\nbut fail to capture the broader phrasal context that defines a cluster’s topic, as explained by the linguistic examples in the Methods section. This shows that\\ncapturing features across multiple scales is crucial for representing short texts effectively. In other words, giving the model the ability to capture local\\ndependencies in different windows allows it to extract much more discriminative features that can find and highlight the important phrases, helping the model\\nto better model the subject of each text and generate final representations more accurately.\\n3. RQ3 finding: Replacing contrastive loss with binary cross-entropy still degrades performance (ΔACC\\u2009=\\u2009–0.0644; ΔNMI\\u2009=\\u2009–0.0807). While binary cross-entropy'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='3. RQ3 finding: Replacing contrastive loss with binary cross-entropy still degrades performance (ΔACC\\u2009=\\u2009–0.0644; ΔNMI\\u2009=\\u2009–0.0807). While binary cross-entropy\\nstill enforces some degree of separation, its relatively smaller ΔACC compared to the other ablations indicates that architectural components (recurrent\\u2009+\\u2009multi-\\nscale conv) can partially structure the embedding space well. However, the contrastive objective’s explicit push–pull mechanism remains essential for\\nmaximizing inter-cluster margins and minimizing intra-cluster variance, delivering the highest cluster cohesion.\\nThese results confirm that each component of DSN-STC is vital for producing high-quality, cluster-aware representations.\\nExperiment6: Hyperparameter sensitivity analysis.\\nTo further validate our architectural choices and assess the model’s stability, we conducted a sensitivity analysis on three key'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='Experiment6: Hyperparameter sensitivity analysis.\\nTo further validate our architectural choices and assess the model’s stability, we conducted a sensitivity analysis on three key\\nhyperparameters: the number of units in the recurrent layer, the combination of kernel sizes in the multi-scale convolutional branch,\\nand the number of filters in each convolutional layer. In these experiments, we varied one hyperparameter at a time while keeping\\nall others fixed to their optimal values as listed in Table 6. The comprehensive results of this analysis are presented in Table 9.\\nTable 9. Sensitivity Analysis of Key Hyperparameters for DSN-STC.\\nhttps://doi.org/10.1371/journal.pone.0335709.t009\\nThe results provide several key insights into the model’s behavior. First, for the Recurrent Units, we observe a clear performance\\npeak at our chosen value of 200. While 100 units also perform well, increasing the capacity to 300 units leads to a notable'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='peak at our chosen value of 200. While 100 units also perform well, increasing the capacity to 300 units leads to a notable\\ndegradation in test performance (from 0.7669 to 0.739 ACC), suggesting the onset of overfitting and confirming that 200 units\\nprovides a robust balance between representational power and generalization.\\nSecond, the analysis of Kernel Sizes empirically validates our multi-scale design hypothesis. The [3, 5, 7] configuration significantly\\noutperforms all other variants. The underperformance of configurations focused on exclusively smaller ([2, 3, 4]) or larger ([5, 7, 9])\\nn-grams indicates that the model must capture patterns across a balanced spectrum of linguistic scales, from tight collocations to\\nlonger phrases, to effectively represent the short texts in our dataset.\\nFinally, the sensitivity to the number of Conv1D filters highlights the importance of model capacity. Using 32 filters provides'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='Finally, the sensitivity to the number of Conv1D filters highlights the importance of model capacity. Using 32 filters provides\\ninsufficient representational power, leading to lower performance. Conversely, increasing the filter count to 128 causes a sharp drop\\nin both Test Accuracy and NMI. This is a classic indicator of overfitting, where the overly complex convolutional branch begins to\\nmodel noise rather than generalizable features, validating our choice of 64 filters as optimal.\\nOverall, this sensitivity analysis demonstrates that the hyperparameters chosen for DSN-STC are not arbitrary but are located\\nwithin a stable and high-performing region of the parameter space, confirming the robustness of our model’s architecture.\\nExperiment7: Computational cost analysis.\\nTo evaluate the practical viability and scalability of our proposed model, we conducted an analysis of its computational cost,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 13}, page_content='Experiment7: Computational cost analysis.\\nTo evaluate the practical viability and scalability of our proposed model, we conducted an analysis of its computational cost,\\nfocusing on training time. For a fair comparison, all experiments were conducted on a Google Colab instance equipped with a 15\\nGB NVIDIA T4 GPU and 12 GB of RAM. We measured the training time for our full DSN-STC model and compared it against its\\nablated variants as well as key transformer-based baselines. The results, presented in Table 10, provide two key insights that\\naddress both the internal cost-benefit of our hybrid design and its external comparison to baselines.\\nTable 10. Training time and performance comparison for DSN-STC and key baselines.\\nhttps://doi.org/10.1371/journal.pone.0335709.t010\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n14/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='First, the analysis of our model’s components is highly informative. The recurrent-only branch (270.39s) is substantially more\\ncomputationally expensive than the convolutional-only branch (114.86s), which is expected due to the sequential nature of recurrent\\noperations. While the full hybrid model (846.38s) requires the most training time, our ablation study (Experiment 5) has already\\ndemonstrated that both branches are essential for achieving optimal clustering performance. This confirms that the combination of\\nthese complementary feature extractors is an effective use of computational resources.\\nSecond, when compared against the strong baselines, DSN-STC demonstrates a highly favorable performance-to-cost trade-off.\\nWhile our model is naturally more computationally intensive than direct clustering on pre-computed embeddings, this increased\\ncost yields a massive improvement in clustering quality—a gain of over 0.16 in Test Accuracy compared to the best SBERT'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='cost yields a massive improvement in clustering quality—a gain of over 0.16 in Test Accuracy compared to the best SBERT\\nbaseline. This substantial performance leap validates our end-to-end, cluster-aware training approach. It confirms that DSN-STC\\nprovides a state-of-the-art accuracy advantage for a reasonable and justifiable increase in computational cost, making it a viable\\nand effective solution.\\nExperiment8: Statistical validation of DSN-STC improvements.\\nTo rigorously assess whether DSN-STC’s performance gains are statistically reliable, paired t-tests (two-tailed, α\\u2009=\\u20090.05) were\\nconducted on N\\u2009=\\u200910 independent runs for both ACC and NMI. Two sets of comparisons were evaluated:\\n1. Token-Length Constraint Impact: DSN-STC trained on the full dataset versus with token-length filtering.\\n2. Baseline Competitors: DSN-STC (filtered) versus each of six baselines, all using ParsBert embeddings.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='2. Baseline Competitors: DSN-STC (filtered) versus each of six baselines, all using ParsBert embeddings.\\nTable 11 summarizes the t-statistics and corresponding p-values for each comparison. All p-values fall below the 0.05 threshold,\\nconfirming that DSN-STC’s improvements in both ACC and NMI are statistically significant across preprocessing conditions and\\nwhen compared to a broad spectrum of clustering methods.\\nTable 11. Paired t-test for DSN-STC performance (ACC and NMI) on the Sep_TD_Tel01 dataset.\\nhttps://doi.org/10.1371/journal.pone.0335709.t011\\nDiscussion\\nIn this study, we propose a novel architecture for clustering short text that has a specialized Siamese network-based model. The\\nkey innovation of our approach lies in its ability to transform textual data from an initial word embedding space into a cluster-aware\\ntext representation latent space that is more efficient for clustering. This transformation that is learned within a Siamese network'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='text representation latent space that is more efficient for clustering. This transformation that is learned within a Siamese network\\nemploying multi-scale hybrid feature extraction enables the model to cluster text data more effectively by generating\\nrepresentations that capture rich, cluster-aware features.\\nSeveral experiments were conducted to evaluate the effectiveness of our approach. In the first experiment, different recurrent\\nneural layers were implemented and compared to see that which of them is the most suitable choice that can extract and model\\nsequential dependencies and features better than others. The comparative experiment showed that the Bi-LSTM layer is the most\\nsuitable choice, outperforming other recurrent layers like LSTM, GRU, and Bi-GRU. This result underscores the importance of\\nbidirectionality in extracting richer and more informative features from text sequences.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='bidirectionality in extracting richer and more informative features from text sequences.\\nIn Experiment 2, we performed a comprehensive margin sweep over the contrastive‐loss parameter m∈ [0,2]. We observed a\\ncharacteristic unimodal performance curve, with insufficient separation at m\\u2009≤\\u20090.5, excessive separation at m\\u2009≥\\u20091.9, and an optimal\\npeak at m\\u2009=\\u20091.7 that confirms that balanced intra-cluster cohesion and inter-cluster separation are essential for maximizing ACC and\\nNMI. In addition, future work could explore dynamic margin selection strategies to further adapt the contrastive objective during\\ntraining.\\nIn Experiment 3, we compared DSN-STC against both standard baselines and recent, advanced clustering approaches on the\\nSep_TD_Tel01 dataset. We focued on 2 main questions for this experiment: “Does applying token-length constraints improve\\nclustering performance?” and “How does our method compare to previous approaches?”. As shown in Table 6, enforcing the token-'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='clustering performance?” and “How does our method compare to previous approaches?”. As shown in Table 6, enforcing the token-\\nlength constraint yielded average gains of 31% in ACC and 27% in NMI. Notably, even before applying these constraints, DSN-STC\\ndelivered improvements of 1.3% in ACC and 2.7% in NMI over competing methods, which shows its intrinsic ability to extract rich,\\ncluster-aware features.\\nThese results demonstrate that DSN-STC effectively learns to map pre-trained word embeddings into a text representation latent\\nspace where cluster overlap is minimized and separability is maximized, thereby facilitating more efficient clustering. In addition, the\\nsuperior performance of DSN-STC\\u2009+\\u2009ParsBert (ACC\\u2009=\\u20090.7669, NMI\\u2009=\\u20090.9207) derives from its capacity to fuse contextualized\\nembeddings with multi-scale hybrid feature extraction. Unlike static embeddings (e.g., GloVe), ParsBert encodes nuanced semantic'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='embeddings with multi-scale hybrid feature extraction. Unlike static embeddings (e.g., GloVe), ParsBert encodes nuanced semantic\\nrelationships in short texts, while our hybrid architecture preserves and combines both long-range dependencies and local n-gram\\npatterns for enhanced cluster discrimination.\\nWhile our primary evaluation targeted Persian text, we also assessed the robustness of DSN-STC on standard English clustering\\nbenchmarks in the experiment 4. These cross-linguistic evaluations demonstrate that our architecture generalizes well beyond\\nPersian, consistently outperforming established clustering methods. Importantly, these results underscore that the quality of the\\nfinal cluster-aware latent space is intrinsically tied to the quality of the initial embedding space. This principle mirrors the findings of\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 14}, page_content='1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n15/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='Moslem et al. [48], who generated synthetic bilingual terminology data with an LLM, fine-tuned a machine-translation model on that\\ndata, and then applied LLM-guided post-editing to enforce domain terms, nearly doubling term integration and demonstrating the\\npower of domain-aware data augmentation for specialized tasks. Consequently, applying DSN-STC to different languages demands\\nhigh-quality embedding techniques, especially contextualized models such as BERT or its language-specific variants, to ensure that\\nthe initial representations encode sufficient syntactic and semantic nuance. Indeed, Rezaei et al. [49]demonstrated across eight\\nsentiment‐analysis benchmarks that the combination of deep architectures and carefully selected word embeddings can lead to up\\nto a 15 point accuracy swing, further underscoring the downstream impact of embedding quality in diverse NLP tasks. Also, Wassie'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='to a 15 point accuracy swing, further underscoring the downstream impact of embedding quality in diverse NLP tasks. Also, Wassie\\net al. recently showed in [50] that fine-tuning open-source large language models on domain-specific corpora yields substantial\\naccuracy gains in specialized translation tasks, underscoring the value of domain-tuned contextual embeddings for high-fidelity\\nrepresentation learning. In practice, leveraging good embeddings for each target language is essential to realize the full potential of\\nour Siamese contrastive framework and to achieve comparable performance gains across diverse linguistic contexts.\\nExperiment 5 (the ablation study) evaluated DSN-STC by removing each core component in turn, including recurrent branch,\\nconvolutional branch, multi-scale kernels, and the contrastive-loss head, and measured the resulting ΔACC and ΔNMI. Every'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='convolutional branch, multi-scale kernels, and the contrastive-loss head, and measured the resulting ΔACC and ΔNMI. Every\\ndeletion caused a substantial performance drop (up to –0.20 in ACC), confirming that all architectural elements are indispensable\\nfor generating high-quality, cluster-aware text representations.\\nRecurrent layers excel at modeling long-range dependencies and capturing topic coherence across a sequence, while\\nconvolutional layers efficiently detect salient n-gram patterns. Their complementary roles explain why removing either branch\\nsignificantly degrades performance. Moreover, restricting the convolutional path to a single kernel size produced the largest\\ndecrease (ΔACC\\u2009=\\u2009–0.20), which highlights the necessity of multi-scale feature extraction. The reason is short texts often contain\\nphrases of varying lengths, such as “climate change” (2-gram) versus “machine learning in healthcare” (4-gram) that multi-scale'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='phrases of varying lengths, such as “climate change” (2-gram) versus “machine learning in healthcare” (4-gram) that multi-scale\\nkernels can adapt to this variability and extract valuable features from diverse n-gram patterns, whereas a single kernel cannot\\ncapture such linguistic diversity.\\nFinally, replacing the contrastive loss with binary cross-entropy (BCE) resulted in a notable drop as well, underscoring that BCE’s\\nindependent treatment of pairs lacks the structural regularization provided by the contrastive objective, which explicitly enforces\\nboth intra-cluster compactness and inter-cluster margins. This dual push–pull mechanism is critical for learning text representations\\nthat faithfully reflect cluster structure. In addition, Experiment 6 employed paired t-tests (at p\\u2009<\\u20090.05) on ACC and NMI gains,\\nconfirming that all observed improvements, including token-length filtering and architectural design, are statistically significant and\\nnot due to chance.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='confirming that all observed improvements, including token-length filtering and architectural design, are statistically significant and\\nnot due to chance.\\nIn addition to previous core performance comparisons and ablation studies, our final experiments were designed to validate the\\nrobustness and practical viability of the DSN-STC architecture. The hyperparameter sensitivity analysis (Experiment 6) confirmed\\nthat our chosen configuration is not fragile; the selected values for recurrent units, kernel sizes, and filter counts reside within a\\nstable performance peak, demonstrating that the model’s strong performance is a robust property of the architecture. Furthermore,\\nthe computational cost analysis (Experiment 7) provided a clear view of the performance-cost trade-offs. It confirmed that DSN-STC\\nprovides a state-of-the-art accuracy advantage for a justifiable increase in training time compared to other methods. It is important'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='provides a state-of-the-art accuracy advantage for a justifiable increase in training time compared to other methods. It is important\\nto note, however, that our exhaustive pairwise training strategy has a time complexity of \\n with respect to the number of\\ntraining samples N. Consequently, while highly effective for datasets of the scale used in this study, applying DSN-STC to\\nsignificantly larger corpora would likely require more sophisticated training pair construction to maintain computational cost\\nefficiency. Taken together, these final analyses validate that DSN-STC is not only effective but also robust and computationally\\npractical for its target application.\\nA further methodological consideration is the approach to model regularization. The contrastive loss function in Equation (3) does\\nnot include an explicit regularization term, such as L2 weight decay, which is often used to penalize model complexity. In this study,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='not include an explicit regularization term, such as L2 weight decay, which is often used to penalize model complexity. In this study,\\nwe instead relied primarily on Early Stopping (with a patience of 10 on the validation loss) as our primary mechanism for preventing\\noverfitting. This technique is a powerful and widely used form of temporal regularization that halts the training process once\\ngeneralization performance on unseen data no longer improves. By selecting the model at its optimal point in the training trajectory,\\nEarly Stopping implicitly prevents the network’s weights from becoming overly specialized to the training set, serving a similar goal\\nto explicit weight decay. Furthermore, the Siamese architecture itself provides a form of structural regularization through its shared\\nweights and encourages generalization. Also, while our results demonstrate that these methods were sufficient for achieving strong'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='weights and encourages generalization. Also, while our results demonstrate that these methods were sufficient for achieving strong\\nperformance, the exploration of explicit weight decay could be a valuable direction for future work.\\nIn the end, it is worth noting a key consideration regarding the scope of this work. While the proposed architecture outperforms\\nother baseline and more advanced recent clustering architectures, the imbalanced nature of the Sep_TD_Tel01 dataset, as seen in\\nFigs 2 and 3, may have influenced the results. Class imbalance and the sparsity of rare classes are known to complicate short-text\\nclustering: minority classes are often under-represented and therefore harder to group reliably. Contrastive and mixup-style\\napproaches have been proposed to mitigate such low-resource challenges [51]. More specifically, prior work has shown that class'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='approaches have been proposed to mitigate such low-resource challenges [51]. More specifically, prior work has shown that class\\nimbalance can bias algorithms toward majority classes and degrade minority-class recovery in both clustering and classification\\ntasks, and that contrastive objectives may benefit from imbalance-aware modifications (e.g., weighted or asymmetric losses,\\nresampling strategies) to avoid deteriorated performance on underrepresented classes [52,53]. Although some clusters in the used\\nPersian dataset are relatively small, we did not apply class-imbalance mitigation (e.g., weighted loss or resampling) in this study\\nbecause our primary goal was to validate the Siamese hybrid architecture.\\nIn conclusion, the results of our experiments confirm the strength and adaptability of the proposed architecture in learning rich,\\ncluster-aware mapping from word embeddings into text representations. Our model demonstrates both effectiveness and'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 15}, page_content='cluster-aware mapping from word embeddings into text representations. Our model demonstrates both effectiveness and\\nadaptability across different languages, providing a robust solution for short text clustering in diverse linguistic contexts.\\nConclusions and future works\\nIn this work, we proposed a novel architecture DSN-STC, based on a Siamese network specifically designed to improve the\\nclustering of Persian short text. Our main idea was to train a model that can transform word embeddings from an initial space which\\nis generated by a pre-trained model, into cluster-aware text representations where clusters can be detected more easily and\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n16/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='1.\\nView Article\\nGoogle Scholar\\n2.\\nView Article\\nGoogle Scholar\\n3.\\n4.\\nView Article\\nPubMed/NCBI\\nGoogle Scholar\\n5.\\nView Article\\nGoogle Scholar\\n6.\\n7.\\n8.\\nView Article\\nGoogle Scholar\\n9.\\n10.\\n11.\\n12.\\n13.\\n14.\\nimprove the clustering by minimizing the overall overlapping between existing clusters. In other words, the representations of text\\nthat are generated by the proposed architecture can capture both the structural and contextual features of each text as well as\\npreserving and improving the cluster-relevant features. So that our model provides a more accurate representations for clustering\\ntasks. The DSN-STC model employs a Siamese network with multi-scale hybrid architecture, consisting of one recurrent layer and\\nthree convolutional neural networks to extract both sequential and local features from the text respectively. These extracted\\nfeatures are then concatenated through fully connected layers to produce the final representation of the input text. During the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='features are then concatenated through fully connected layers to produce the final representation of the input text. During the\\ntraining phase, the Siamese network takes pairs of inputs and then is trained based on the similarity of their cluster assignments,\\nusing a contrast loss function to update the model parameters. In better words, when two inputs belong to the same cluster, they\\nare considered similar, and the model minimizes the distance between them. Conversely, for inputs from different clusters, the\\nmodel maximizes their distance. This loss function, by minimizing the distances for similar data and maximizing those for dissimilar\\ndata, enables our model to learn high-quality, cluster-aware representations that are well-suited for short text clustering. In fact,\\neach short text within its limited context inherently includes sequential and long-range dependencies and also local n-gram patterns'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='each short text within its limited context inherently includes sequential and long-range dependencies and also local n-gram patterns\\nin different windows. By the proposed multi-scale hybrid feature extraction architecture, The proposed multi-scale hybrid feature\\nextractor is designed to maximize the extractable diverse features to highlight the main subject of the text and then give ability to\\nthe model to learn which complementary feature subsets it should use to generate the final text representation based on each\\ncluster. In this way model can learn a cluster-aware mapping from the initial word embedding space into a text representation latent\\nspace and improve the clustering performance. Our experiments show significant improvements in clustering accuracy and NMI\\nmetrics compared to previous methods. Although our main focus was on clustering Persian data, DSN-STC was also evaluated on'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='metrics compared to previous methods. Although our main focus was on clustering Persian data, DSN-STC was also evaluated on\\ncommonly used English dataset, where it continued to show significant improvements over other methods. This cross-linguistic\\nperformance highlights the robustness and adaptability of our model for short text clustering tasks.\\nFor future works, we suggest exploring additional pre-trained embedding models to determine whether they can further improve our\\nmodel’s performance. Also using attention mechanisms in the architecture may provide further improvements in clustering because\\nin this way, model can learn better to attend to which set of features and use them for generating the final representations. A\\nparticularly important direction, as noted in our discussion, is the investigation of imbalance-aware training strategies, such as\\nweighted loss functions or resampling techniques, to potentially improve performance on minority clusters. Finally, while our study'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='weighted loss functions or resampling techniques, to potentially improve performance on minority clusters. Finally, while our study\\nmanaged the inherent noise of the real-world dataset through pre-processing, a valuable future direction involves leveraging these\\nsignals instead of only removing them. Techniques such as incorporating dedicated emoji embeddings or applying robust spell-\\ncorrection could potentially capture additional semantic cues and probably enhance clustering performance. These avenues\\npresent promising opportunities to build upon the strong foundation established by the DSN-STC model.\\nReferences\\nAhmed MH, Tiun S, Omar N, Sani NS. Short Text Clustering Algorithms, Application and Challenges: A Survey. Applied Sciences. 2022;13(1):342.\\nJain AK. Data clustering: 50 years beyond K-means. Pattern Recognition Letters. 2010;31(8):651–66.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='Jain AK. Data clustering: 50 years beyond K-means. Pattern Recognition Letters. 2010;31(8):651–66.\\nEster M, Kriegel H-P, Sander J, Xu X, editors. A density-based algorithm for discovering clusters in large spatial databases with noise. kdd; 1996.\\nXu J, Xu B, Wang P, Zheng S, Tian G, Zhao J, et al. Self-Taught convolutional neural networks for short text clustering. Neural Netw. 2017;88:22–31.\\npmid:28157556\\nSalton G, Buckley C. Term-weighting approaches in automatic text retrieval. Information Processing & Management. 1988;24(5):513–23.\\nLiu K, Bellet A, Sha F, editors. Similarity learning for high-dimensional sparse data. Artificial Intelligence and Statistics. PMLR; 2015.\\nXie J, Girshick R, Farhadi A, editors. Unsupervised deep embedding for clustering analysis. International conference on machine learning. PMLR; 2016.\\nBromley J, Guyon I, LeCun Y, Säckinger E, Shah R. Signature verification using a “siamese” time delay neural network. Advances in neural information'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='Bromley J, Guyon I, LeCun Y, Säckinger E, Shah R. Signature verification using a “siamese” time delay neural network. Advances in neural information\\nprocessing systems. 1993;6.\\nGuo X, Gao L, Liu X, Yin J, editors. Improved deep embedded clustering with local structure preservation. Ijcai; 2017.\\nHadifar A, Sterckx L, Demeester T, Develder C, editors. A self-training approach for short text clustering. Proceedings of the 4th Workshop on\\nRepresentation Learning for NLP (RepL4NLP-2019). 2019.\\nYang B, Fu X, Sidiropoulos ND, Hong M, editors. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. international conference\\non machine learning. PMLR; 2017.\\nGuo X, Zhu E, Liu X, Yin J, editors. Deep embedded clustering with data augmentation. Asian conference on machine learning. PMLR; 2018.\\nDahal P, editor Learning embedding space for clustering from deep representations. 2018 IEEE International Conference on Big Data (Big Data). IEEE;\\n2018.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 16}, page_content='Dahal P, editor Learning embedding space for clustering from deep representations. 2018 IEEE International Conference on Big Data (Big Data). IEEE;\\n2018.\\nYin H, Song X, Yang S, Huang G, Li J, editors. Representation learning for short text clustering. Web Information Systems Engineering–WISE 2021: 22nd\\nInternational Conference on Web Information Systems Engineering, WISE 2021, Melbourne, VIC, Australia, October 26–29, 2021, Proceedings, Part II 22.\\nSpringer; 2021.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n17/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 17}, page_content='15.\\nView Article\\nGoogle Scholar\\n16.\\n17.\\n18.\\n19.\\nView Article\\nGoogle Scholar\\n20.\\nView Article\\nGoogle Scholar\\n21.\\nView Article\\nGoogle Scholar\\n22.\\nView Article\\nGoogle Scholar\\n23.\\n24.\\n25.\\n26.\\nView Article\\nGoogle Scholar\\n27.\\nView Article\\nPubMed/NCBI\\nGoogle Scholar\\n28.\\nView Article\\nGoogle Scholar\\n29.\\n30.\\nView Article\\nGoogle Scholar\\n31.\\n32.\\n33.\\nView Article\\nGoogle Scholar\\n34.\\nView Article\\nGoogle Scholar\\n35.\\nGuan R, Zhang H, Liang Y, Giunchiglia F, Huang L, Feng X. Deep Feature-Based Text Clustering and its Explanation. IEEE Trans Knowl Data Eng.\\n2022;34(8):3669–80.\\nDing X, Mei Y, editors. Research on short text classification method based on semantic fusion and BiLSTM-CNN. 4th International Conference on\\nInformation Science, Electrical, and Automation Engineering (ISEAE 2022). SPIE; 2022.\\nGuo Y, Leng Y, editors. Research on Short Text Clustering Algorithm Combining BERT and BTM. 2024 7th International Conference on Advanced\\nAlgorithms and Control Engineering (ICAACE). IEEE; 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 17}, page_content='Guo Y, Leng Y, editors. Research on Short Text Clustering Algorithm Combining BERT and BTM. 2024 7th International Conference on Advanced\\nAlgorithms and Control Engineering (ICAACE). IEEE; 2024.\\nWang Q, Jin M, Yang N, editors. Short text classification model based on dynamic routing and CNN with attention mechanism. Sixth International\\nConference on Computer Information Science and Application Technology (CISAT 2023). SPIE; 2023.\\nDaneshfar F, Soleymanbaigi S, Nafisi A, Yamini P. Elastic deep autoencoder for text embedding clustering by an improved graph regularization. Expert\\nSystems with Applications. 2024;238:121780.\\nDaneshfar F, Saifee BS, Soleymanbaigi S, Aeini M. Elastic deep multi-view autoencoder with diversity embedding. Information Sciences.\\n2025;689:121482.\\nWang J, Jiang J. Unsupervised deep clustering via adaptive GMM modeling and optimization. Neurocomputing. 2021;433:199–211.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 17}, page_content='2025;689:121482.\\nWang J, Jiang J. Unsupervised deep clustering via adaptive GMM modeling and optimization. Neurocomputing. 2021;433:199–211.\\nGharavi E, Veisi H. Improve document embedding for text categorization through deep siamese neural network. arXiv preprint arXiv:200600572. 2020.\\nde Souza JVA, Oliveira LESE, Gumiel YB, Carvalho DR, Moro CMC, editors. Exploiting siamese neural networks on short text similarity tasks for multiple\\ndomains and languages. International conference on computational processing of the portuguese language. Springer; 2020.\\nVilhagra LA, Fernandes ER, Nogueira BM, editors. Textcsn: a semi-supervised approach for text clustering using pairwise constraints and convolutional\\nsiamese network. Proceedings of the 35th Annual ACM Symposium on Applied Computing; 2020.\\nBilenko M, Basu S, Mooney RJ, editors. Integrating constraints and metric learning in semi-supervised clustering. Proceedings of the twenty-first\\ninternational conference on Machine learning; 2004.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 17}, page_content='Bilenko M, Basu S, Mooney RJ, editors. Integrating constraints and metric learning in semi-supervised clustering. Proceedings of the twenty-first\\ninternational conference on Machine learning; 2004.\\nReimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:190810084. 2019.\\nHochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9(8):1735–80. pmid:9377276\\nCho K. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:14061078. 2014.\\nKim Y, editor. Convolutional Neural Networks for Sentence Classification2014 October. Doha, Qatar: Association for Computational Linguistics.\\nZhang Y, Wallace B. A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification. arXiv preprint\\narXiv:151003820. 2015.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 17}, page_content='Zhang Y, Wallace B. A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification. arXiv preprint\\narXiv:151003820. 2015.\\nRanjbar-Khadivi M, Feizi Derakhshi M, Forouzandeh A, Gholami P, Feizi-Derakhshi A, Zafarani-Moattar E. Sep_TD_Tel01. Mendeley Data.\\n2022;1:10.17632. https://doi.org/10.17632/372rnwf9pc.1\\nAbadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al., editors. {TensorFlow}: a system for {Large-Scale} machine learning. 12th USENIX\\nsymposium on operating systems design and implementation (OSDI 16); 2016.\\nForouzandeh A, Feizi-Derakhshi M-R, Gholami-Dastgerdi P. Persian Named Entity Recognition by Gray Wolf Optimization Algorithm. Scientific\\nProgramming. 2022;2022:1–12.\\nGholami-Dastgerdi P, Feizi-Derakhshi M-R, Salehpour P. SSKG: Subject stream knowledge graph, a new approach for event detection from text. Ain\\nShams Engineering Journal. 2024;15(12):103040.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 17}, page_content='Gholami-Dastgerdi P, Feizi-Derakhshi M-R, Salehpour P. SSKG: Subject stream knowledge graph, a new approach for event detection from text. Ain\\nShams Engineering Journal. 2024;15(12):103040.\\nRanjbar-Khadivi M, Akbarpour S, Feizi-Derakhshi M-R, Anari B. A Human Word Association Based Model for Topic Detection in Social Networks. Ann\\nData Sci. 2024;12(4):1211–35.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n18/19'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 18}, page_content='View Article\\nGoogle Scholar\\n36.\\nView Article\\nGoogle Scholar\\n37.\\nView Article\\nGoogle Scholar\\n38.\\nView Article\\nGoogle Scholar\\n39.\\n40.\\n41.\\nView Article\\nGoogle Scholar\\n42.\\nView Article\\nGoogle Scholar\\n43.\\nView Article\\nGoogle Scholar\\n44.\\nView Article\\nGoogle Scholar\\n45.\\nView Article\\nGoogle Scholar\\n46.\\n47.\\n48.\\n49.\\nView Article\\nGoogle Scholar\\n50.\\nView Article\\nGoogle Scholar\\n51.\\nView Article\\nPubMed/NCBI\\nGoogle Scholar\\n52.\\nView Article\\nGoogle Scholar\\n53.\\nView Article\\nGoogle Scholar\\nZafarani-Moattar E, Kangavari MR, Rahmani AM. Neural Network Meaningful Learning Theory and its Application for Deep Text Clustering. IEEE Access.\\n2024;12:42411–22.\\nMolaei M, Mohamadpur D. Distributed online pre-processing framework for big data sentiment analytics. Journal of AI and Data Mining. 2022;10(2):197–\\n205.\\nKuhn HW. The Hungarian method for the assignment problem. Naval Research Logistics. 1955;2(1–2):83–97.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 18}, page_content='205.\\nKuhn HW. The Hungarian method for the assignment problem. Naval Research Logistics. 1955;2(1–2):83–97.\\nHadsell R, Chopra S, LeCun Y, editors. Dimensionality reduction by learning an invariant mapping. 2006 IEEE computer society conference on computer\\nvision and pattern recognition (CVPR’06). IEEE; 2006.\\nPennington J, Socher R, Manning CD, editors. Glove: Global vectors for word representation. Proceedings of the 2014 conference on empirical methods\\nin natural language processing (EMNLP); 2014.\\nBojanowski P, Grave E, Joulin A, Mikolov T. Enriching Word Vectors with Subword Information. TACL. 2017;5:135–46.\\nFarahani M, Gharachorloo M, Farahani M, Manthouri M. ParsBERT: Transformer-based Model for Persian Language Understanding. Neural Process Lett.\\n2021;53(6):3831–47.\\nHosseini S, Varzaneh ZA. Deep text clustering using stacked AutoEncoder. Multimed Tools Appl. 2022;81(8):10861–81.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 18}, page_content='2021;53(6):3831–47.\\nHosseini S, Varzaneh ZA. Deep text clustering using stacked AutoEncoder. Multimed Tools Appl. 2022;81(8):10861–81.\\nAkram MW, Salman M, Bashir MF, Salman SMS, Gadekallu TR, Javed AR. A Novel Deep Auto-Encoder Based Linguistics Clustering Model for Social\\nText. ACM Trans Asian Low-Resour Lang Inf Process. 2022.\\nPark HH, Zhang KJ, Haley C, Steimel K, Liu H, Schwartz L. Morphology Matters: A Multilingual Language Modeling Analysis. Transactions of the\\nAssociation for Computational Linguistics. 2021;9:261–76.\\nLang K. Newsweeder: Learning to filter netnews. Machine learning proceedings 1995: Elsevier; 1995. p. 331–9.\\nDodda R, Alladi SB. Enhancing Document Clustering with Hybrid Recurrent Neural Networks and Autoencoders: A Robust Approach for Effective\\nSemantic Organization of Large Textual Datasets. EAI Endorsed Transactions on Intelligent Systems and Machine Learning Applications. 2024;1.\\nhttps://doi.org/10.4108/eetismla.4564'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 18}, page_content='Semantic Organization of Large Textual Datasets. EAI Endorsed Transactions on Intelligent Systems and Machine Learning Applications. 2024;1.\\nhttps://doi.org/10.4108/eetismla.4564\\nMoslem Y, Romani G, Molaei M, Kelleher JD, Haque R, Way A, editors. Domain Terminology Integration into Machine Translation: Leveraging Large\\nLanguage Models2023 December; Singapore: Association for Computational Linguistics.\\nRezaei S, Tanha J, Roshan S, Jafari Z, Molaei M, Mirzadoust S, et al. An experimental study of sentiment classification using deep-based models with\\nvarious word embedding techniques. Journal of Experimental & Theoretical Artificial Intelligence. 2024;:1–37.\\nWassie AK, Molaei M, Moslem Y. Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis. arXiv preprint\\narXiv:241205862. 2024.\\nXu Q, Zan H, Ji S. A lightweight mixup-based short texts clustering for contrastive learning. Front Comput Neurosci. 2024;17:1334748. pmid:38348466'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36', 'creationdate': '2026-01-08T07:34:27+00:00', 'source': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\DSN-STC_ Leveraging Siamese networks for optimized short text clustering _ PLOS One.pdf', 'total_pages': 19, 'format': 'PDF 1.4', 'title': 'DSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-08T07:34:27+00:00', 'trapped': '', 'modDate': \"D:20260108073427+00'00'\", 'creationDate': \"D:20260108073427+00'00'\", 'page': 18}, page_content='arXiv:241205862. 2024.\\nXu Q, Zan H, Ji S. A lightweight mixup-based short texts clustering for contrastive learning. Front Comput Neurosci. 2024;17:1334748. pmid:38348466\\nMunguía Mondragón JC, Rendón Lara E, Alejo Eleuterio R, Granda Gutirrez EE, Del Razo López F. Density-Based Clustering to Deal with Highly\\nImbalanced Data in Multi-Class Problems. Mathematics. 2023;11(18):4008.\\nJiang Z, Chen T, Chen T, Wang Z. Improving contrastive learning on imbalanced data via open-world sampling. Advances in Neural Information\\nProcessing Systems. 2021;34:5997–6009.\\n1/8/26, 1:04 PM\\nDSN-STC: Leveraging Siamese networks for optimized short text clustering | PLOS One\\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0335709\\n19/19'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 0}, page_content='1 of 21\\nExpert Systems, 2026; 43:e70182\\nhttps://doi.org/10.1111/exsy.70182\\nExpert Systems\\nORIGINAL ARTICLE\\nOPEN ACCESS\\nMitigating the Negative Transfer in Multi-\\xadTask Learning \\nfor Harmful Language Detection in Spanish and Arabic\\nAngel\\xa0Felipe\\xa0Magnossão\\xa0de\\xa0Paula1,2\\n\\u2002 |  Imene\\xa0Bensalem3,4\\n\\u2002 |  Damiano\\xa0Spina2\\u2002 |  Paolo\\xa0Rosso1,5\\n1Department of Computer Systems and Computation, Universitat Politècnica de València, València, Spain\\u2002 |\\u2002 2School of Computing Technologies, \\nRMIT University, Melbourne, Victoria, Australia\\u2002 |\\u2002 3MISC-\\xadLab, Constantine 2 University, Constantine, Algeria\\u2002 |\\u2002 4ESCF de Constantine, Constantine, \\nAlgeria\\u2002 |\\u2002 5ValgrAI, Valencian Graduate School and Research Network of Artificial Intelligence, València,\\xa0Spain\\nCorrespondence: Angel Felipe Magnossão de Paula (adepau@doctor.upv.es)\\nReceived: 27 September 2025\\u2002 |\\u2002 Revised: 14 November 2025\\u2002 |\\u2002 Accepted: 28 November 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 0}, page_content='Correspondence: Angel Felipe Magnossão de Paula (adepau@doctor.upv.es)\\nReceived: 27 September 2025\\u2002 |\\u2002 Revised: 14 November 2025\\u2002 |\\u2002 Accepted: 28 November 2025\\nKeywords: hate speech\\xa0| multi-\\xadtask learning\\xa0| negative transfer\\xa0| offensive language\\xa0| sexism\\xa0| toxic language\\nABSTRACT\\nNegative transfer continues to limit the benefits of multi-\\xadtask learning (MTL) in harmful language detection, where related \\ntasks must share representations without diluting task-\\xadspecific nuances. We introduce task awareness (TA), a methodological \\nframework that explicitly conditions MTL models on the task they must solve. TA is instantiated through two complementary \\nmechanisms: Task-\\xadaware input (TAI), which augments textual inputs with natural-\\xadlanguage task descriptions, and task embed-\\nding (TE), which learns task-\\xadspecific transformations guided by a task identification vector. Together they enable the encoder'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 0}, page_content='ding (TE), which learns task-\\xadspecific transformations guided by a task identification vector. Together they enable the encoder \\nto disentangle shared and task-\\xaddependent signals, reducing interference during joint optimisation. We integrate TA with BETO \\nand AraBERT encoders and evaluate on six Spanish and Arabic datasets covering sexism, toxicity, offensive language, and hate \\nspeech. Across cross-\\xadvalidation and official train-\\xadtest splits, TA consistently mitigates negative transfer, surpasses single-\\xadtask \\nand conventional MTL baselines, and yields new state-\\xadof-\\xadthe-\\xadart scores on EXIST-\\xad2021, HatEval-\\xad2019, and HSArabic-\\xad2023. The \\nproposed methodology therefore combines a principled architectural innovation with demonstrated practical gains for multilin-\\ngual harmful language detection. The resources to reproduce our experiments are publicly available at https://\\u200bgithub.\\u200bcom/\\u200bAngel\\u200b\\nFelip\\u200beMP/\\u200bArabi\\u200bc-\\xad\\u200bMulti\\u200bTask-\\xad\\u200bLearning.\\n1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fIntroduction'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 0}, page_content='gual harmful language detection. The resources to reproduce our experiments are publicly available at https://\\u200bgithub.\\u200bcom/\\u200bAngel\\u200b\\nFelip\\u200beMP/\\u200bArabi\\u200bc-\\xad\\u200bMulti\\u200bTask-\\xad\\u200bLearning.\\n1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fIntroduction\\nMachine learning applications are widespread, covering areas \\nfrom natural language processing (NLP)—which includes tasks \\nlike named-\\xadentity recognition and automated hate speech detec-\\ntion—to computer vision (CV), enabling systems for object de-\\ntection and classification (Otter et\\xa0al.\\xa02020; Lauriola et\\xa0al.\\xa02022; \\nVoulodimos et\\xa0al.\\xa02018; Jamil et\\xa0al.\\xa02023). Standard practice often \\ninvolves training a dedicated model or ensemble for each task, \\nrefining it until further performance gains are negligible. While \\nthis single-\\xadtask learning (STL) approach frequently produces ac-\\nceptable outcomes, it fails to leverage potential knowledge shar-\\ning from related tasks, which might otherwise improve model \\ngeneralisation. Furthermore, insufficient data can impede the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 0}, page_content='ceptable outcomes, it fails to leverage potential knowledge shar-\\ning from related tasks, which might otherwise improve model \\ngeneralisation. Furthermore, insufficient data can impede the \\ndevelopment of robust models. To overcome these limitations, \\nseveral methods have been proposed for transferring knowledge \\nacross different tasks (Kulis et\\xa0al.\\xa02011; Zhu et\\xa0al.\\xa02023).\\nAn emerging area, multi-\\xadtask learning (MTL) (Ruder\\xa0 2017; \\nAguilar et\\xa0al.\\xa02017; Plaza-\\xaddel-\\xadArco et\\xa0al.\\xa02021, 2021a, 2021b; \\nZhang and Yang\\xa02022; Chen et\\xa0al.\\xa02024), aims to exploit syn-\\nergies between various tasks, potentially lowering require-\\nments for data and computational power. MTL endeavours to \\nenhance generalisation by concurrently training on several \\ntasks. Within MTL using neural networks, two prevalent \\ntechniques are soft (Wu, Fei, and Ji\\xa02020; Wang et\\xa0al.\\xa02022) \\nand hard parameter-\\xadsharing (Fang et\\xa0 al.\\xa0 2022; De Freitas \\net\\xa0al.\\xa02022).'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 0}, page_content='tasks. Within MTL using neural networks, two prevalent \\ntechniques are soft (Wu, Fei, and Ji\\xa02020; Wang et\\xa0al.\\xa02022) \\nand hard parameter-\\xadsharing (Fang et\\xa0 al.\\xa0 2022; De Freitas \\net\\xa0al.\\xa02022).\\nThis is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution and reproduction in any \\nmedium, provided the original work is properly cited and is not used for commercial purposes.\\n© 2026 The Author(s). Expert Systems published by John Wiley & Sons Ltd.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='2 of 21\\nExpert Systems, 2026\\nIn soft parameter-\\xadsharing, task-\\xadspecific networks are employed, \\nwhile cross-\\xadtask communication is facilitated through feature-\\xad\\nsharing methods to encourage parameter similarity. However, \\nscalability can be an issue, as the size of the multi-\\xadtask network \\nincreases linearly with the number of tasks. In contrast, hard \\nparameter-\\xadsharing strategy divides the parameter set into shared \\nand task-\\xadspecific components, often implemented using a shared \\nencoder with multiple task-\\xadspecific decoding heads (Zhang and \\nYang\\xa02022; Chen et\\xa0al.\\xa02024). This approach has the added bene-\\nfit of reducing overfitting (Ruder\\xa02017).\\nThe hard parameter-\\xadsharing framework has been augmented \\nby multilinear relationship networks (Long et\\xa0al.\\xa02017), which \\napply tensor normal priors to the parameters of fully connected \\nlayers. Nevertheless, selecting branching points arbitrarily in \\nthese networks can result in suboptimal task arrangements.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content=\"apply tensor normal priors to the parameters of fully connected \\nlayers. Nevertheless, selecting branching points arbitrarily in \\nthese networks can result in suboptimal task arrangements. \\nTree-\\xadbased structures (Lu et\\xa0al.\\xa02017; Vandenhende et\\xa0al.\\xa02020) \\nhave been suggested to address this issue.\\nDespite these developments, learning multiple tasks concur-\\nrently can sometimes result in negative transfer (Vandenhende \\net\\xa0al.\\xa02022; Wu, Zhang, and Ré\\xa02020). This occurs when shared \\nnoisy information between tasks impairs the model's perfor-\\nmance. Negative transfer signifies a reduction in the model's \\neffectiveness on target tasks due to knowledge transfer (Wu, \\nZhang, and Ré\\xa02020; Vandenhende et\\xa0al.\\xa02022).\\nThis work introduces a novel method to address the negative \\ntransfer challenge, utilising the concept of task awareness \\n(TA) (Magnossão de Paula et\\xa0al.\\xa02023). Our technique allows \\nMTL models to use information about the particular task\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='transfer challenge, utilising the concept of task awareness \\n(TA) (Magnossão de Paula et\\xa0al.\\xa02023). Our technique allows \\nMTL models to use information about the particular task \\nbeing processed, enabling the model to prioritise its internal \\nweights appropriately for each task. In contrast to state-\\xadof-\\xadthe-\\xad\\nart (SOTA) approaches (see Section\\xa02), our method avoids re-\\ncursive structures, thus conserving computational resources \\nand time.\\nEmploying the TA concept, we devised two mechanisms inte-\\ngrated into two distinct MTL TA (MTL-\\xadTA) architectures. The \\ngoal of these architectures is to tackle SOTA difficulties in iden-\\ntifying sexism, toxic language, and hate speech within Spanish \\ntext, as well as sexism, offensive language, and hate speech in \\nArabic comments.\\nExamples illustrating each task in its original language, ac-\\ncompanied by English translations, are provided in Table\\xa0 1. \\nThis table also specifies the source dataset for each text sample.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='Examples illustrating each task in its original language, ac-\\ncompanied by English translations, are provided in Table\\xa0 1. \\nThis table also specifies the source dataset for each text sample. \\nSection\\xa04.1 offers a detailed account of the datasets used.\\nAlthough hate speech, sexism, offensive language, and toxic lan-\\nguage represent related concepts (Poletto et\\xa0al.\\xa02021; Alkomah \\nand Ma\\xa02022; Pachinger et\\xa0al.\\xa02023; Bensalem et\\xa0al.\\xa02024), they \\neach possess unique characteristics and societal consequences. \\nHate Speech typically targets specific demographics (Plaza-\\xaddel-\\xad\\nArco et\\xa0al.\\xa02021, 2021a, 2021b), whereas Sexism relates to gender-\\xad\\nbased discrimination (Frenda et\\xa0al.\\xa02019). Offensive and Toxic \\nLanguage are broader terms encompassing expressions that pro-\\nmote hostility or negativity (Derczynski et\\xa0al.\\xa02024; Magnossão \\nde Paula and Schlicht\\xa02021). The conceptual overlaps between \\nthese tasks are depicted in the Venn diagram (Figure\\xa01). Given'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='mote hostility or negativity (Derczynski et\\xa0al.\\xa02024; Magnossão \\nde Paula and Schlicht\\xa02021). The conceptual overlaps between \\nthese tasks are depicted in the Venn diagram (Figure\\xa01). Given \\ntheir interrelations, creating MTL models capable of identifying \\nthese various forms of harmful language presents a valuable \\nopportunity.\\nAmong the languages most frequently used on social media \\nplatforms like Twitter, Facebook, and TikTok are Spanish \\nand Arabic. For instance, data indicates Spanish ranks as \\nthe third most common language on Twitter, with Arabic \\nfourth Alshaabi et\\xa0 al.\\xa0 (2021). Regrettably, the prevalence of \\na language often correlates with the volume of hostile and \\nharmful content generated in it. We believe this paper is the \\nfirst to put forward effective strategies for reducing negative \\ntransfer across numerous sensitive tasks in both Spanish and \\nArabic. The source code developed for this study is openly \\naccessible.1'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='first to put forward effective strategies for reducing negative \\ntransfer across numerous sensitive tasks in both Spanish and \\nArabic. The source code developed for this study is openly \\naccessible.1\\nThe primary contributions of this research include:\\n•\\t Introduction of task awareness (TA): Our paper introduces \\nthe concept of TA and proposes two unified architectures \\nequipped with TA mechanisms (MTL-\\xadTAI & MTL-\\xadTE) \\nthat can mitigate the negative transfer phenomenon during \\nMTL training.\\n•\\t Development of TA mechanisms: To equip MTL models \\nwith task recognition capabilities, we developed the Task-\\xad\\naware input (TAI) and task embedding (TE) mechanisms, \\naimed at alleviating negative transfer and improving per-\\nformance compared to traditional MTL approaches.\\n•\\t Validation of MTL-\\xadTA models: We evaluated the effective-\\nness of the two TA-\\xadequipped architectures in detecting sex-\\nism, toxic language, and hate speech in Spanish comments,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='•\\t Validation of MTL-\\xadTA models: We evaluated the effective-\\nness of the two TA-\\xadequipped architectures in detecting sex-\\nism, toxic language, and hate speech in Spanish comments, \\nas well as sexism, offensive language, and hate speech in \\nArabic textual comments. The results demonstrate that \\nboth MTL-\\xadTAI and MTL-\\xadTE mitigate negative transfer in \\nthese two languages.2\\n•\\t Achieving SOTA performance: Our approach exceeds \\nSOTA results on established public benchmarks for detect-\\ning sexism (EXIST-\\xad2021) and hate speech (HatEval-\\xad2019). \\nFurthermore, it sets a new SOTA benchmark for the \\nHSArabic-\\xad2023 dataset concerning offensive language \\nidentification, marking considerable advancements over \\nprior techniques.\\nThe novelty of this work lies in its focus on methods and prac-\\ntical applications. We present effective architectural mecha-\\nnisms that implement the TA principle within MTL systems and \\ndemonstrate how these mechanisms can be integrated into real'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='tical applications. We present effective architectural mecha-\\nnisms that implement the TA principle within MTL systems and \\ndemonstrate how these mechanisms can be integrated into real \\nmoderation workflows. While the concept of using task cues to \\nguide shared representations is rooted in the theory of negative \\ntransfer, our key advances are in the practical design, integra-\\ntion, and empirical validation of TAI and TE for detecting harm-\\nful language in Spanish and Arabic.\\nThe rest of this paper is structured as follows. Section\\xa02 reviews \\nrelated work on transfer learning, MTL and its application to \\nHarmful Language detection. Section\\xa03 describes our proposed \\nmethod in detail. Section\\xa0 4 outlines the experimental setup. \\nSection\\xa0 5 presents and analyzes the experimental outcomes.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 1}, page_content='Harmful Language detection. Section\\xa03 describes our proposed \\nmethod in detail. Section\\xa0 4 outlines the experimental setup. \\nSection\\xa0 5 presents and analyzes the experimental outcomes. \\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 2}, page_content='3 of 21\\nExpert Systems, 2026\\nPotential limitations of our method are discussed in Section\\xa06. \\nFinally, Section\\xa07 offers concluding remarks and outlines poten-\\ntial avenues for future investigation.\\n2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fRelated Work\\n2.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fTransfer Learning and Multi-\\xadTask Learning\\nTransfer learning represents a common machine learning \\nmethod, founded on the idea that a model developed for one \\ntask can be improved by integrating knowledge from a related \\ntask (Pan and Yang\\xa02009; Weiss et\\xa0al.\\xa02016; Zhu et\\xa0al.\\xa02023). \\nTraining models entirely from scratch often demands signifi-\\ncant data and computational power; however, situations arise \\nwhere acquiring enough training data is excessively costly or \\ninfeasible. This necessitates creating high-\\xadperforming learn-\\ning systems using more accessible data from alternate tasks. \\nKnowledge transfer methods facilitate enhancing target task \\nperformance by utilising information derived from associ-\\nated tasks. Such methods have seen successful deployment'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 2}, page_content='Knowledge transfer methods facilitate enhancing target task \\nperformance by utilising information derived from associ-\\nated tasks. Such methods have seen successful deployment \\nin diverse machine learning domains, notably NLP (Ruder \\net\\xa0 al.\\xa0 2019; Wang and Mahadevan\\xa0 2011; Prettenhofer and \\nStein\\xa02010; Wang et\\xa0al.\\xa02022) and CV (Duan et\\xa0al.\\xa02012; Kulis \\net\\xa0al.\\xa02011). Closely associated with transfer learning is the \\nMTL framework (Ruder\\xa02017; Zhang and Yang\\xa02022), which \\nTABLE 1\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fExamples of tweets containing sexism, hate speech, toxic language, and offensive language in Spanish and Arabic.\\nTask\\nDataset\\nOriginal language\\nEnglish translation\\nSexism\\nEXIST-\\xad2021\\n@USER Que. rica putita obediente, \\nafortunado tu marido de tener \\nuna mujer como tú, saludos\\n@USER What a nice obedient little whore, your \\nhusband is lucky to have a woman like you, greetings\\nArMI-\\xad2021\\nUSER@ USER@ USER@ The wife takes care of the \\nkitchen, cleanliness and organisation of the house and'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 2}, page_content=\"husband is lucky to have a woman like you, greetings\\nArMI-\\xad2021\\nUSER@ USER@ USER@ The wife takes care of the \\nkitchen, cleanliness and organisation of the house and \\nsits at home, while the husband works for his home.\\nHate speech\\nHatEval-\\xad2019\\nHay varias paginas de feministas a las \\nque deberia darles verguenza exponer sus \\nideas ya que no tienen ni pies ni cabeza\\nThere are several feminist pages that should \\nbe ashamed of themselves for expressing their \\nideas because they have no head or tai\\nHSArabic-\\xad2023\\nRT @USER:\\n \\n https://t.\\u200bco/\\u200bnbSrV\\u200bJdVlm\\u200b\\nRT @USER: #Hezbollah We will shut down \\nyour barking, you stinking, liar terrorist, \\nforever https://t.\\u200bco/\\u200bnbSrV\\u200bJdVlm\\u200b\\nToxic \\nlanguage\\nDETOXIS-\\xad2021\\nEstá claro que vienen los mejores. Haced \\nque pase putos rojos de mierda.\\nIt's clear that the best are coming. Make \\nit happen you fucking Reds\\nOffensive \\nlanguage\\nOSACT-\\xad2022\\nRT @USER:\\n \\n URL\\nRT @USER If Al Arabiya had not reported this \\nnews, we would have been surprised. We are\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 2}, page_content='it happen you fucking Reds\\nOffensive \\nlanguage\\nOSACT-\\xad2022\\nRT @USER:\\n \\n URL\\nRT @USER If Al Arabiya had not reported this \\nnews, we would have been surprised. We are \\nused to its ridiculous news that shows the world \\nthe worst image of the Saudi people URL\\nNote: The original texts are provided alongside their English translations, which were generated using Google Translate.\\nFIGURE 1\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAdaptation of the (Poletto et\\xa0 al.\\xa0 2021; Alkomah and \\nMa\\xa02022) Venn diagrams showing the relationships among sexism, hate \\nspeech, toxic language, and offensive language.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content='4 of 21\\nExpert Systems, 2026\\nseeks to learn multiple, potentially different, tasks in parallel. \\nThe effectiveness of this paradigm stems from its capacity to \\nleverage shared information across tasks. Nevertheless, if the \\ntasks lack sufficient relatedness, negative transfer can occur. \\nThis term describes performance decline resulting from shar-\\ning noisy or inappropriate information between tasks (Wu, \\nZhang, and Ré\\xa02020; Vandenhende et\\xa0al.\\xa02022).\\nRecent studies have focussed on identifying and mitigating \\nnegative transfer in MTL. Li et\\xa0al.\\xa0(2023) introduced a surro-\\ngate modelling approach to predict and partially prevent nega-\\ntive transfer by estimating relevance scores for each task. This \\nmethod significantly improves the accuracy of MTL by selecting \\noptimal task subsets.\\nHierarchical Prompt Learning (HiPro), as proposed by Liu \\net\\xa0al.\\xa0(2023), demonstrates how a hierarchical task-\\xadsharing ap-\\nproach can reduce the risks of negative transfer. By organising'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content='Hierarchical Prompt Learning (HiPro), as proposed by Liu \\net\\xa0al.\\xa0(2023), demonstrates how a hierarchical task-\\xadsharing ap-\\nproach can reduce the risks of negative transfer. By organising \\ntasks into more granular groups based on their relatedness, \\nHiPro constructs a task tree that allows the model to learn both \\nshared and individual task prompts, balancing generalisation \\nwith task-\\xadspecific adaptation.\\nSeveral approaches have been proposed to address nega-\\ntive transfer and balance learning across different tasks. \\nThese include re-\\xadweighting of losses through methods like \\nHomoscedastic uncertainty (Cipolla et\\xa0al.\\xa02018), Gradient nor-\\nmalisation (Chen et\\xa0al.\\xa02018), and Adversarial training (Sinha \\net\\xa0 al.\\xa0 2021), as well as task prioritisation (Guo et\\xa0 al.\\xa0 2018; \\nZhao et\\xa0al.\\xa02018; Sener and Koltun\\xa02018). Additionally, other \\napproaches (Xu et\\xa0 al.\\xa0 2018; Zhang et\\xa0 al.\\xa0 2018, 2019) utilise \\ninitial predictions from multi-\\xadtask networks to iteratively re-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content=\"Zhao et\\xa0al.\\xa02018; Sener and Koltun\\xa02018). Additionally, other \\napproaches (Xu et\\xa0 al.\\xa0 2018; Zhang et\\xa0 al.\\xa0 2018, 2019) utilise \\ninitial predictions from multi-\\xadtask networks to iteratively re-\\nfine each task's output, thereby overcoming the limitations \\nof methods that compute all task outputs simultaneously. \\nHowever, these approaches are often time-\\xadconsuming and re-\\nquire substantial computational resources due to their recur-\\nsive nature.\\nKnight and Duan\\xa0(2023) introduced an innovative framework \\nthat uses summary statistics to address the challenges of MTL \\nin data-\\xadsharing-\\xadconstrained environments, such as health-\\ncare settings. This approach enables efficient model training \\nwithout the need for access to individual-\\xadlevel data, preserv-\\ning privacy while still benefiting from shared information \\nacross tasks.\\nIn the domain of NLP, Chen et\\xa0al.\\xa0(2024) provide an overview \\nthat underscores the importance of MTL in mitigating over-\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content='ing privacy while still benefiting from shared information \\nacross tasks.\\nIn the domain of NLP, Chen et\\xa0al.\\xa0(2024) provide an overview \\nthat underscores the importance of MTL in mitigating over-\\nfitting and addressing data scarcity. The authors review MTL \\narchitectures and optimisation techniques, demonstrating how \\nMTL can leverage related tasks to enhance overall performance \\nacross NLP applications.\\n2.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fMTL In Harmful Language Detection\\nThe initial semi-\\xadsupervised multi-\\xadtask method for Sexism clas-\\nsification was introduced by Abburi et\\xa0al.\\xa0(2020). Their work ad-\\ndressed three tasks utilising labels derived from unsupervised \\nlearning or weak labelling processes. The neural multi-\\xadtask \\narchitecture they designed facilitates shared learning among \\ntasks through common weights and an aggregated loss function, \\nsurpassing several SOTA baselines.\\nWu, Fei, and Ji\\xa0(2020) put forward a novel MTL strategy to con-\\ncurrently manage Aggressive Language Detection (ALD) and'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content='surpassing several SOTA baselines.\\nWu, Fei, and Ji\\xa0(2020) put forward a novel MTL strategy to con-\\ncurrently manage Aggressive Language Detection (ALD) and \\ntext normalisation. They employed a shared encoder for learn-\\ning common inter-\\xadtask features and a task-\\xadspecific encoder for \\ntask-\\xadrelevant features. This configuration led to considerable \\nperformance gains in ALD.\\nAbu Farha and Magdy\\xa0 (2020) proposed CNN-\\xadBiLSTM-\\xadbased \\nmodels trained for three tasks: Hate speech detection, offensive \\nlanguage detection, and sentiment analysis. The authors evalu-\\nated their models using the OSACT2020 (Mubarak et\\xa0al.\\xa02020) \\nArabic dataset, demonstrating that their multi-\\xadtask architecture \\noutperformed traditional monotask models.\\nIn this paper, we introduce two unified architectures designed \\nto identify sexism, toxic language, and hate speech in comments \\nwritten in Spanish, as well as to detect sexism, offensive lan-\\nguage, and hate speech in Arabic textual comments. The de-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content='to identify sexism, toxic language, and hate speech in comments \\nwritten in Spanish, as well as to detect sexism, offensive lan-\\nguage, and hate speech in Arabic textual comments. The de-\\nsigned architectures intend to lessen the negative transfer effect \\nin MTL training, consequently enhancing the identification rate \\nfor harmful content.\\nThe methodology detailed here draws inspiration from mech-\\nanisms suggested by (Abburi et\\xa0al.\\xa02020; Wu, Fei, and Ji\\xa02020). \\nWhile those methods focus on refining the representations \\npassed to task heads to improve MTL models, our TA technique \\ndistinguishes itself. It empowers the model to independently \\nascertain the task it needs to execute. Consequently, MTL-\\xadTA \\nmodels can generate suitable representations for every task \\nhead without needing an auxiliary learning task, enhancing \\nefficiency. The core concept involves learning a task-\\xadpertinent \\nlatent data representation capable of effectively addressing mul-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content='head without needing an auxiliary learning task, enhancing \\nefficiency. The core concept involves learning a task-\\xadpertinent \\nlatent data representation capable of effectively addressing mul-\\ntiple NLP tasks (Wang et\\xa0 al.\\xa0 2022; Indurthi et\\xa0 al.\\xa0 2021). The \\nspecific mechanisms developed are elaborated upon in the sub-\\nsequent section.\\n3\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fProposed Approach\\nThis section details the MTL-\\xadTA models introduced in \\n(Magnossão de Paula et\\xa0al.\\xa02023). We begin by introducing the \\nconcept of TA and explaining its potential in reducing the effects \\nof negative transfer (Vandenhende et\\xa0al.\\xa02022; Wu, Zhang, and \\nRé\\xa0 2020) in multi-\\xadtask joint training (Ruder\\xa0 2017). Following \\nthis, we introduce two specific TA mechanisms designed for in-\\ncorporating task self-\\xadawareness into MTL models.\\nOur methodological innovation is anchored in three design \\nprinciples. First, we expose the encoder to explicit task descrip-\\ntors so that the shared representation can be shaped by both'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 3}, page_content='Our methodological innovation is anchored in three design \\nprinciples. First, we expose the encoder to explicit task descrip-\\ntors so that the shared representation can be shaped by both \\nlinguistic content and the downstream objective. Second, we \\ninterpose a lightweight task-\\xadconditional transformation that \\ncan reconfigure the shared representation before it reaches \\neach task head, thereby curbing interference from unrelated \\ngradients. Third, we ensure both mechanisms can be trained \\nend-\\xadto-\\xadend within standard hard-\\xadparameter-\\xadsharing pipelines, \\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content=\"5 of 21\\nExpert Systems, 2026\\nallowing practitioners to retrofit TA into existing moderation \\nsystems without extensive re-\\xadengineering. The remainder of this \\nsection formalises these ideas and details how they differ from \\nprior MTL formulations.\\nThe most common technique for supervised MTL utilises the \\nhard parameter-\\xadsharing strategy (Zhang and Yang\\xa02022). In \\nthis configuration, the model comprises an encoder along-\\nside N decoders (or task heads), with N representing the \\ncount of tasks the model trains on concurrently (Worsham \\nand Kalita\\xa0 2020). During operation, the encoder takes an \\ninput and produces a task-\\xadneutral latent representation, sub-\\nsequently passed to the designated task head for the final \\nprediction.\\nHowever, a weaker connection between the encoder's generated \\nlatent representation and the specific tasks can impair over-\\nall MTL model efficacy (Vandenhende et\\xa0al.\\xa02022). It is prob-\\nable that the ideal latent representations for identical inputs\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content=\"latent representation and the specific tasks can impair over-\\nall MTL model efficacy (Vandenhende et\\xa0al.\\xa02022). It is prob-\\nable that the ideal latent representations for identical inputs \\nwill differ across various task heads (De Freitas et\\xa0 al.\\xa0 2022). \\nFurthermore, during the training phase, the encoder's represen-\\ntation might develop a bias towards tasks that are more complex \\nor possess larger datasets (Ruder\\xa02017). Such reductions in per-\\nformance exemplify the negative transfer issue (Vandenhende \\net\\xa0al.\\xa02022; Wu, Zhang, and Ré\\xa02020), where a task head gets an \\nunsuitable input representation, hindering its capacity to effec-\\ntively address its assigned task.\\nTo mitigate negative transfer when tackling multiple NLP tasks \\nusing the MTL approach (Zhang and Yang\\xa02022), we propose \\ntwo TA mechanisms. These mechanisms tailor the task heard \\ninput representation based on the specific task being addressed, \\nensuring that the representation sent to each respective head is\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content='two TA mechanisms. These mechanisms tailor the task heard \\ninput representation based on the specific task being addressed, \\nensuring that the representation sent to each respective head is \\noptimised for that task. Furthermore, our proposed MTL model \\ncontinues to benefit from the generalisation improvements pro-\\nvided by multi-\\xadtask joint training. Updates during training apply \\nto the encoder and other MTL model components preceding the \\ntask heads for every task. It is crucial to recognise that all our \\nsuggested MTL models belong to the MTL-\\xadTA category and fol-\\nlow the standard MTL paradigm. Consequently, parameter up-\\ndates only involve the specific task head corresponding to the \\ncurrent input data.\\nThis holistic formulation means that TA augments, rather than \\nreplaces, classic hard-\\xadparameter sharing: practitioners can reuse \\nestablished optimisation recipes while equipping the model \\nwith explicit mechanisms to preserve task-\\xadspecific signals. In'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content=\"replaces, classic hard-\\xadparameter sharing: practitioners can reuse \\nestablished optimisation recipes while equipping the model \\nwith explicit mechanisms to preserve task-\\xadspecific signals. In \\nSection\\xa04 we describe how this design readily scales to hetero-\\ngeneous datasets without bespoke tuning for each task pairing.\\n3.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fTAI\\nThe initial mechanism formulated to embed TA within MTL \\nmodels is the TAI. To help the encoder produce appropriate rep-\\nresentations for every task head, we suggest altering the stan-\\ndard MTL input structure for NLP applications.\\nConcretely, for each sample we concatenate a short natural-\\xad\\nlanguage task description (TD) to the original text snippet and \\nrely on the encoder's positional embeddings to disentangle \\nthe segments. This approach aligns the latent space with task \\nsemantics from the very first layer, steering the encoder to \\nhighlight lexical and syntactic cues that are predictive for the\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content='the segments. This approach aligns the latent space with task \\nsemantics from the very first layer, steering the encoder to \\nhighlight lexical and syntactic cues that are predictive for the \\nspecified task. Unlike prompt-\\xadbased conditioning that often re-\\nquires task-\\xadspecific templates or additional pre-\\xadtraining, TAI \\nuses a uniform schema that can be populated automatically \\nfrom dataset metadata, which makes it robust across languages \\nand label distributions.\\nThe TAI comprises a text snippet (TS) paired with a TD, illus-\\ntrated in Figure\\xa02. The TS represents a segment of text, variable \\nin length based on the task, and usually serves as the primary \\ninput for MTL encoders. The TD is textual information specify-\\ning the particular task managed by a certain head, for instance, \\n“sexism detection” or “hate speech detection”. This adjusted \\ninput supplies context to the encoder, facilitating the creation of \\na task-\\xadfocused representation. An MTL model incorporating the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content='“sexism detection” or “hate speech detection”. This adjusted \\ninput supplies context to the encoder, facilitating the creation of \\na task-\\xadfocused representation. An MTL model incorporating the \\nTAI mechanism is denoted as MTL task-\\xadaware input (MTL-\\xadTAI).\\n3.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fTE\\nThe second mechanism conceived to bestow TA capability \\nupon MTL models is named TE. We propose inserting an extra \\ncomponent situated between the task heads and the encoder, \\ndesignated as the task embedding block (TEB), depicted in \\nFigure\\xa03. This block takes two inputs: (i) the task identification \\nvector (TIV), and (ii) the latent representation from the encoder. \\nThe TIV is constructed as a one-\\xaddimensional one-\\xadhot vector, \\nits length matching the number of task heads. Every position \\nwithin the TIV corresponds to a distinct task head.\\nThe TEB is composed of learning units (LU), each containing a \\nlinear layer succeeded by a ReLU activation function. The quan-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content='within the TIV corresponds to a distinct task head.\\nThe TEB is composed of learning units (LU), each containing a \\nlinear layer succeeded by a ReLU activation function. The quan-\\ntity of LUs acts as a hyperparameter, influenced by factors like \\ntask type and data properties. The primary goal of the TEB is to \\ncraft a fitting representation for the task the MTL model is cur-\\nrently addressing. Consequently, given the same encoder output, \\nthe TEB yields varied outputs contingent on the task. It uses the \\nTIV as a signal to determine for which task the representation \\nshould be generated. As shown in Figure\\xa03, the TIV features a “1” \\nat the index related to the task being processed, while all other \\npositions are zero. An MTL model utilising the TE mechanism is \\nidentified as MTL task embedding (MTL-\\xadTE).\\nThe TEB acts as a learned gating function that re-\\xadweights shared \\nfeatures according to the active task. During training, gradi-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content='identified as MTL task embedding (MTL-\\xadTE).\\nThe TEB acts as a learned gating function that re-\\xadweights shared \\nfeatures according to the active task. During training, gradi-\\nents flowing through the TIV-\\xadconditioned layers encourage the \\nmodel to isolate features that consistently help a given task while \\nsuppressing features that trigger negative transfer. Because the \\nsame parameters are reused across tasks with different activa-\\ntions, TE mechanism promotes parameter efficiency while still \\nenabling task-\\xadspecific specialisation.\\n4\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fExperimental Setup\\nThis section commences by detailing the tasks and datasets em-\\nployed for evaluating our method. Subsequently, it provides im-\\nplementation specifics and reference models. Lastly, it outlines \\nthe experimental configurations.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 4}, page_content='ployed for evaluating our method. Subsequently, it provides im-\\nplementation specifics and reference models. Lastly, it outlines \\nthe experimental configurations.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 5}, page_content='6 of 21\\nExpert Systems, 2026\\n4.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fData\\nIn addition to addressing the negative transfer problem, our \\nresearch focussed on identifying and filtering harmful and \\nabusive content in social media text. Our overarching goal is \\nto promote a respectful and inclusive environment by prevent-\\ning the spread of discriminatory or harmful language online \\nand in other communication channels. We decided to focus on \\nSpanish and Arabic, as these languages are among the most \\nwidely used on social media Alshaabi et\\xa0al.\\xa0(2021). However, \\nthe prevalence of these languages also corresponds with the \\nproduction of hostile and harmful content. Therefore, it is \\ncrucial to make substantial efforts to tackle harmful language \\nbehaviour in Spanish and Arabic, rather than English (Plaza-\\xad\\ndel-\\xadArco et\\xa0 al.\\xa0 2021, 2021a, 2021b). For Spanish, we tackled \\ntasks related to detecting Sexism, Toxic Language and Hate'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 5}, page_content='behaviour in Spanish and Arabic, rather than English (Plaza-\\xad\\ndel-\\xadArco et\\xa0 al.\\xa0 2021, 2021a, 2021b). For Spanish, we tackled \\ntasks related to detecting Sexism, Toxic Language and Hate \\nFIGURE 2\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fMulti-\\xadtask learning (MTL) model including task-\\xadaware input (TAI) mechanism (MTL-\\xadTAI).\\nFIGURE 3\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fMulti-\\xadtask learning (MTL) model including task embedding (TE) mechanism (MTL-\\xadTE).\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content='7 of 21\\nExpert Systems, 2026\\nSpeech. In Arabic, we focussed on detecting Sexism, Offensive \\nLanguage, and Hate Speech. We utilised six datasets, each tai-\\nlored to specific tasks within these languages. For Spanish, \\nwe used the EXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0 al.\\xa0 2021), \\nDETOXIS-\\xad2021 (Taulé et\\xa0al.\\xa02021), and HateEval-\\xad2019 (Basile \\net\\xa0al.\\xa02019) datasets. For Arabic, we employed the ArMI-\\xad2021 \\n(Mulki and Ghanem\\xa02021), HSArabic-\\xad2023, and OSACT-\\xad2022 \\n(Mubarak et\\xa0al.\\xa02022) datasets. Below is a detailed description \\nof each dataset:\\n4.1.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fEXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021)\\nThis dataset originated from the sExism Identification in \\nSocial neTworks (EXIST) shared task during the Iberian \\nLanguages Evaluation Forum (IberLEF) 2021. It comprises \\n11,345 annotated social media posts (in English and Spanish) \\nsourced from Twitter and the uncensored platform Gab.\\u200bcom \\n(Gab). Experts in gender issues supervised and monitored'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content=\"11,345 annotated social media posts (in English and Spanish) \\nsourced from Twitter and the uncensored platform Gab.\\u200bcom \\n(Gab). Experts in gender issues supervised and monitored \\nthe dataset's creation. EXIST represented the inaugural chal-\\nlenge focused on social media Sexism detection, aiming to \\nidentify Sexism broadly, encompassing explicit misogyny to \\nsubtler sexist actions. The task related to Sexism identifica-\\ntion attracted 70 official submissions. It involves binary clas-\\nsification, categorising samples as either Sexist or Not-\\xadSexist. \\nAccuracy served as the official evaluation standard, with data \\npartitioned into training and test sets. Table\\xa0 2 presents the \\ndata breakdown.\\n4.1.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fDETOXIS-\\xad2021 (Taulé et\\xa0al.\\xa02021)\\nData collection for this set occurred for the DEtection of \\nTOxicity in comments In Spanish (DETOXIS) shared task at \\nIberLEF 2021. The task's goal was detecting Toxic Language \\nwithin comments responding to online news articles concern-\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content=\"TOxicity in comments In Spanish (DETOXIS) shared task at \\nIberLEF 2021. The task's goal was detecting Toxic Language \\nwithin comments responding to online news articles concern-\\ning immigration. The annotation method developed aimed at \\nreducing subjectivity in toxicity labeling by considering con-\\ntext (like linguistic cues and conversation threads). The data \\nannotation team comprised trained annotators and linguistics \\nexperts. The collection contains 4354 text comments respond-\\ning to various articles from Spanish online news sources (e.g., \\nABC, elDiario.es, El Mundo, NIUS) and discussion platforms \\n(like Menéame). The task involves a binary classification, as-\\nsigning samples to either Toxic or Not-\\xadToxic categories. Over 30 \\nteams assessed their machine learning models using this data-\\nset during the DETOXIS shared task participation. The F1-\\xadscore \\nfor the Toxic class was the official evaluation measure, and the\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content='teams assessed their machine learning models using this data-\\nset during the DETOXIS shared task participation. The F1-\\xadscore \\nfor the Toxic class was the official evaluation measure, and the \\ndataset was split into training and testing portions. The data dis-\\ntribution is shown in Table\\xa03.\\n4.1.3\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fHatEval-\\xad2019 (Basile et\\xa0al.\\xa02019)\\nThis dataset was assembled for the Detection of Hate Speech \\nAgainst Immigrants and Women in Twitter (HatEval) task, a com-\\nponent of the SemEval 2019 workshop. It includes 19,600 tweets \\n(English and Spanish) with labels for hate speech detection. The \\ncollection process involved several gathering techniques: (i) ob-\\nserving accounts likely targeted by hate; (ii) obtaining records \\nfrom known haters; (iii) applying keyword filters to Twitter feeds. \\nAnnotation involved experts and crowdsourced workers verified \\nfor annotation reliability. The task required binary classification, \\nassociating samples with hateful or not-\\xadhateful labels. The data-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content=\"Annotation involved experts and crowdsourced workers verified \\nfor annotation reliability. The task required binary classification, \\nassociating samples with hateful or not-\\xadhateful labels. The data-\\nset contains training, development, and test partitions. The offi-\\ncial metric was F1-\\xadmacro (the unweighted average F1-\\xadscore across \\nboth classes). HatEval ranked among SemEval 2019's most partic-\\nipated tasks, receiving over 100 submissions for hate speech detec-\\ntion. Table\\xa04 details the dataset's composition.\\n4.1.4\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fArMI-\\xad2021 (Mulki and Ghanem\\xa02021)\\nThe dataset served the Arabic Misogyny Identification shared \\ntask (ArMI), part of the hate speech and offensive content de-\\ntection (HASOC) track at FIRE-\\xad2021. It comprises 9833 tweets \\nin formal Arabic and various dialects, including Levantine, Gulf, \\nand Egyptian. These tweets were gathered using different expres-\\nsions and hashtags related to anti-\\xadwomen topics, as well as from\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content='in formal Arabic and various dialects, including Levantine, Gulf, \\nand Egyptian. These tweets were gathered using different expres-\\nsions and hashtags related to anti-\\xadwomen topics, as well as from \\nthe accounts of seven female journalists who were active during \\nthe Lebanon protests in October 2019. The annotators labelled \\nthe tweets for sexism identification (a binary task) and sexism \\ncategorization (a multi-\\xadclass task). The challenge received 15 of-\\nficial runs. The official evaluation metric was accuracy, and data \\nwas split into training and test sets. The dataset statistics are pre-\\nsented in Table\\xa05, with our focus exclusively on the binary task.\\n4.1.5\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fHSArabic-\\xad2023\\nThis dataset was created by Hamad Bin Khalifa University \\nand Carnegie Mellon University in Qatar as part of a research \\nproject. It contains more than 15,000 tweets in different Arabic \\ndialects. Annotators from various Arabic-\\xadspeaking countries'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 6}, page_content='and Carnegie Mellon University in Qatar as part of a research \\nproject. It contains more than 15,000 tweets in different Arabic \\ndialects. Annotators from various Arabic-\\xadspeaking countries \\nin the Middle East and North Africa labelled the tweets. The \\nTABLE 2\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fEXIST-\\xad2021 data distribution (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021).\\nTraining\\nTest\\nSpanish\\nEnglish\\nSpanish\\nEnglish\\nTwitter\\nTwitter\\nTwitter\\nGab\\nTwitter\\nGab\\nSexist\\n1741\\n1636\\n858\\n265\\n858\\n300\\nNot-\\xadSexist\\n1800\\n1800\\n812\\n225\\n858\\n192\\nTABLE 3\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fDETOXIS-\\xad2021 data distribution (Taulé et\\xa0al.\\xa02021).\\nTraining\\nTest\\nToxic\\n1147\\n239\\nNot-\\xadtoxic\\n2316\\n652\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content='8 of 21\\nExpert Systems, 2026\\nannotations cover different tasks, including Offensive Language \\ndetection, which we considered in our experiments. An up-\\ndated version of the dataset is described in the paper by (Charfi \\net\\xa0al.\\xa02024). We divided the dataset into training and test sub-\\nsets, as shown in Table\\xa06 and adopted F1-\\xadmacro as an official \\nevaluation metric.\\n4.1.6\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fOSACT-\\xad2022 (Mubarak et\\xa0al.\\xa02022)\\nThis dataset was utilised in the OSACT 2022 shared task on \\nArabic offensive language and hate speech detection. It com-\\nprises 12,698 examples collected from Twitter using a predefined \\nlist of emojis frequently associated with offensive texts. The \\ntweets were labelled via a crowdsourcing platform for three sub-\\ntasks: Offensive language detection (binary task), Hate Speech \\ndetection (binary task), and fine-\\xadgrained hate speech detection \\n(multi-\\xadclass task). The data is composed of training, develop-\\nment, and test sets. In total, 40 teams signed up to participate in'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content='detection (binary task), and fine-\\xadgrained hate speech detection \\n(multi-\\xadclass task). The data is composed of training, develop-\\nment, and test sets. In total, 40 teams signed up to participate in \\nthe offensive language detection task, and the official evaluation \\nmetric was the F1-\\xadmacro. Our analysis focuses on the binary an-\\nnotations, with statistics detailed in Table\\xa07.\\n4.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fImplementation Details\\nThe encoder employs a bidirectional encoder representation \\nfrom transformers (BERT) (Devlin et\\xa0al.\\xa02019), pre-\\xadtrained in \\nthe language of the applied task data. We utilised the most \\npopular BERT versions for each language: BERT for Spanish \\ntransformers (BETO) (Canete et\\xa0 al.\\xa0 2020) and Arabic BERT \\n(AraBERT) (Antoun et\\xa0 al.\\xa0 2020). Following the BERT en-\\ncoding, we applied both max pooling and mean pooling \\ncalculations to its output. These BERT models consist of 12 \\nself-\\xadattention layers, each with 12 attention heads, and a hid-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content='coding, we applied both max pooling and mean pooling \\ncalculations to its output. These BERT models consist of 12 \\nself-\\xadattention layers, each with 12 attention heads, and a hid-\\nden size of 768 dimensions, totaling approximately 110 mil-\\nlion parameters.\\nThe respective encoder (BETO or AraBERT) handles a text se-\\nquence, yielding a hidden representation per token equivalent \\nto the 768 hidden size dimensions. Concatenating the max and \\nmean pooling results derived from the full sequence of encoder \\noutput tokens forms the latent encoder representation. Within \\nthe TE method, the TEB preserves the precise dimensionality of \\nthis latent encoder representation.\\nTask heads function as linear classifiers. Their input dimensions \\nalign with the latent encoder representation, while output di-\\nmensions vary by task. For binary classification tasks, the linear \\nclassifier outputs two values; the larger value determines the pre-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content=\"align with the latent encoder representation, while output di-\\nmensions vary by task. For binary classification tasks, the linear \\nclassifier outputs two values; the larger value determines the pre-\\ndicted class. Additionally, task descriptions (TDs) for each data-\\nset were formulated by appending ‘detection’ to the task name; \\nfor example, for EXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021), the \\nTD used was “Sexism Detection”.\\nModel training utilised the AdamW optimizer (Loshchilov and \\nHutter\\xa02019), incorporating a linear decay learning rate sched-\\nule spanning from 5e-\\xad6 to 1e-\\xad4. Training involved 15 epochs, a \\ndropout rate of 0.3, and a batch size of 64. We tested configura-\\ntions using 1, 2, and 3 LUs. Adopting an approach akin to early \\nstopping (Caruana et\\xa0al.\\xa02000), the model demonstrating best \\nperformance on the task's official metric was chosen.\\n4.3\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fComparison Models\\nOur method is compared against two model categories: (i)\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content=\"performance on the task's official metric was chosen.\\n4.3\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fComparison Models\\nOur method is compared against two model categories: (i) \\nBaseline models and (ii) SOTA models. We specifically imple-\\nmented two baseline types: STL and MTL models. These base-\\nlines are essential for determining whether negative transfer \\noccurred during the training of the classic MTL model, as re-\\nvealed by comparing the models' performance on the test data. \\nNegative transfer is identified when the classic MTL model per-\\nforms worse—according to the chosen evaluation metric—than \\nthe STL model. In such cases, we further evaluate the perfor-\\nmance of the MTL Task-\\xadAware (MTL-\\xadTA) models against the \\nclassic MTL model to determine whether our proposed solu-\\ntions effectively address the negative transfer issue by achiev-\\ning superior results. Below there is a detailed description of the \\ntwo modes:\\nTABLE 4\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fHatEval-\\xad2019 data distribution (Basile et\\xa0al.\\xa02019).\\nTraining\\nDevelopment\\nTest\\nSpanish\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content='ing superior results. Below there is a detailed description of the \\ntwo modes:\\nTABLE 4\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fHatEval-\\xad2019 data distribution (Basile et\\xa0al.\\xa02019).\\nTraining\\nDevelopment\\nTest\\nSpanish\\nEnglish\\nSpanish\\nEnglish\\nSpanish\\nEnglish\\nHate\\n1741\\n1636\\n1741\\n1636\\n858\\n300\\nNot-\\xadhate\\n1800\\n1800\\n1800\\n1800\\n812\\n192\\nTABLE 5\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fArMI-\\xad2021 data distribution (Mulki and Ghanem\\xa02021).\\nTraining\\nTest\\nSexist\\n4805\\n1201\\nNot-\\xadsexist\\n3061\\n766\\nTABLE 6\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fHSArabic-\\xad2023 data distribution.\\nTraining\\nTest\\nOffensive\\n2234\\n559\\nNot-\\xadoffensive\\n10,057\\n2514\\nTABLE 7\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fOSACT-\\xad2022 data distribution (Mubarak et\\xa0al.\\xa02022).\\nTraining\\nDevelopment\\nTest\\nHate\\n959\\n109\\n271\\nNot-\\xadhate\\n7928\\n1161\\n2270'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 7}, page_content='Training\\nTest\\nOffensive\\n2234\\n559\\nNot-\\xadoffensive\\n10,057\\n2514\\nTABLE 7\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fOSACT-\\xad2022 data distribution (Mubarak et\\xa0al.\\xa02022).\\nTraining\\nDevelopment\\nTest\\nHate\\n959\\n109\\n271\\nNot-\\xadhate\\n7928\\n1161\\n2270\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content=\"9 of 21\\nExpert Systems, 2026\\n•\\t MTL refers to the standard MTL model. Its construction \\nmirrors the MTL-\\xadTA model architecture (see Section\\xa03) but \\nlacks the TAI mechanism. Consequently, the MTL model \\nprocesses only the TS as input.\\n•\\t STL designates the standard STL model. While sharing the \\nMTL model's architecture, it features just one task head. \\nTherefore, comparing this model type against MTL mod-\\nels necessitates training a separate model for every task \\naddressed.\\nSOTA models signify the top-\\xadperforming methods currently avail-\\nable for the datasets included in our experiments. Comparing the \\nperformance of classic MTL, SOTA, and MTL-\\xadTA models pro-\\nvides valuable insights into how effectively a simple MTL model \\ncan approach SOTA results when negative transfer is mitigated. \\nBelow is a comprehensive overview of the SOTA models:\\n•\\t AI-\\xadUPV (Magnossão de Paula et\\xa0al.\\xa02021): A deep learn-\\ning architecture leveraging a combination of different\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content='Below is a comprehensive overview of the SOTA models:\\n•\\t AI-\\xadUPV (Magnossão de Paula et\\xa0al.\\xa02021): A deep learn-\\ning architecture leveraging a combination of different \\nTransformer models (Vaswani et\\xa0al.\\xa02017). It capitalises on \\nensemble techniques and incorporates data augmentation \\nduring training. This model holds the SOTA position for \\nEXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021).\\n•\\t SINAI (Plaza-\\xaddel-\\xadArco et\\xa0al.\\xa02021, 2021a, 2021b): A BERT \\nbase model (Devlin et\\xa0al.\\xa02019) trained via the MTL hard \\nparameter-\\xadsharing approach. Despite covering five tasks \\nand six datasets, its primary focus was Toxic Language \\ndetection, utilising other tasks as auxiliary support. It rep-\\nresents the SOTA for DETOXIS-\\xad2021 (Taulé et\\xa0al.\\xa02021).\\n•\\t Atalaya (Pérez and Luque\\xa0 2019): This model employs \\nSupport Vector Machines (Boser et\\xa0 al.\\xa0 1992). Training in-\\nvolved multiple representations derived from FastText \\n(Bojanowski et\\xa0 al.\\xa0 2017) sentiment-\\xadfocused word vectors,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content='Support Vector Machines (Boser et\\xa0 al.\\xa0 1992). Training in-\\nvolved multiple representations derived from FastText \\n(Bojanowski et\\xa0 al.\\xa0 2017) sentiment-\\xadfocused word vectors, \\nincluding tweet embeddings (Mikolov et\\xa0 al.\\xa0 2013), bag-\\xad\\nof-\\xadcharacters (Bojanowski et\\xa0 al.\\xa0 2017), and bag-\\xadof-\\xadwords \\n(Blizard\\xa01988). It is recognised as the SOTA for HatEval-\\xad2019 \\n(Basile et\\xa0al.\\xa02019).\\n•\\t UM6P-\\xadNLP (Mahdaouy et\\xa0 al.\\xa0 2021): This approach uti-\\nlises MARBERT (Abdul-\\xadMageed and Elmadany\\xa0 2021), a \\nlanguage model pre-\\xadtrained on a corpus of 1 billion tweets \\nspanning various Arabic dialects. It uses a multi-\\xadtask meth-\\nodology, formulated to address both binary and multiclass \\nclassification tasks within the ArMI-\\xad2021 shared task \\n(Mulki and Ghanem\\xa02021), where it secured SOTA status.\\n•\\t GOF (Mostafa et\\xa0 al.\\xa0 2022): This method uses an ensem-\\nble strategy based on majority voting among three pre-\\xad\\ntrained models: QARiB (Abdelali et\\xa0al.\\xa02021), MARBERT,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content=\"•\\t GOF (Mostafa et\\xa0 al.\\xa0 2022): This method uses an ensem-\\nble strategy based on majority voting among three pre-\\xad\\ntrained models: QARiB (Abdelali et\\xa0al.\\xa02021), MARBERT, \\nand MERBERT v2 (Abdul-\\xadMageed and Elmadany\\xa0 2021). \\nOptimization for each model involved a distinct loss func-\\ntion. It stands as the SOTA for OSACT-\\xad2022 (Mubarak \\net\\xa0al.\\xa02022).\\n4.4\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fEvaluation Setup\\nTwo experiments were performed to assess our TA method's ef-\\nficacy in reducing negative transfer (Vandenhende et\\xa0al.\\xa02022; \\nWu, Zhang, and Ré\\xa02020), detailed below.\\n4.4.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fCross-\\xadValidation Experiment\\nTo determine if the TAI and TE mechanisms could decrease \\nnegative transfer in MTL training contexts, a cross-\\xadvalidation \\nprocedure was executed. For every dataset mentioned in \\nSubsection\\xa04.1, the constituent sets were merged into one con-\\nsolidated set. Subsequently, 5-\\xadfold cross-\\xadvalidation was applied \\nto the STL, MTL, MTL-\\xadTAI, and MTL-\\xadTE models.\\n4.4.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fOfficial Training-\\xadTest Split\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content='solidated set. Subsequently, 5-\\xadfold cross-\\xadvalidation was applied \\nto the STL, MTL, MTL-\\xadTAI, and MTL-\\xadTE models.\\n4.4.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fOfficial Training-\\xadTest Split\\nFor comparison against SOTA models (Magnossão de Paula \\net\\xa0 al.\\xa0 2021; Plaza-\\xaddel-\\xadArco et\\xa0 al.\\xa0 2021, 2021a, 2021b; Pérez \\nand Luque\\xa02019; Mahdaouy et\\xa0al.\\xa02021; Mostafa et\\xa0al.\\xa02022) \\nrelevant to the datasets, an experiment was run using the \\nofficial train-\\xadtest partitions provided with these datasets. \\nHSArabic-\\xad2023 was the sole exception due to being a single \\npartition; for this, we performed a stratified 80/20 split into \\ntraining and test sets. Model training utilised the designated \\ntraining set, or a combination of training and development \\nsets if available. Post-\\xadtraining, model evaluation occurred on \\nthe test partitions.\\nFor both experimental setups, only Spanish or Arabic data sam-\\nples were employed. Models were assessed using the official \\nmetrics specific to each dataset (as described in Section\\xa04.1). In'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content='For both experimental setups, only Spanish or Arabic data sam-\\nples were employed. Models were assessed using the official \\nmetrics specific to each dataset (as described in Section\\xa04.1). In \\npractice, three evaluation metrics were required. Accuracy—used \\nfor EXIST-\\xad2021 and ArMI-\\xad2021—measures the proportion of cor-\\nrectly classified texts across both labels, which is appropriate for \\nthe moderately balanced sexism datasets. The DETOXIS-\\xad2021 and \\nHSArabic-\\xad2023 benchmark specifies the F1-\\xadscore for the Toxic \\nclass. We therefore compute precision and recall for that class \\nand report their harmonic mean to reflect performance on the \\nminority label. HatEval-\\xad2019 and OSACT-\\xad2022 rely on F1-\\xadmacro, \\ndefined as the average of the per-\\xadclass F1-\\xadscores, so that the score \\nweights positive and negative classes equally, despite dataset im-\\nbalance. When aggregating results across tasks we keep the metric \\nrequired by each dataset, ensuring that comparisons remain faith-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content='weights positive and negative classes equally, despite dataset im-\\nbalance. When aggregating results across tasks we keep the metric \\nrequired by each dataset, ensuring that comparisons remain faith-\\nful to the official evaluation protocols.\\nAll metrics are computed on the corresponding validation or test \\nsplits for every fold or official partition. During cross-\\xadvalidation, \\nwe calculate the metric on each fold before averaging them to \\nobtain the reported values. For the training-\\xadtest experiments, \\nthe metric is derived once on the held-\\xadout test portion. This con-\\nsistent procedure allows us to attribute performance differences \\ndirectly to the presence or absence of the TA mechanisms. We \\ninvestigated MTL model versions combining two tasks and ver-\\nsions combining three tasks. The 95% confidence interval for \\nresults was computed via the formula:\\nwhere value denotes the obtained evaluation metric score (like \\naccuracy or F1-\\xadscore), nsample signifies the test set size, and'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 8}, page_content='results was computed via the formula:\\nwhere value denotes the obtained evaluation metric score (like \\naccuracy or F1-\\xadscore), nsample signifies the test set size, and \\nZ = 1.96 relates to the 95% confidence level. This calculation \\nquantifies the uncertainty associated with the reported perfor-\\nmance figures by supplying confidence intervals.\\nMargin of Error = Z ×\\n√\\nvalue × (1 −value)\\nnsample\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content=\"10 of 21\\nExpert Systems, 2026\\n5\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fResults and Analysis\\nThis section reports the outcomes of the experiments and con-\\ntrasts the performance of the models evaluated (detailed in \\nSection\\xa04). The results are displayed in two table formats. The \\nexperimental results (large tables) are organised into three parts: \\nmodel type, model's task heads, and model's performance. The \\naggregated experimental results (small tables) are organised into \\ntwo sections: model type and aggregated task heads' performance. \\nBold values indicate the highest values among all analysed mod-\\nels (column), while underlined values indicate the highest values \\namong the MTL models. We also include bar charts showcasing \\nthe best result of each model for every dataset.\\n5.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fCross-\\xadValidation Experiment\\n5.1.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fSpanish\\nThe outcomes from the Spanish cross-\\xadvalidation experiment \\nare presented in Table\\xa08. Analysis of the Baseline models (out-\\nlined in Section\\xa04.3) indicates the conventional MTL model ex-\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content=\"The outcomes from the Spanish cross-\\xadvalidation experiment \\nare presented in Table\\xa08. Analysis of the Baseline models (out-\\nlined in Section\\xa04.3) indicates the conventional MTL model ex-\\nperienced negative transfer in almost all scenarios. Relative to \\nthe STL model, the MTL model demonstrated improvements \\nonly in the Sexism detection task under two conditions: when \\ntrained jointly for Sexism and Hate-\\xadspeech detection, and when \\ntrained across all three tasks. For every other configuration, the \\nSTL model yielded better performance. This suggests that neg-\\native transfer likely impeded the MTL model's learning process \\nin those instances. Our findings suggest the TA mechanisms \\nsuccessfully reduced negative transfer. As detailed in Table\\xa08, \\nboth the MTL-\\xadTAI model (using the TAI mechanism) and the \\nMTL-\\xadTE model (using the TE mechanism) achieved consistently \\nsuperior performance compared to the standard MTL model. \\nFurthermore, the MTL-\\xadTAI and MTL-\\xadTE models surpassed\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content=\"MTL-\\xadTE model (using the TE mechanism) achieved consistently \\nsuperior performance compared to the standard MTL model. \\nFurthermore, the MTL-\\xadTAI and MTL-\\xadTE models surpassed \\nthe STL model's performance across the three assessed tasks. \\nBetween the two TA models, the MTL-\\xadTE generally performed \\nbetter than the MTL-\\xadTAI model.\\nFigure\\xa04 compares the top results of each model across the datasets in \\nthe Spanish cross-\\xadvalidation experiment. Negative transfer is evident \\nin bar charts (b) DETOXIS-\\xad2021 and (c) HatEval-\\xad2019, where the STL \\nmodel outperforms the classic MTL model. However, the MTL-\\xadTA \\nmodels (MTL-\\xadTAI and MTL-\\xadTE) effectively mitigate this negative \\ntransfer and outperform the classic MTL model. Additionally, the \\nMTL-\\xadTA models also surpass the classic MTL model in bar chart (a) \\nEXIST-\\xad2021. This indicates that negative transfer may have influ-\\nenced the learning process of the MTL model, but not to the extent \\nthat it performs worse than the STL model.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content=\"EXIST-\\xad2021. This indicates that negative transfer may have influ-\\nenced the learning process of the MTL model, but not to the extent \\nthat it performs worse than the STL model.\\nTable\\xa09 presents the aggregated results of the MTL models for \\nthe Spanish cross-\\xadvalidation experiment. The classic MTL model \\nperformed poorly, achieving the lowest aggregated results across \\nall task combinations. This outcome is likely attributable to the \\nnegative transfer effect hindering the model's learning capabil-\\nity. In contrast, the MTL-\\xadTAI model achieved the highest aggre-\\ngated results for Toxic-\\xadlanguage and Hate-\\xadspeech detection. The \\nMTL-\\xadTE model obtained the highest aggregated results for all \\nother task combinations. Consistent with the results in Table\\xa08, \\nthe TAI and TE mechanisms lessen the negative transfer effect \\nduring MTL training. As a result, the MTL-\\xadTAI and MTL-\\xadTE \\nmodels outperform the traditional MTL model in all cases. These\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content='the TAI and TE mechanisms lessen the negative transfer effect \\nduring MTL training. As a result, the MTL-\\xadTAI and MTL-\\xadTE \\nmodels outperform the traditional MTL model in all cases. These \\nTABLE 8\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Spanish cross-\\xadvalidation experiment with 95% confidence intervals.\\nModel\\nTask heads\\nEXIST-\\xad2021\\nDETOXIS-\\xad2021\\nHatEval-\\xad2019\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nSTL\\nSexism\\n0.789\\u2009±\\u20090.011\\n—\\n—\\nToxic-\\xadlanguage\\n—\\n0.640\\u2009±\\u20090.014\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.846\\u2009±\\u20090.009\\nMTL\\nSexism + toxic-\\xadlanguage\\n0.788\\u2009±\\u20090.011\\n0.628\\u2009±\\u20090.014\\n—\\nSexism + hate-\\xadspeech\\n0.791\\u2009±\\u20090.011\\n—\\n0.843\\u2009±\\u20090.009\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.632\\u2009±\\u20090.014\\n0.841\\u2009±\\u20090.009\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.799\\u2009±\\u20090.010\\n0.634\\u2009±\\u20090.014\\n0.842\\u2009±\\u20090.009\\nMTL-\\xadTAI\\nSexism + toxic-\\xadlanguage\\n0.799\\u2009±\\u20090.010\\n0.649\\u2009±\\u20090.014\\n—\\nSexism + hate-\\xadspeech\\n0.805\\u2009±\\u20090.010\\n—\\n0.984\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.649\\u2009±\\u20090.014\\n0.988\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.800\\u2009±\\u20090.010\\n0.650\\u2009±\\u20090.014\\n0.980\\u2009±\\u20090.003\\nMTL-\\xadTE'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content='0.805\\u2009±\\u20090.010\\n—\\n0.984\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.649\\u2009±\\u20090.014\\n0.988\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.800\\u2009±\\u20090.010\\n0.650\\u2009±\\u20090.014\\n0.980\\u2009±\\u20090.003\\nMTL-\\xadTE\\nSexism + toxic-\\xadlanguage\\n0.797\\u2009±\\u20090.011\\n0.653\\u2009±\\u20090.014\\n—\\nSexism + hate-\\xadspeech\\n0.806\\u2009±\\u20090.010\\n—\\n0.992\\u2009±\\u20090.002\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.653\\u2009±\\u20090.014\\n0.980\\u2009±\\u20090.003\\nToxic-\\xadlanguage + hate-\\xadspeech + Sexism\\n0.801\\u2009±\\u20090.010\\n0.659\\u2009±\\u20090.014\\n0.988\\u2009±\\u20090.003\\nNote: Evaluation metric values are shown with their 95% confidence intervals. Bold values indicate the highest scores across all analysed models, while underlined \\nvalues denote the highest scores among the MTL models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 9}, page_content='values denote the highest scores among the MTL models.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 10}, page_content='11 of 21\\nExpert Systems, 2026\\nresults demonstrate the superiority of the MTL-\\xadTA approach \\nover the traditional MTL model, owing to its ability to mitigate \\nthe negative transfer phenomenon.\\n5.1.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fArabic\\nTable\\xa010 shows the results of the Arabic cross-\\xadvalidation exper-\\niment. Comparing the outcomes of the classic MTL and STL \\nmodels, we observe that the MTL model performs worse due to \\nthe negative transfer phenomenon. The STL model outperforms \\nthe classic MTL model in Sexism and Offensive-\\xadlanguage detec-\\ntion. The MTL-\\xadTA approach demonstrates superior performance \\ncompared to the traditional MTL approach in two of the three \\ntasks. Among the MTL models, the MTL-\\xadTAI model achieves the \\nbest performance for Sexism detection, while the MTL-\\xadTE model \\nobtains the highest F1-\\xadmacro score for Offensive-\\xadlanguage detec-\\ntion. The TA mechanisms effectively minimise negative transfer, \\nleading to consistent improvements. For Hate-\\xadspeech detection,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 10}, page_content='obtains the highest F1-\\xadmacro score for Offensive-\\xadlanguage detec-\\ntion. The TA mechanisms effectively minimise negative transfer, \\nleading to consistent improvements. For Hate-\\xadspeech detection, \\nthe traditional MTL approach performs slightly better than the \\nMTL-\\xadTA models. Overall, the results in Table\\xa010 suggest that in-\\ncorporating TAI and TE mechanisms in MTL models significantly \\nenhances performance, reducing the adverse effects of negative \\ntransfer. The MTL-\\xadTE model, in particular, demonstrates the best \\noverall performance across the evaluated tasks, making it the \\nmost effective model for the Arabic cross-\\xadvalidation experiment.\\nFigure\\xa05 compares the top result of each model across the data-\\nsets in the Arabic cross-\\xadvalidation experiment. Negative trans-\\nfer is present in bar charts (a) ArMI-\\xad2021 and (c) OSACT-\\xad2022, \\nwhere the STL model outperforms the classic MTL model. In \\nboth cases, the MTL-\\xadTAI and MTL-\\xadTE models show slight im-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 10}, page_content=\"fer is present in bar charts (a) ArMI-\\xad2021 and (c) OSACT-\\xad2022, \\nwhere the STL model outperforms the classic MTL model. In \\nboth cases, the MTL-\\xadTAI and MTL-\\xadTE models show slight im-\\nprovement over the classic MTL model. This suggests that the \\ntask-\\xadaware capability helped to mitigate the negative transfer \\neffect at least partially.\\nTable\\xa011 presents the aggregated results of the MTL models for \\nthe Arabic cross-\\xadvalidation experiment. The classic MTL model \\nperformed poorly, achieving the lowest aggregated results in \\nthree out of the four task combinations. This is due to the neg-\\native transfer phenomenon that compromised MTL model's \\nlearning process. In contrast, the MTL-\\xadTAI model achieved the \\nhighest aggregated results for the Sexism, Toxic-\\xadlanguage and \\nHate-\\xadspeech task combination. The MTL-\\xadTE model obtained the \\nhighest aggregated results for the combinations of Sexism and \\nOffensive-\\xadlanguage tasks and Offensive-\\xadlanguage and Hate-\\xad\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 10}, page_content=\"Hate-\\xadspeech task combination. The MTL-\\xadTE model obtained the \\nhighest aggregated results for the combinations of Sexism and \\nOffensive-\\xadlanguage tasks and Offensive-\\xadlanguage and Hate-\\xad\\nspeech tasks. Consistent with the results in Table\\xa010, the TAI \\nand TE mechanisms mitigate the negative transfer effect during \\nMTL training. As a result, the MTL-\\xadTAI and MTL-\\xadTE models \\noutperform the traditional MTL model in three out of the four \\ntask combinations. These results demonstrate the superiority of \\nthe MTL-\\xadTA approach over the traditional MTL model, owing to \\nits ability to mitigate the negative transfer phenomenon.\\nFIGURE 4\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Spanish cross-\\xadvalidation \\nexperiment. The bars include 95% Confidence Intervals at the top. (a) Displays the models' best result for sexism detection on the EXIST-\\xad2021 dataset;\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 10}, page_content=\"experiment. The bars include 95% Confidence Intervals at the top. (a) Displays the models' best result for sexism detection on the EXIST-\\xad2021 dataset; \\n(b) showcases the models' best result for toxic language detection on the DETOXIS-\\xad2021 dataset; (c) illustrates the models' best result for Hate Speech \\ndetection on the HatEval-\\xad2019 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) EXIST-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) DETOXIS-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) HatEval-2019\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\nTABLE 9\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Spanish cross-\\xadvalidation results for the MTL models by model type. Bold values indicate the highest score across all \\nanalysed models within a column.\\nModels\\nTask Heads\\nSexism\\nSexism\\nSexism\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.708\\n0.817\\n0.737\\n0.758\\nMTL-\\xadTAI\\n0.724\\n0.895\\n0.819\\n0.810\\nMTL-\\xadTE\\n0.725\\n0.899\\n0.817\\n0.816\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 10}, page_content='Sexism\\nSexism\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.708\\n0.817\\n0.737\\n0.758\\nMTL-\\xadTAI\\n0.724\\n0.895\\n0.819\\n0.810\\nMTL-\\xadTE\\n0.725\\n0.899\\n0.817\\n0.816\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 11}, page_content='12 of 21\\nExpert Systems, 2026\\n5.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fOfficial Training-\\xadTest Split\\n5.2.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fSpanish\\nThe experiment involving the three Spanish datasets using their \\nofficial train-\\xadtest partitions is detailed in Table\\xa012. The results \\nindicate that MTL training did not benefit the standard MTL \\nmodel for the Sexism detection task, yielding lower accuracy \\nthan the STL model. This outcome is likely a consequence of the \\nnegative transfer phenomenon. Nevertheless, the MTL-\\xadTAI and \\nMTL-\\xadTE models, incorporating TA mechanisms, counteracted \\nthe negative transfer observed during standard MTL train-\\ning. They achieved superior accuracy compared to both the \\nSTL model and the SOTA model for EXIST-\\xad2021 (AI-\\xadUPV \\n(Magnossão de Paula et\\xa0al.\\xa02021)). For Toxic-\\xadlanguage detection, \\nMTL training yielded better results than the STL baseline in the \\ntraining-\\xadtest setup. Broadly, the MTL, MTL-\\xadTAI, and MTL-\\xadTE \\nmodels produced comparable outcomes, suggesting minimal'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 11}, page_content='MTL training yielded better results than the STL baseline in the \\ntraining-\\xadtest setup. Broadly, the MTL, MTL-\\xadTAI, and MTL-\\xadTE \\nmodels produced comparable outcomes, suggesting minimal \\nnegative transfer effects for this specific task during the stan-\\ndard MTL training process. Table\\xa0 12 also demonstrates that \\nMTL training enhanced performance for Hate-\\xadspeech detec-\\ntion. The MTL model registered a higher F1-\\xadmacro score than \\nTABLE 10\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Arabic cross-\\xadvalidation experiment with 95% confidence intervals. Evaluation metric values are shown with their 95% \\nconfidence intervals.\\nModel\\nTask Heads\\nArMI-\\xad2021\\nHSArabic-\\xad2023\\nOSACT-\\xad2022\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nSTL\\nSexism\\n0.884\\u2009±\\u20090.006\\n—\\n—\\nOffensive-\\xadlanguage\\n—\\n0.608\\u2009±\\u20090.008\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.778\\u2009±\\u20090.007\\nMTL\\nSexism + offensive-\\xadlanguage\\n0.874\\u2009±\\u20090.007\\n0.617\\u2009±\\u20090.008\\n—\\nSexism + hate-\\xadspeech\\n0.876\\u2009±\\u20090.007\\n—\\n0.777\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.603\\u2009±\\u20090.008\\n0.771\\u2009±\\u20090.007'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 11}, page_content='—\\n—\\n0.778\\u2009±\\u20090.007\\nMTL\\nSexism + offensive-\\xadlanguage\\n0.874\\u2009±\\u20090.007\\n0.617\\u2009±\\u20090.008\\n—\\nSexism + hate-\\xadspeech\\n0.876\\u2009±\\u20090.007\\n—\\n0.777\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.603\\u2009±\\u20090.008\\n0.771\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.878\\u2009±\\u20090.007\\n0.613\\u2009±\\u20090.008\\n0.770\\u2009±\\u20090.007\\nMTL-\\xadTAI\\nSexism + offensive-\\xadlanguage\\n0.883\\u2009±\\u20090.006\\n0.612\\u2009±\\u20090.008\\n—\\nSexism + hate-\\xadspeech\\n0.880\\u2009±\\u20090.006\\n—\\n0.766\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.603\\u2009±\\u20090.008\\n0.770\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.883\\u2009±\\u20090.006\\n0.618\\u2009±\\u20090.008\\n0.776\\u2009±\\u20090.007\\nMTL-\\xadTE\\nSexism + offensive-\\xadlanguage\\n0.881\\u2009±\\u20090.006\\n0.620\\u2009±\\u20090.008\\n—\\nSexism + hate-\\xadspeech\\n0.878\\u2009±\\u20090.007\\n—\\n0.771\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + Hate-\\xadspeech\\n—\\n0.619\\u2009±\\u20090.008\\n0.772\\u2009±\\u20090.007\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.883\\u2009±\\u20090.006\\n0.618\\u2009±\\u20090.008\\n0.772\\u2009±\\u20090.007\\nNote: Bold values indicate the highest scores across all analysed models, while underlined values denote the highest scores among the MTL models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 11}, page_content=\"0.883\\u2009±\\u20090.006\\n0.618\\u2009±\\u20090.008\\n0.772\\u2009±\\u20090.007\\nNote: Bold values indicate the highest scores across all analysed models, while underlined values denote the highest scores among the MTL models.\\nFIGURE 5\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Arabic cross-\\xadvalidation ex-\\nperiment. The bars include 95% confidence intervals at the top. (a) Displays the models' best result for Sexism detection on the ArMI-\\xad2021 dataset; (b) \\nshowcases the models' best result for Toxic Language detection on the HSArabic-\\xad2023 dataset; (c) illustrates the models' best result for hate speech \\ndetection on the OSACT-\\xad2022 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) ArMI-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) HSArabic-2023\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) OSACT-2022\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 11}, page_content='0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) ArMI-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) HSArabic-2023\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) OSACT-2022\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 12}, page_content=\"13 of 21\\nExpert Systems, 2026\\nboth the HatEval-\\xad2019 SOTA (Atalaya (Pérez and Luque\\xa02019)) \\nand the STL baseline. Models equipped with TA mechanisms \\nfurther boosted these results, effectively lessening the negative \\ntransfer associated with conventional MTL training and yield-\\ning higher F1-\\xadmacro scores than the standard MTL approach.\\nFigure\\xa0 6 compares the top results of each model across the \\ndatasets in the Spanish training-\\xadtest experiment. Bar chart (a) \\nEXIST-\\xad2021 shows negative transfer, where the STL model out-\\nperforms the classic MTL model. However, the MTL-\\xadTA mod-\\nels (MTL-\\xadTAI and MTL-\\xadTE) effectively mitigate this negative \\ntransfer, outperforming the classic MTL model. In bar chart (c) \\nHatEval-\\xad2019, the MTL-\\xadTA models also surpass the classic MTL \\nmodel. This suggests negative transfer happened during train-\\ning, even though the classic MTL's results were higher than the \\nSTL model's in this instance.\\nTable\\xa013 displays the aggregated results of the MTL models for\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 12}, page_content=\"ing, even though the classic MTL's results were higher than the \\nSTL model's in this instance.\\nTable\\xa013 displays the aggregated results of the MTL models for \\nthe Spanish official training-\\xadtest split experiment. The tradi-\\ntional MTL model performed the worst, achieving the lowest \\naggregate results across all task combinations. This poor perfor-\\nmance is attributed to the negative transfer phenomenon, which \\nimpaired the model's learning during training and resulted in \\nsubpar performance on the test. The MTL-\\xadTAI model achieved \\nthe best results for the task combinations of Sexism and Hate-\\xad\\nspeech, and Toxic-\\xadlanguage and Hate-\\xadspeech. Along with the \\nTABLE 11\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Arabic cross-\\xadvalidation results for the MTL models, shown by model type.\\nModels\\nTask Heads\\nSexism\\nSexism\\nSexism\\nOffensive-\\xadlanguage\\nOffensive-\\xadlanguage\\nOffensive-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.746\\n0.826\\n0.687\\n0.754\\nMTL-\\xadTAI\\n0.748\\n0.823\\n0.687\\n0.759\\nMTL-\\xadTE\\n0.751\\n0.825\\n0.696\\n0.758\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 12}, page_content='Sexism\\nOffensive-\\xadlanguage\\nOffensive-\\xadlanguage\\nOffensive-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.746\\n0.826\\n0.687\\n0.754\\nMTL-\\xadTAI\\n0.748\\n0.823\\n0.687\\n0.759\\nMTL-\\xadTE\\n0.751\\n0.825\\n0.696\\n0.758\\nNote: Bold values indicate the highest score across all analysed models within a column.\\nTABLE 12\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Spanish training-\\xadtest experiment with 95% confidence intervals.\\nModel\\nTask heads\\nEXIST-\\xad2021\\nDETOXIS-\\xad2021\\nHatEval-\\xad2019\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nAI-\\xadUPV\\n—\\n0.790\\u2009±\\u20090.018\\n—\\n—\\nSINAI\\n—\\n—\\n0.646\\u2009±\\u20090.031\\n—\\nAtalaya\\n—\\n—\\n—\\n0.730\\u2009±\\u20090.022\\nSTL\\nSexism\\n0.790\\u2009±\\u20090.017\\n—\\n—\\nToxic-\\xadlanguage\\n—\\n0.620\\u2009±\\u20090.032\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.764\\u2009±\\u20090.021\\nMTL\\nSexism + toxic-\\xadlanguage\\n0.776\\u2009±\\u20090.018\\n0.639\\u2009±\\u20090.032\\n—\\nSexism + hate-\\xadspeech\\n0.785\\u2009±\\u20090.017\\n—\\n0.778\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.593\\u2009±\\u20090.032\\n0.777\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.775\\u2009±\\u20090.018\\n0.629\\u2009±\\u20090.032\\n0.773\\u2009±\\u20090.021\\nMTL-\\xadTAI\\nSexism + toxic-\\xadlanguage\\n0.797\\u2009±\\u20090.017\\n0.633\\u2009±\\u20090.032\\n—\\nSexism + hate-\\xadspeech'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 12}, page_content='—\\n0.593\\u2009±\\u20090.032\\n0.777\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.775\\u2009±\\u20090.018\\n0.629\\u2009±\\u20090.032\\n0.773\\u2009±\\u20090.021\\nMTL-\\xadTAI\\nSexism + toxic-\\xadlanguage\\n0.797\\u2009±\\u20090.017\\n0.633\\u2009±\\u20090.032\\n—\\nSexism + hate-\\xadspeech\\n0.809\\u2009±\\u20090.017\\n—\\n0.789\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.628\\u2009±\\u20090.032\\n0.790\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.792\\u2009±\\u20090.017\\n0.629\\u2009±\\u20090.032\\n0.782\\u2009±\\u20090.020\\nMTL-\\xadTE\\nSexism + toxic-\\xadlanguage\\n0.804\\u2009±\\u20090.017\\n0.626\\u2009±\\u20090.032\\n—\\nSexism + hate-\\xadspeech\\n0.804\\u2009±\\u20090.017\\n—\\n0.786\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech\\n—\\n0.623\\u2009±\\u20090.032\\n0.786\\u2009±\\u20090.020\\nToxic-\\xadlanguage + hate-\\xadspeech + sexism\\n0.802\\u2009±\\u20090.017\\n0.633\\u2009±\\u20090.032\\n0.789\\u2009±\\u20090.020\\nNote: Evaluation metric values are shown with their 95% confidence intervals. Bold values indicate the highest scores across all analysed models, while underlined \\nvalues denote the highest scores among the MTL models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 12}, page_content='values denote the highest scores among the MTL models.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 13}, page_content=\"14 of 21\\nExpert Systems, 2026\\nMTL-\\xadTE model, it also achieved top results for the combination \\nof Sexism and Toxic-\\xadlanguage. Additionally, the MTL-\\xadTE model \\nachieved the highest results for the combination of all three \\ntasks. The aggregated results demonstrate that the TAI and TE \\nmechanisms alleviated the negative transfer phenomenon. In all \\ncases, models equipped with these mechanisms outperformed \\nthe traditional MTL model, achieving superior aggregated \\nresults.\\n5.2.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fArabic\\nThe study conducted on the three Arabic datasets using their \\nspecified training-\\xadtest partitions is documented in Table\\xa014. \\nObservations from the table indicate that standard MTL train-\\ning did not improve results for the Sexism detection task com-\\npared to STL. The peak accuracy achieved by the conventional \\nMTL model matched that of the STL model. This lack of im-\\nprovement is ascribed to the negative transfer effect constrain-\\ning the MTL model's learning phase. Conversely, the MTL-\\xadTAI\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 13}, page_content=\"MTL model matched that of the STL model. This lack of im-\\nprovement is ascribed to the negative transfer effect constrain-\\ning the MTL model's learning phase. Conversely, the MTL-\\xadTAI \\nand MTL-\\xadTE models, utilising TA mechanisms, lessened the \\nnegative transfer observed in the standard MTL approach, \\nyielding higher accuracy than both STL and standard MTL \\nmodels. For the Sexism detection task, the SOTA model from \\nArMI-\\xad2021, UM6P-\\xadNLP (Mahdaouy et\\xa0 al.\\xa0 2021), produced \\nthe best outcome. Regarding Offensive-\\xadlanguage detection, \\nMTL training showed enhanced results over the STL baseline \\nwithin the training-\\xadtest framework. Models incorporating TA \\nmechanisms further amplified these gains, effectively reduc-\\ning negative transfer from traditional MTL training and re-\\nsulting in a better F1-\\xadscore than the conventional MTL model. \\nNotably, the MTL-\\xadTE model recorded the highest F1-\\xadscore for \\nOffensive-\\xadlanguage detection, establishing a new SOTA for\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 13}, page_content='sulting in a better F1-\\xadscore than the conventional MTL model. \\nNotably, the MTL-\\xadTE model recorded the highest F1-\\xadscore for \\nOffensive-\\xadlanguage detection, establishing a new SOTA for \\nthe HSArabic-\\xad2023 dataset. As seen in Table\\xa014, MTL train-\\ning also led to better outcomes for Hate-\\xadspeech detection, with \\nthe standard MTL model scoring a higher F1-\\xadmacro than the \\nSTL baseline. The MTL-\\xadTA models (MTL-\\xadTAI and MTL-\\xadTE) \\nadvanced these results further by mitigating the negative \\ntransfer effects present in standard MTL training, achieving \\nsuperior F1-\\xadmacro scores compared to the traditional MTL \\nmodel. The top performance for Hate-\\xadspeech detection was \\nby the OSACT-\\xad2021 SOTA model, GOF (Mostafa et\\xa0al.\\xa02022), \\nwhile among the MTL variants, the MTL-\\xadTE model achieved \\nthe best result for this task. Across all evaluated scenarios in \\nthe Arabic training-\\xadtest split experiment, the MTL-\\xadTA models \\nconsistently delivered superior results relative to the classic \\nMTL model.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 13}, page_content=\"the Arabic training-\\xadtest split experiment, the MTL-\\xadTA models \\nconsistently delivered superior results relative to the classic \\nMTL model.\\nFigure\\xa0 7 compares the top results of each model across the \\ndatasets in the Arabic training-\\xadtest experiment. Negative \\ntransfer is not clearly observed in any of the charts, as the \\nclassic MTL model consistently outperforms the STL model. \\nFIGURE 6\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Spanish training-\\xadtest exper-\\niment. The bars include 95% confidence intervals at the top. (a) Displays the models' best result for sexism detection on the EXIST-\\xad2021 dataset; (b) \\nshowcases the models' best result for Toxic language detection on the DETOXIS-\\xad2021 dataset; (c) illustrates the models' best result for hate speech \\ndetection on the HatEval-\\xad2019 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) EXIST-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) DETOXIS-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 13}, page_content='detection on the HatEval-\\xad2019 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) EXIST-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) DETOXIS-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) HatEval-2019\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\nAI-UPV\\nSINAI\\nAtalaya\\nTABLE 13\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Spanish training-\\xadtest results for the MTL models by model type.\\nModels\\nTask Heads\\nSexism\\nSexism\\nSexism\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nToxic-\\xadlanguage\\nHate-\\xadspeech\\nHate-\\xadspeech\\nHate-\\xadspeech\\nMTL\\n0.708\\n0.782\\n0.685\\n0.726\\nMTL-\\xadTAI\\n0.715\\n0.799\\n0.709\\n0.731\\nMTL-\\xadTE\\n0.715\\n0.795\\n0.702\\n0.738\\nNote: Bold values indicate the highest score across all analysed models within a column.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 14}, page_content=\"15 of 21\\nExpert Systems, 2026\\nHowever, in all three charts, at least one of the MTL-\\xadTA mod-\\nels (MTL-\\xadTAI and MTL-\\xadTE) surpasses the classic MTL model, \\ndemonstrating their ability to mitigate negative transfer and \\nimprove performance.\\nTable\\xa015 displays the aggregated results of the MTL models for \\nthe Arabic official training-\\xadtest split experiment. The classic \\nMTL model performed poorly, achieving the lowest aggregated \\nresults in three of the four task combinations. This likely stems \\nfrom the negative transfer effect impeding the MTL model's \\nlearning progress. The MTL-\\xadTAI model obtained higher ag-\\ngregated results than the MTL model in all cases except for the \\nSexism and Offensive-\\xadlanguage task combination, where the \\ndifference was marginal. The MTL-\\xadTE model achieved the high-\\nest aggregated results for all task combinations. In line with the \\nresults from Table\\xa014, the TAI and TE mechanisms reduce the\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 14}, page_content='difference was marginal. The MTL-\\xadTE model achieved the high-\\nest aggregated results for all task combinations. In line with the \\nresults from Table\\xa014, the TAI and TE mechanisms reduce the \\nTABLE 14\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fResults of the Arabic training-\\xadtest experiment with 95% confidence intervals.\\nModel\\nTask Heads\\nArMI-\\xad2021\\nHSArabic-\\xad2023\\nOSACT-\\xad2022\\nAccuracy\\nF1-\\xadscore\\nF1-\\xadmacro\\nUM6P-\\xadNLP\\n—\\n0.919 ±\\u20090.012\\n—\\n—\\nGOF\\n—\\n—\\n—\\n0.852 ±\\u20090.014\\nSTL\\nSexism\\n0.892 ±\\u20090.014\\n—\\n—\\nOffensive-\\xadlanguage\\n—\\n0.605 ±\\u20090.017\\n—\\nHate-\\xadspeech\\n—\\n—\\n0.775 ±\\u20090.016\\nMTL\\nSexism + offensive-\\xadlanguage\\n0.892 ±\\u20090.014\\n0.617 ±\\u20090.017\\n—\\nSexism + hate-\\xadspeech\\n0.890 ±\\u20090.014\\n—\\n0.767 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.613 ±\\u20090.017\\n0.768 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.884 ±\\u20090.014\\n0.610 ±\\u20090.017\\n0.790 ±\\u20090.016\\nMTL-\\xadTAI\\nSexism + offensive-\\xadlanguage\\n0.888 ±\\u20090.014\\n0.617 ±\\u20090.017\\n—\\nSexism + hate-\\xadspeech\\n0.895 ±\\u20090.014\\n—\\n0.776 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.625 ±\\u20090.017\\n0.786 ±\\u20090.016'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 14}, page_content=\"0.790 ±\\u20090.016\\nMTL-\\xadTAI\\nSexism + offensive-\\xadlanguage\\n0.888 ±\\u20090.014\\n0.617 ±\\u20090.017\\n—\\nSexism + hate-\\xadspeech\\n0.895 ±\\u20090.014\\n—\\n0.776 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.625 ±\\u20090.017\\n0.786 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.888 ±\\u20090.014\\n0.630 ±\\u20090.017\\n0.786 ±\\u20090.016\\nMTL-\\xadTE\\nSexism + offensive-\\xadlanguage\\n0.893 ±\\u20090.014\\n0.632 ±\\u20090.017\\n—\\nSexism + hate-\\xadspeech\\n0.889 ±\\u20090.014\\n—\\n0.790 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech\\n—\\n0.634 ±\\u20090.017\\n0.786 ±\\u20090.016\\nOffensive-\\xadlanguage + hate-\\xadspeech + sexism\\n0.894 ±\\u20090.014\\n0.635 ±\\u20090.017\\n0.794 ±\\u20090.016\\nNote: Evaluation metric values are shown with their 95% confidence intervals. Bold values indicate the highest scores across all analysed models, while underlined \\nvalues denote the highest scores among the MTL models.\\nFIGURE 7\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Arabic training-\\xadtest exper-\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 14}, page_content=\"FIGURE 7\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fBar plots depicting the models' best performance based on the datasets' official evaluation metrics in the Arabic training-\\xadtest exper-\\niment. The bars include 95% confidence intervals at the top. (a) Displays the models' best result for sexism detection on the ArMI-\\xad2021 dataset; (b) \\nshowcases the models' best result for toxic language detection on the HSArabic-\\xad2023 dataset; (c) illustrates the models' best result for hate speech \\ndetection on the OSACT-\\xad2022 dataset.\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) ArMI-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) HSArabic-2023\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) OSACT-2022\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\nUM6P-NLP\\nGOF\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 14}, page_content='0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nAccuracy\\n(a) ArMI-2021\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-score\\n(b) HSArabic-2023\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nF1-macro\\n(c) OSACT-2022\\nModels\\nSTL\\nMTL\\nMTL-TAI\\nMTL-TE\\nUM6P-NLP\\nGOF\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='16 of 21\\nExpert Systems, 2026\\nnegative transfer impact during MTL training. As a result, the \\nMTL-\\xadTAI and MTL-\\xadTE models outperform the traditional MTL \\nmodel in all cases.\\n6\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fDiscussion and Limitations\\n6.1\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fDiscussion\\nThe results from our experiments demonstrate the impact of \\nnegative transfer on the performance of traditional MTL mod-\\nels. Across both the Spanish and Arabic datasets, the classic \\nMTL models consistently underperformed compared to STL \\nmodels, particularly in tasks prone to negative transfer phe-\\nnomena such as Sexism and Offensive Language detection. \\nHowever, the introduction of TA mechanisms (TAI and TE) \\nsignificantly mitigated the effects of negative transfer. Both \\nMTL-\\xadTAI and MTL-\\xadTE models showed improvements over \\nthe traditional MTL models, achieving higher accuracy and F1 \\nscores in almost all task combinations. In the Spanish cross-\\xad\\nvalidation experiment, the MTL-\\xadTE model outperformed all'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='the traditional MTL models, achieving higher accuracy and F1 \\nscores in almost all task combinations. In the Spanish cross-\\xad\\nvalidation experiment, the MTL-\\xadTE model outperformed all \\nothers for the three evaluated tasks. The MTL-\\xadTAI model ex-\\ncelled for Sexism and Hate-\\xadspeech detection in the Spanish \\nofficial training-\\xadtest split and the MTL-\\xadTE model became the \\nnew DETOXIS-\\xad2021 SOTA model for Toxic-\\xadlanguage detec-\\ntion. These findings were consistent across the aggregated re-\\nsults evaluation for cross-\\xadvalidation and official training-\\xadtest \\nsplit, where the MTL-\\xadTA models continued to achieve superior \\nresults than the classic MTL model.\\nThese gains align with the linguistic overlap among the Spanish \\ndatasets: sexism, toxic language, and hate speech corpora share \\nrecurring lexical markers (e.g., slurs and gendered stereotypes) \\nbut differ in their annotation scope. By conditioning the en-\\ncoder on the task through TAI, the model learns to differentiate'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='recurring lexical markers (e.g., slurs and gendered stereotypes) \\nbut differ in their annotation scope. By conditioning the en-\\ncoder on the task through TAI, the model learns to differentiate \\nwhen a term should be interpreted as toxic rhetoric versus ex-\\nplicitly sexist and hate speech. TE further exploits this structure \\nby amplifying shared signals—such as intensity modifiers and \\ntarget mentions—only for the tasks that benefit from them. As \\na result, the TA-\\xadequipped models can capitalise on beneficial \\ncross-\\xadtask cues while suppressing misleading correlations that \\ncaused the baseline MTL model to underperform. Similarly, \\nfor the Arabic cross-\\xadvalidation experiment, the MTL-\\xadTE model \\noutperformed all others in Offensive Language and obtained \\ncompetitive results for Sexism and Hate-\\xadspeech detection. In \\nthe official training-\\xadtest split experiment, the MTL-\\xadTAI obtained \\nthe top result for Offensive-\\xadlanguage detection and became the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='competitive results for Sexism and Hate-\\xadspeech detection. In \\nthe official training-\\xadtest split experiment, the MTL-\\xadTAI obtained \\nthe top result for Offensive-\\xadlanguage detection and became the \\nnew HSArabic-\\xad2023 SOTA model for the task. These results \\nwere consistent throughout the aggregated evaluations for cross-\\xad\\nvalidation and the official training-\\xadtest split, where the MTL-\\xadTA \\nmodels consistently outperformed the traditional MTL model.\\nArabic datasets exhibit stronger dialectal variation and class \\nimbalance than their Spanish counterparts, which amplifies \\nnegative transfer when models rely solely on shared represen-\\ntations. Injecting task context via TAI helps the encoder focus \\non morphological patterns that are discriminative for each phe-\\nnomenon (e.g., misogynistic verb forms versus generic insults), \\nwhile TE recalibrates the representation to handle skewed label \\ndistributions by emphasising features that consistently charac-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='nomenon (e.g., misogynistic verb forms versus generic insults), \\nwhile TE recalibrates the representation to handle skewed label \\ndistributions by emphasising features that consistently charac-\\nterise the minority class. This explains why the largest relative \\nimprovements arise on HSArabic-\\xad2023, where offensive con-\\ntent is substantially rarer than neutral statements. The analy-\\nsis of the bar charts presenting the best results for each model \\nacross all datasets reveals a consistent pattern: when negative \\ntransfer is identified—indicated by the classic MTL model un-\\nderperforming the STL model—the MTL-\\xadTA model effectively \\nmitigates this issue, outperforming both the classic MTL and \\nSTL models. Furthermore, the MTL-\\xadTA model often achieves \\nsuperior performance compared to the classic MTL model, even \\nin scenarios where negative transfer is not evident, as demon-\\nstrated by the comparative analysis of the classic MTL and STL'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content=\"superior performance compared to the classic MTL model, even \\nin scenarios where negative transfer is not evident, as demon-\\nstrated by the comparative analysis of the classic MTL and STL \\nmodels' results. In summary, the incorporation of TAI and TE \\nmechanisms in MTL models not only provides a robust solu-\\ntion to the negative transfer problem but also opens up new \\npossibilities for enhancing overall performance. The MTL-\\xadTAI \\nand MTL-\\xadTE models emerge as the most effective MTL models \\nacross the evaluated tasks, demonstrating the exciting potential \\nfor performance improvement in MTL scenarios.\\nAcross both languages, a common trend is that TA mitigates sit-\\nuations where dataset-\\xadspecific annotation guidelines diverge. By \\nexplicitly signalling the task objective, the model can maintain \\nseparate decision boundaries for nuanced categories while still \\nsharing underlying lexical knowledge. This supports our hy-\\npothesis that negative transfer stems from conflating task intent\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='separate decision boundaries for nuanced categories while still \\nsharing underlying lexical knowledge. This supports our hy-\\npothesis that negative transfer stems from conflating task intent \\nrather than from a lack of shared information. Consequently, TA \\ndelivers the most benefit when tasks are semantically related yet \\noperationalised differently—a regime that typifies harmful lan-\\nguage detection benchmarks.\\n6.2\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fLimitations\\nDespite the promising results demonstrated by the incorpora-\\ntion of TAI and TE mechanisms in MTL models, several lim-\\nitations should be acknowledged. Firstly, the negative transfer \\nphenomenon remains a challenge, especially for certain task \\ncombinations. While the TA mechanisms effectively mitigate \\nthis issue, the extent of their effectiveness is not fully known. \\nWe are still unable to determine whether the TA mechanisms \\nentirely eliminate negative transfer, and traditional MTL models'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='this issue, the extent of their effectiveness is not fully known. \\nWe are still unable to determine whether the TA mechanisms \\nentirely eliminate negative transfer, and traditional MTL models \\ncontinue to suffer from its effects, particularly in tasks such as \\nSexism and Offensive Language detection. The evaluation is lim-\\nited to specific datasets and tasks within the Spanish and Arabic \\nlanguages. This scope may not fully capture the generalisability \\nTABLE 15\\u202f\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202f\\u202fAggregated Arabic training-\\xadtest results for the MTL \\nmodels by model type.\\nModels\\nTask heads\\nSexism\\nSexism\\nOffensive-\\xad\\nlanguage\\nHate-\\xad\\nspeech\\nOffensive-\\xad\\nlanguage\\nHate-\\xad\\nspeech\\nMTL\\n0.755\\n0.828\\n0.691\\n0.761\\nMTL-\\xadTAI\\n0.752\\n0.835\\n0.705\\n0.768\\nMTL-\\xadTE\\n0.763\\n0.839\\n0.710\\n0.774\\nNote: Bold values indicate the highest score across all analysed models within a \\ncolumn.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 15}, page_content='Hate-\\xad\\nspeech\\nMTL\\n0.755\\n0.828\\n0.691\\n0.761\\nMTL-\\xadTAI\\n0.752\\n0.835\\n0.705\\n0.768\\nMTL-\\xadTE\\n0.763\\n0.839\\n0.710\\n0.774\\nNote: Bold values indicate the highest score across all analysed models within a \\ncolumn.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content=\"17 of 21\\nExpert Systems, 2026\\nof the findings across different languages, datasets, or task do-\\nmains. Future work should explore a broader range of datasets \\nand tasks to validate the robustness of the proposed mecha-\\nnisms. Additionally, achieving strong performance with the two \\nMTL-\\xadTA models hinges on employing a potent encoder. This \\ndependence could pose difficulties for computational systems \\nwith limited resources that lack the capacity for deep learning \\nframeworks like Transformers (Vaswani et\\xa0al.\\xa02017) as the en-\\ncoder. The inclusion of supplementary layers and mechanisms \\nmight also escalate computational requirements, potentially \\nrestricting the scalability of these models for practical, real-\\xad\\nworld uses. A detailed examination of the balance between \\nperformance gains and computational overhead is warranted. \\nHandling an increased number of tasks necessitates more task \\nheads, consequently enlarging the model's parameter count. As\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content=\"performance gains and computational overhead is warranted. \\nHandling an increased number of tasks necessitates more task \\nheads, consequently enlarging the model's parameter count. As \\na result, fine-\\xadtuning MTL-\\xadTA models demands greater compu-\\ntational resources. This escalation in resource needs could pres-\\nent a considerable barrier for applications operating under tight \\ncomputational constraints. A further consideration is whether \\nthe fine-\\xadtuning process, which uses task-\\xadspecific information, \\ndiminishes the MTL-\\xadTA models' capacity for adapting to novel, \\npreviously unseen tasks (e.g., in few-\\xadshot learning or instruction-\\xad\\nfollowing scenarios). The specialisation towards specific tasks \\nduring fine-\\xadtuning could potentially impede their adaptability \\nand generalisation capabilities for new tasks, an area deserving \\ninvestigation in future studies.\\n7\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fConclusion and Future Work\\nThis paper introduced the TA strategy aimed at tackling the\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content=\"and generalisation capabilities for new tasks, an area deserving \\ninvestigation in future studies.\\n7\\u202f\\u202f\\u202f|\\u202f\\u202f\\u202fConclusion and Future Work\\nThis paper introduced the TA strategy aimed at tackling the \\nnegative transfer issue (Wu, Zhang, and Ré\\xa02020; Vandenhende \\net\\xa0 al.\\xa0 2022; Li et\\xa0 al.\\xa0 2023) encountered during MTL training \\nphases. Our approch presented two distinct mechanisms: TAI \\nand TE. The TAI mechanism enhances the MTL model encoder's \\ninput by integrating task description details. Concurrently, the \\nTE method adds a TEB, an extra module processing the encod-\\ner's latent output alongside a Task Identification Vector (TIV). \\nThrough these mechanisms, the MTL model can generate task-\\xad\\ntailored representations, which effectively reduce negative trans-\\nfer effects and boost overall model performance.\\nOur experimental results demonstrate that the TA mechanisms sig-\\nnificantly reduce negative transfer and improve performance over\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content='fer effects and boost overall model performance.\\nOur experimental results demonstrate that the TA mechanisms sig-\\nnificantly reduce negative transfer and improve performance over \\nstandard MTL models across different tasks. Notably, we achieved \\ncompetitive results compared to SOTA methods for both the \\nSpanish and Arabic datasets. The proposed models set new SOTA \\nbenchmarks on the EXIST-\\xad2021 (Rodríguez-\\xadSánchez et\\xa0al.\\xa02021) \\nand HatEval-\\xad2019 (Basile et\\xa0al.\\xa02019) datasets for Spanish, as well \\nas on the HSArabic-\\xad2023 dataset for Arabic. These findings under-\\nscore the generalisability and effectiveness of the TA approach in \\nmitigating negative transfer across different languages and tasks.\\nBeyond the immediate results, the broader impact of our ap-\\nproach lies in its potential to shape future research in NLP. By \\nenabling more accurate and efficient MTL systems, the intro-\\nduction of TA mechanisms paves the way for improved detection'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content='proach lies in its potential to shape future research in NLP. By \\nenabling more accurate and efficient MTL systems, the intro-\\nduction of TA mechanisms paves the way for improved detection \\nand moderation of harmful content on social media platforms. \\nThis has significant implications for the development of auto-\\nmated systems tasked with moderating online spaces, reducing \\nhuman bias, and fostering safer digital environments.\\nFurthermore, the ability of TA-\\xadequipped models to enhance per-\\nformance across multiple languages and tasks suggests broader \\napplicability in multilingual and cross-\\xadlinguistic NLP challenges. \\nThis opens the door for future research to explore TA in other \\nlanguages and domains, where traditional single-\\xadtask models \\noften struggle due to data scarcity and computational constraints.\\nFor future work, it would be valuable to further investigate the \\nminimum amount of labelled data or information volume re-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content=\"often struggle due to data scarcity and computational constraints.\\nFor future work, it would be valuable to further investigate the \\nminimum amount of labelled data or information volume re-\\nquired for MTL to outperform STL models. Additionally, exploring \\nthe augmentation of MTL models with low-\\xadlevel task supervision, \\nwhere the decoder leverages the entirety or a portion of the en-\\ncoder's hidden states, could provide further performance gains. \\nWe also plan to extend the application of MTL combined with \\nTA into novel areas, such as identifying Sexism within learning-\\xad\\nwith-\\xaddisagreement paradigms (Uma et\\xa0al.\\xa02021; Plaza et\\xa0al.\\xa02024; \\nPlaza, de Carrillo-\\xadAlbornoz, Morante, Amigó, et\\xa0al.\\xa02023; Plaza, \\nde Carrillo-\\xadAlbornoz, Morante, Gonzalo, et\\xa0al.\\xa02023), where mul-\\ntiple annotator labels are considered rather than relying on a sin-\\ngle aggregated gold label (Frenda et\\xa0al.\\xa02025).\\nFinally, future research will also focus on incorporating unsu-\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content='tiple annotator labels are considered rather than relying on a sin-\\ngle aggregated gold label (Frenda et\\xa0al.\\xa02025).\\nFinally, future research will also focus on incorporating unsu-\\npervised learning techniques to enhance the proposed models for \\nidentifying Hate Speech, Toxic Language, and Sexism. Potential \\ntechniques include Latent Dirichlet Allocation (Blei et\\xa0al.\\xa02003), \\nSelf-\\xadOrganising Maps (Miljković\\xa02017), and K-\\xadMeans Clustering \\n(Ezugwu et\\xa0al.\\xa02022), which could offer further improvements \\nin model robustness and accuracy across different linguistic and \\ncultural contexts.\\nAuthor Contributions\\nAngel Felipe Magnossão de Paula: conceptualization, data cura-\\ntion, formal analysis, investigation, methodology, resources, software, \\nvalidation, writing – original draft. Imene Bensalem: data curation, \\nresources, supervision, writing – original draft. Damiano Spina: \\nconceptualization, methodology, resources, supervision, funding ac-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content='resources, supervision, writing – original draft. Damiano Spina: \\nconceptualization, methodology, resources, supervision, funding ac-\\nquisition, writingen – original draft. Paolo Rosso: conceptualization, \\nmethodology, resources, supervision, funding acquisition.\\nAcknowledgements\\nTThis research is partially supported by the Australian Research Council \\n(ARC) Centre of Excellence for Automated Decision-Making and Society \\n(ADM+S, CE200100005).  The research work of Paolo Rosso was in the \\nframework of the Malicious Actors Profiling and Detection in Online \\nSocial Networks Through Artificial Intelligence (MARTINI) project(Grant \\nPCI2022-135008-2) funded by MCIN/AEI/ 10.13039/501100011033 and \\nby European Union NextGenerationEU/PRTR.\\nFunding\\nThis work was supported by MCIN/AEI/10.13039/501100011033 \\n(PCI2022-\\xad135008-\\xad2); European Union NextGenerationEU/PRTR.\\nConflicts of Interest\\nThe authors declare no conflicts of interest.\\nData Availability Statement'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 16}, page_content='(PCI2022-\\xad135008-\\xad2); European Union NextGenerationEU/PRTR.\\nConflicts of Interest\\nThe authors declare no conflicts of interest.\\nData Availability Statement\\nThe data that support the findings of this study are available on request \\nfrom the corresponding sources. The data are not publicly available due \\nto privacy or ethical restrictions.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content='18 of 21\\nExpert Systems, 2026\\nEndnotes\\n\\t1\\thttps://\\u200bgithub.\\u200bcom/\\u200bAngel\\u200bFelip\\u200beMP/\\u200bArabi\\u200bc-\\xad\\u200bMulti\\u200bTask-\\xad\\u200bLearning.\\nReferences\\nAbburi, H., P. Parikh, N. Chhaya, and V. Varma. 2020. “Semi-\\xadSupervised \\nMulti-\\xadTask Learning for Multi-\\xadLabel Fine-\\xadGrained Sexism Classification.” \\nIn Proceedings of the 28th International Conference on Computational \\nLinguistics, 5810–5820. International Committee on Computational \\nLinguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2020.\\u200bcolin\\u200bg-\\xad\\u200bmain.\\u200b511.\\nAbdelali, A., S. Hassan, H. Mubarak, K. Darwish, and Y. Samih. 2021. \\n“Pre-\\xadTraining BERT on Arabic Tweets: Practical Considerations.” \\narXiv:2102.10684.\\nAbdul-\\xadMageed, M., and A. Elmadany. 2021. “ARBERT & MARBERT: \\nDeep Bidirectional Transformers for Arabic.” In Proceedings of the 59th \\nAnnual Meeting of the Association for Computational Linguistics and \\nthe 11th International Joint Conference on Natural Language Processing \\n(Volume 1: Long Papers), 7088–7105.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content='Annual Meeting of the Association for Computational Linguistics and \\nthe 11th International Joint Conference on Natural Language Processing \\n(Volume 1: Long Papers), 7088–7105.\\nAbu Farha, I., and W. Magdy. 2020. “Multitask Learning for Arabic \\nOffensive Language and Hate-\\xadSpeech Detection.” In Proceedings of the \\n4th Workshop on Open-\\xadSource Arabic Corpora and Processing Tools, \\nWith a Shared Task on Offensive Language Detection, edited by H. Al-\\xad\\nKhalifa, W. Magdy, K. Darwish, T. Elsayed, and H. Mubarak, 86–90. \\nEuropean Language Resource Association. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b\\n2020.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b14.\\nAguilar, G., S. Maharjan, A. P. López-\\xadMonroy, and T. Solorio. 2017. “A \\nMulti-\\xadTask Approach for Named Entity Recognition in Social Media \\nData.” In Proceedings of the 3rd Workshop on Noisy User-\\xadGenerated Text, \\nedited by L. Derczynski, W. Xu, A. Ritter, and T. Baldwin, 148–153. \\nAssociation for Computational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200b\\nv1/\\u200bW17-\\xad\\u200b4419.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content='edited by L. Derczynski, W. Xu, A. Ritter, and T. Baldwin, 148–153. \\nAssociation for Computational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200b\\nv1/\\u200bW17-\\xad\\u200b4419.\\nAlkomah, F., and X. Ma. 2022. “A Literature Review of Textual Hate \\nSpeech Detection Methods and Datasets.” Information 13: 273. https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b3390/\\u200binfo1\\u200b3060273.\\nAlshaabi, T., D. R. Dewhurst, J. R. Minot, et\\xa0 al. 2021. “The Growing \\nAmplification of Social Media: Measuring Temporal and Social Contagion \\nDynamics for Over 150 Languages on Twitter for 2009–2020.” EPJ Data \\nScience 10: 15. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1140/\\u200bepjds/\\u200bs1368\\u200b8-\\xad\\u200b021-\\xad\\u200b00271\\u200b-\\xad\\u200b0.\\nAntoun, W., F. Baly, and H. Hajj. 2020. “AraBERT: Transformer-\\xadBased \\nModel for Arabic Language Understanding.” In Proceedings of the 4th \\nWorkshop on Open-\\xadSource Arabic Corpora and Processing Tools, With a \\nShared Task on Offensive Language Detection, 9–15. European Language \\nResource Association. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b2020.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b2.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content='Shared Task on Offensive Language Detection, 9–15. European Language \\nResource Association. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b2020.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b2.\\nBasile, V., C. Bosco, E. Fersini, et\\xa0 al. 2019. “SemEval-\\xad2019 Task 5: \\nMultilingual Detection of Hate Speech Against Immigrants and \\nWomen in Twitter.” In Proceedings of the 13th International Workshop \\non Semantic Evaluation, 54–63. Association for Computational \\nLinguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200bS19-\\xad\\u200b2007.\\nBensalem, I., P. Rosso, and H. Zitouni. 2024. “Toxic Language Detection: \\nA Systematic Review of Arabic Datasets.” Expert Systems 41: e13551. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1111/\\u200bexsy.\\u200b13551\\u200b.\\nBlei, D. M., A. Y. Ng, and M. I. Jordan. 2003. “Latent Dirichlet \\nAllocation.” Journal of Machine Learning Research 3: 993–1022. https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b944919.\\u200b944937.\\nBlizard, W. D. 1988. “Multiset Theory.” Notre Dame Journal of Formal \\nLogic 30: 36–66. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1305/\\u200bndjfl/\\u200b10936\\u200b34995\\u200b.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content=\"doi.\\u200borg/\\u200b10.\\u200b5555/\\u200b944919.\\u200b944937.\\nBlizard, W. D. 1988. “Multiset Theory.” Notre Dame Journal of Formal \\nLogic 30: 36–66. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1305/\\u200bndjfl/\\u200b10936\\u200b34995\\u200b.\\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017. “Enriching \\nWord Vectors With Subword Information.” Transactions of the \\nAssociation for Computational Linguistics 5: 135–146. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1162/\\u200btacl_a_\\u200b00051\\u200b.\\nBoser, B. E., I. M. Guyon, and V. N. Vapnik. 1992. “A Training Algorithm \\nfor Optimal Margin Classifiers.” In Proceedings of the Fifth Annual \\nWorkshop on Computational Learning Theory COLT '92, 144–152. \\nAssociation for Computing Machinery. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1145/\\u200b130385.\\u200b\\n130401.\\nCanete, J., G. Chaperon, R. Fuentes, J.-\\xadH. Ho, H. Kang, and J. Pérez. \\n2020. “Spanish Pre-\\xadtrained Bert Model and Evaluation Data.” In \\nPractical Machine Learning for Developing Countries (PML4DC) at \\nEleventh International Conference on Learning Representations (ICLR),\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content=\"Practical Machine Learning for Developing Countries (PML4DC) at \\nEleventh International Conference on Learning Representations (ICLR), \\n2020, 1–10. https://\\u200bpml4dc.\\u200bgithub.\\u200bio/\\u200biclr2\\u200b020/\\u200bpapers/\\u200bPML4D\\u200bC2020_\\u200b\\n10.\\u200bpdf.\\nCaruana, R., S. Lawrence, and L. Giles. 2000. “Overfitting in Neural \\nNets: Backpropagation, Conjugate Gradient, and Early Stopping.” In \\nProceedings of the 13th International Conference on Neural Information \\nProcessing Systems NIPS'00, 381–387. MIT Press. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n5555/\\u200b30087\\u200b51.\\u200b3008807.\\nCharfi, A., M. Besghaier, R. Akasheh, A. Atalla, and W. Zaghouani. \\n2024. “Hate Speech Detection With ADHAR: A Multi-\\xadDialectal Hate \\nSpeech Corpus in Arabic.” Frontiers in Artificial Intelligence 7: 1391472. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b3389/\\u200bfrai.\\u200b2024.\\u200b1391472.\\nChen, S., Y. Zhang, and Q. Yang. 2024. “Multi-\\xadTask Learning in Natural \\nLanguage Processing: An Overview.” ACM Computing Surveys 56: 1–32. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1145/\\u200b3663363.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content='Chen, S., Y. Zhang, and Q. Yang. 2024. “Multi-\\xadTask Learning in Natural \\nLanguage Processing: An Overview.” ACM Computing Surveys 56: 1–32. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1145/\\u200b3663363.\\nChen, Z., V. Badrinarayanan, C.-\\xadY. Lee, and A. Rabinovich. 2018. \\n“GradNorm: Gradient Normalization for Adaptive Loss Balancing in \\nDeep Multitask Networks.” In Proceedings of the 35th International \\nConference on Machine Learning (794–803). PMLR volume 80 of \\nProceedings of Machine Learning Research. http://\\u200bproce\\u200bedings.\\u200bmlr.\\u200b\\npress/\\u200bv80/\\u200bchen1\\u200b8a/\\u200bchen1\\u200b8a.\\u200bpdf.\\nCipolla, R., Y. Gal, and A. Kendall. 2018. “Multi-\\xadTask Learning Using \\nUncertainty to Weigh Losses for Scene Geometry and Semantics.” In \\n2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, \\nSalt Lake City, UT, USA, 7482–7491. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b\\n2018.\\u200b00781\\u200b.\\nDe Freitas, J. M., S. Berg, B. C. Geiger, and M. Mucke. 2022. \\n“Compressed Hierarchical Representations for Multi-\\xadTask Learning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content='2018.\\u200b00781\\u200b.\\nDe Freitas, J. M., S. Berg, B. C. Geiger, and M. Mucke. 2022. \\n“Compressed Hierarchical Representations for Multi-\\xadTask Learning \\nand Task Clustering.” In 2022 International Joint Conference on \\nNeural Networks (IJCNN), 1–8. IEEE. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bijcnn\\u200b\\n55064.\\u200b2022.\\u200b9892342.\\nDerczynski, L., M. Guerini, D. Nozza, F. M. del Plaza-\\xadArco, J. \\nSorensen, and M. Zampieri. 2024. “Countering Hateful and Offensive \\nSpeech Online—Open Challenges.” In Proceedings of the 2024 \\nConference on Empirical Methods in Natural Language Processing: \\nTutorial Abstracts, edited by J. Li and F. Liu, 11–16. Association for \\nComputational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2024.\\u200bemnlp\\u200b\\n-\\xad\\u200btutor\\u200bials.\\u200b2.\\nDevlin, J., M.-\\xadW. Chang, K. Lee, and K. Toutanova. 2019. “BERT: \\nPre-\\xadTraining of Deep Bidirectional Transformers for Language \\nUnderstanding.” In Proceedings of the 2019 Conference of the North \\nAmerican Chapter of the Association for Computational Linguistics:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content=\"Pre-\\xadTraining of Deep Bidirectional Transformers for Language \\nUnderstanding.” In Proceedings of the 2019 Conference of the North \\nAmerican Chapter of the Association for Computational Linguistics: \\nHuman Language Technologies, 4171–4186. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200b\\nv1/\\u200bN19-\\xad\\u200b1423.\\nDuan, L., D. Xu, and I. W. Tsang. 2012. “Learning With Augmented \\nFeatures for Heterogeneous Domain Adaptation.” In Proceedings of the \\n29th International Coference on International Conference on Machine \\nLearning ICML'12, 667–674. Omnipress. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b30425\\u200b\\n73.\\u200b3042661.\\nEzugwu, A. E., A. M. Ikotun, O. O. Oyelade, et\\xa0 al. 2022. “A \\nComprehensive Survey of Clustering Algorithms: State-\\xadof-\\xadthe-\\xadArt \\nMachine Learning Applications, Taxonomy, Challenges, and Future \\nResearch Prospects.” Engineering Applications of Artificial Intelligence \\n110: 104743. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200bengap\\u200bpai.\\u200b2022.\\u200b104743.\\nFang, L., G. Liu, and R. Zhang. 2022. “Sense-\\xadaware BERT and Multi-\\xadtask\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 17}, page_content='110: 104743. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200bengap\\u200bpai.\\u200b2022.\\u200b104743.\\nFang, L., G. Liu, and R. Zhang. 2022. “Sense-\\xadaware BERT and Multi-\\xadtask \\nFine-\\xadtuning for Multimodal Sentiment Analysis.” In 2022 International \\nJoint Conference on Neural Networks (JCNN), 1–8. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1109/\\u200bIJCNN\\u200b55064.\\u200b2022.\\u200b9892116.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content='19 of 21\\nExpert Systems, 2026\\nFrenda, S., G. Abercrombie, V. Basile, et\\xa0 al. 2025. “Perspectivist \\nApproaches to Natural Language Processing: A Survey.” Language \\nResources and Evaluation 59: 1719–1746. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200bs1057\\u200b\\n9-\\xad\\u200b024-\\xad\\u200b09766\\u200b-\\xad\\u200b4.\\nFrenda, S., B. Ghanem, M. M. y Gómez, and P. Rosso. 2019. “Online \\nHate Speech Against Women: Automatic Identification of Misogyny \\nand Sexism on Twitter.” Journal of Intelligent & Fuzzy Systems 36: 4743–\\n4752. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b3233/\\u200bJIFS-\\xad\\u200b179023.\\nGuo, M., A. Haque, D.-\\xadA. Huang, S. Yeung, and L. Fei-\\xadFei. 2018. \\n“Dynamic Task Prioritization for Multitask Learning.” In Computer \\nVision—ECCV 2018: 15th European Conference, Munich, Germany, \\nSeptember 8–14, 2018, Proceedings, Part XVI, 282–299. Springer-\\xadVerlag. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01270\\u200b-\\xad\\u200b0_\\u200b17.\\nIndurthi, S., M. A. Zaidi, N. Kumar Lakumarapu, et\\xa0 al. 2021. “Task \\nAware Multi-\\xadTask Learning for Speech to Text Tasks.” In ICASSP 2021–'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content=\"https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01270\\u200b-\\xad\\u200b0_\\u200b17.\\nIndurthi, S., M. A. Zaidi, N. Kumar Lakumarapu, et\\xa0 al. 2021. “Task \\nAware Multi-\\xadTask Learning for Speech to Text Tasks.” In ICASSP 2021–\\n2021 IEEE International Conference on Acoustics, Speech and Signal \\nProcessing (ICASSP), 7723–7727. IEEE. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bICASS\\u200b\\nP39728.\\u200b2021.\\u200b9414703.\\nJamil, S., M. Jalil Piran, and O.-\\xadJ. Kwon. 2023. “A Comprehensive \\nSurvey of Transformers for Computer Vision.” Drones 7: 287. https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b3390/\\u200bdrone\\u200bs7050287.\\nKnight, P., and R. Duan. 2023. “Multi-\\xadTask Learning With Summary \\nStatistics.” In Proceedings of the 37th International Conference on Neural \\nInformation Processing Systems NIPS '23, vol. 36, 54020–54031. Curran \\nAssociates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b36661\\u200b22.\\u200b3668472.\\nKulis, B., K. Saenko, and T. Darrell. 2011. “What You Saw Is Not What \\nYou Get: Domain Adaptation Using Asymmetric Kernel Transforms.”\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content=\"Associates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b36661\\u200b22.\\u200b3668472.\\nKulis, B., K. Saenko, and T. Darrell. 2011. “What You Saw Is Not What \\nYou Get: Domain Adaptation Using Asymmetric Kernel Transforms.” \\nIn Proceedings of the 2011 IEEE Conference on Computer Vision and \\nPattern Recognition CVPR '11, 1785–1792. IEEE Computer Society. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b2011.\\u200b5995702.\\nLauriola, I., A. Lavelli, and F. Aiolli. 2022. “An Introduction to Deep \\nLearning in Natural Language Processing: Models, Techniques, and \\nTools.” Neurocomputing 470: 443–456. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200bneu-\\ncom.\\u200b2021.\\u200b05.\\u200b103.\\nLi, D., H. Nguyen, and H. R. Zhang. 2023. “Identification of Negative \\nTransfers in Multitask Learning Using Surrogate Models.” Transactions \\non Machine Learning Research. https://\\u200bopenr\\u200beview.\\u200bnet/\\u200bforum?\\u200bid=\\u200b\\nKgfFA\\u200bI9f3E\\u200b.\\nLiu, Y., Y. Lu, H. Liu, et\\xa0al. 2023. “Hierarchical Prompt Learning for \\nMulti-\\xadTask Learning.” In 2023 IEEE/CVF Conference on Computer\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content=\"KgfFA\\u200bI9f3E\\u200b.\\nLiu, Y., Y. Lu, H. Liu, et\\xa0al. 2023. “Hierarchical Prompt Learning for \\nMulti-\\xadTask Learning.” In 2023 IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR), 10888–10898. https://\\u200bdoi.\\u200borg/\\u200b\\n10.\\u200b1109/\\u200bCVPR5\\u200b2729.\\u200b2023.\\u200b01048\\u200b.\\nLong, M., Z. Cao, J. Wang, and P. S. Yu. 2017. “Learning Multiple Tasks \\nwith Multilinear Relationship Networks.” In Proceedings of the 31st \\nInternational Conference on Neural Information Processing Systems \\nNIPS'17, 1593–1602. Curran Associates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b\\n32947\\u200b71.\\u200b3294923.\\nLoshchilov, I., and F. Hutter. 2019. “Decoupled Weight Decay \\nRegularization.” In 7th International Conference on Learning \\nRepresentations, ICLR, New Orleans, LA, USA. https://\\u200bopenr\\u200beview.\\u200bnet/\\u200b\\npdf?\\u200bid=\\u200bBkg6R\\u200biCqY7\\u200b.\\nLu, Y., A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. Feris. 2017. “Fully-\\xad\\nAdaptive Feature Sharing in Multi-\\xadTask Networks With Applications in \\nPerson Attribute Classification.” In 2017 IEEE Conference on Computer\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content='Adaptive Feature Sharing in Multi-\\xadTask Networks With Applications in \\nPerson Attribute Classification.” In 2017 IEEE Conference on Computer \\nVision and Pattern Recognition (CVPR), 1131–1140. IEEE. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b2017.\\u200b126.\\nMagnossão de Paula, A. F., R. F. da Silva, and I. B. Schlicht. 2021. \\n“Sexism Prediction in Spanish and English Tweets Using Monolingual \\nand Multilingual BERT and Ensemble Models.” In Proceedings of the \\nIberian Languages Evaluation Forum (IberLEF 2021) Co-\\xadLocated With \\nthe XXXVII International Conference of the Spanish Society for Natural \\nLanguage Processing (SEPLN 2021), 356–373. CEUR. https://\\u200bceur-\\xad\\u200bws.\\u200b\\norg/\\u200bVol-\\xad\\u200b2943/\\u200bexist_\\u200bpaper2.\\u200bpdf.\\nMagnossão de Paula, A. F., P. Rosso, and D. Spina. 2023. “Mitigating \\nNegative Transfer With Task Awareness for Sexism, Hate Speech, and \\nToxic Language Detection.” In 2023 International Joint Conference on \\nNeural Networks (IJCNN), 1–8. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bIJCNN\\u200b54540.\\u200b\\n2023.\\u200b10191347.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content='Toxic Language Detection.” In 2023 International Joint Conference on \\nNeural Networks (IJCNN), 1–8. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bIJCNN\\u200b54540.\\u200b\\n2023.\\u200b10191347.\\nMagnossão de Paula, A. F., and I. B. Schlicht. 2021. “AI-\\xadUPV at \\nIberLEF-\\xad2021 DETOXIS Task: Toxicity Detection in Immigration-\\xad\\nRelated Web News Comments Using Transformers and Statistical \\nModels.” In Proceedings of the Iberian Languages Evaluation Forum \\n(IberLEF 2021) Co-\\xadLocated With the XXXVII International Conference \\nof the Spanish Society for Natural Language Processing (SEPLN 2021), \\n547–566. CEUR. https://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b2943/\\u200bdetox\\u200bis_\\u200bpaper2.\\u200bpdf.\\nMahdaouy, A. E., A. E. Mekki, A. Oumar, H. Mousannif, and I. \\nBerrada. 2021. “Deep Multi-\\xadTask Models for Misogyny Identification \\nand Categorization on Arabic Social Media.” In Working Notes of FIRE \\n2021 -\\xad Forum for Information Retrieval Evaluation, Gandhinagar, India, \\n852–860. CEUR-\\xadWS.org Volume 3159 of CEUR Workshop Proceedings.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content='2021 -\\xad Forum for Information Retrieval Evaluation, Gandhinagar, India, \\n852–860. CEUR-\\xadWS.org Volume 3159 of CEUR Workshop Proceedings. \\nhttps://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b3159/\\u200bT5-\\xad\\u200b5.\\u200bpdf.\\nMikolov, T., K. Chen, G. Corrado, and J. Dean. 2013. “Efficient \\nEstimation of Word Representations in Vector Space.” In 1st \\nInternational \\nConference \\non \\nLearning \\nRepresentations, \\nICLR \\n2013, Scottsdale, Arizona, USA, May 2–4, 2013, Workshop Track \\nProceedings. \\nhttps://\\u200bpeople.\\u200bfjfi.\\u200bcvut.\\u200bcz/\\u200bvybir\\u200bja2/\\u200bSemin\\u200bar/\\u200bword2\\u200b\\nvec_\\u200b1301.\\u200b3781.\\u200bpdf.\\nMiljković, D. 2017. “Brief Review of Self-\\xadOrganizing Maps.” In 2017 \\n40th International Convention on Information and Communication \\nTechnology, Electronics and Microelectronics (MIPRO), 1061–1066. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b23919/\\u200bMIPRO.\\u200b2017.\\u200b7973581.\\nMostafa, A., O. Mohamed, and A. Ashraf. 2022. “GOF at Arabic \\nHate Speech 2022: Breaking the Loss Function Convention for Data-\\xad\\nImbalanced Arabic Offensive Text Detection.” In Proceedings of the 5th'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content=\"Hate Speech 2022: Breaking the Loss Function Convention for Data-\\xad\\nImbalanced Arabic Offensive Text Detection.” In Proceedings of the 5th \\nWorkshop on Open-\\xadSource Arabic Corpora and Processing Tools With \\nShared Tasks on Qur'an QA and Fine-\\xadGrained Hate Speech Detection, \\n167–175. European Language Resources Association. https://\\u200baclan\\u200btholo\\u200b\\ngy.\\u200borg/\\u200b2022.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b21.\\nMubarak, H., H. Al-\\xadKhalifa, and A. Al-\\xadThubaity. 2022. “Overview of \\nOSACT5 Shared Task on Arabic Offensive Language and Hate Speech \\nDetection.” In Proceedings of the 5th Workshop on Open-\\xadSource Arabic \\nCorpora and Processing Tools With Shared Tasks on Qur'an QA and \\nFine-\\xadGrained Hate Speech Detection (Pp. 162–166). European Language \\nResources Association. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b2022.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b20.\\nMubarak, H., K. Darwish, W. Magdy, T. Elsayed, and H. Al-\\xadKhalifa. \\n2020. “Overview of OSACT4 Arabic Offensive Language Detection \\nShared Task.” In Proceedings of the 4th Workshop on Open-\\xadSource\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content='Mubarak, H., K. Darwish, W. Magdy, T. Elsayed, and H. Al-\\xadKhalifa. \\n2020. “Overview of OSACT4 Arabic Offensive Language Detection \\nShared Task.” In Proceedings of the 4th Workshop on Open-\\xadSource \\nArabic Corpora and Processing Tools, With a Shared Task on Offensive \\nLanguage Detection, edited by H. Al-\\xadKhalifa, W. Magdy, K. Darwish, \\nT. Elsayed, and H. Mubarak, 48–52. European Language Resource \\nAssociation. https://\\u200baclan\\u200btholo\\u200bgy.\\u200borg/\\u200b2020.\\u200bosact\\u200b-\\xad\\u200b1.\\u200b7.\\nMulki, H., and B. Ghanem. 2021. “ArMI at FIRE 2021: Overview of \\nthe First Shared Task on Arabic Misogyny Identification.” In Working \\nNotes of FIRE 2021 -\\xad Forum for Information Retrieval Evaluation, \\nGandhinagar, India, December 13–17, 2021, 820–830. CEUR-WS.org \\nvolume 3159 of CEUR Workshop Proceedings. https://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b\\n3159/\\u200bT5-\\xad\\u200b1.\\u200bpdf.\\nOtter, D. W., J. R. Medina, and J. K. Kalita. 2020. “A Survey of the Usages \\nof Deep Learning for Natural Language Processing.” IEEE Transactions'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 18}, page_content='3159/\\u200bT5-\\xad\\u200b1.\\u200bpdf.\\nOtter, D. W., J. R. Medina, and J. K. Kalita. 2020. “A Survey of the Usages \\nof Deep Learning for Natural Language Processing.” IEEE Transactions \\non Neural Networks and Learning Systems 32: 604–624. https://\\u200bdoi.\\u200borg/\\u200b\\n10.\\u200b1109/\\u200bTNNLS.\\u200b2020.\\u200b2979670.\\nPachinger, P., A. Hanbury, J. Neidhardt, and A. Planitzer. 2023. \\n“Toward Disambiguating the Definitions of Abusive, Offensive, Toxic, \\nand Uncivil Comments.” In Proceedings of the First Workshop on Cross-\\xad\\nCultural Considerations in NLP (C3NLP), 107–113. Association for \\nComputational \\nLinguistics. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2023.\\u200bc3nlp\\u200b\\n-\\xad\\u200b1.\\u200b11.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content='20 of 21\\nExpert Systems, 2026\\nPan, S. J., and Q. Yang. 2009. “A Survey on Transfer Learning.” IEEE \\nTransactions on Knowledge and Data Engineering 22: 1345–1359. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bTKDE.\\u200b2009.\\u200b191.\\nPérez, J. M., and F. M. Luque. 2019. “Atalaya at SemEval 2019 Task \\n5: Robust Embeddings for Tweet Classification.” In In Proceedings \\nof the 13th International Workshop on Semantic Evaluation, 64–69. \\nAssociation for Computational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200b\\nv1/\\u200bS19-\\xad\\u200b2008.\\nPlaza, L., J. de Carrillo-\\xadAlbornoz, R. Morante, et\\xa0al. 2023. “Overview \\nof EXIST 2023—Learning With Disagreement for Sexism Identification \\nand Characterization.” In Experimental IR Meets Multilinguality, \\nMultimodality, and Interaction, edited by A. Arampatzis, E. Kanoulas, \\nT. Tsikrika, et\\xa0 al., 316–342. Springer Nature Switzerland. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b031-\\xad\\u200b42448\\u200b-\\xad\\u200b9_\\u200b23.\\nPlaza, L., J. de Carrillo-\\xadAlbornoz, R. Morante, et\\xa0al. 2023. “Overview of'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content=\"T. Tsikrika, et\\xa0 al., 316–342. Springer Nature Switzerland. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b031-\\xad\\u200b42448\\u200b-\\xad\\u200b9_\\u200b23.\\nPlaza, L., J. de Carrillo-\\xadAlbornoz, R. Morante, et\\xa0al. 2023. “Overview of \\nEXIST 2023: sEXism Identification in Social neTworks.” In Proceedings \\nof ECIR'23, 593–599. Springer Nature Switzerland. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b031-\\xad\\u200b28241\\u200b-\\xad\\u200b6_\\u200b68.\\nPlaza, L., J. de Carrillo-\\xadAlbornoz, V. Ruiz, et\\xa0 al. 2024. “Overview of \\nEXIST 2024—Learning With Disagreement for Sexism Identification \\nand Characterization in Tweets and Memes.” In Experimental IR Meets \\nMultilinguality, Multimodality, and Interaction, 93–117. Springer Nature \\nSwitzerland. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b031-\\xad\\u200b71908\\u200b-\\xad\\u200b0_\\u200b5.\\nPlaza-\\xaddel-\\xadArco, F. M., M. D. Molina-\\xadGonzález, and L. Alfonso. 2021. \\n“SINAI at IberLEF-\\xad2021 DETOXIS Task: Exploring Features as Tasks \\nin a Multi-\\xadTask Learning Approach to Detecting Toxic Comments.” In\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content='“SINAI at IberLEF-\\xad2021 DETOXIS Task: Exploring Features as Tasks \\nin a Multi-\\xadTask Learning Approach to Detecting Toxic Comments.” In \\nProceedings of the Iberian Languages Evaluation Forum (IberLEF 2021) \\nCo-\\xadLocated With the XXXVII International Conference of the Spanish \\nSociety for Natural Language Processing (SEPLN 2021), 580–590. \\nMálaga, Spain. https://\\u200bceur-\\xad\\u200bws.\\u200borg/\\u200bVol-\\xad\\u200b2943/\\u200bdetox\\u200bis_\\u200bpaper5.\\u200bpdf.\\nPlaza-\\xaddel-\\xadArco, F. M., M. D. Molina-\\xadGonzález, L. A. Ureña-\\xadLópez, and \\nM. T. Martín-\\xadValdivia. 2021a. “A Multi-\\xadTask Learning Approach to \\nHate Speech Detection Leveraging Sentiment Analysis.” IEEE Access 9: \\n112478–112489. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bACCESS.\\u200b2021.\\u200b3103697.\\nPlaza-\\xaddel-\\xadArco, F. M., M. D. Molina-\\xadGonzález, L. A. Ureña-\\xadLópez, \\nand M. T. Martín-\\xadValdivia. 2021b. “Comparing Pre-\\xadTrained Language \\nModels for Spanish Hate Speech Detection.” Expert Systems with \\nApplications 166: 114120. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200beswa.\\u200b2020.\\u200b114120.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content='Models for Spanish Hate Speech Detection.” Expert Systems with \\nApplications 166: 114120. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200beswa.\\u200b2020.\\u200b114120.\\nPoletto, F., V. Basile, M. Sanguinetti, C. Bosco, and V. Patti. 2021. \\n“Resources and Benchmark Corpora for Hate Speech Detection: A \\nSystematic Review.” Language Resources and Evaluation 55: 477–523. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200bs1057\\u200b9-\\xad\\u200b020-\\xad\\u200b09502\\u200b-\\xad\\u200b8.\\nPrettenhofer, P., and B. Stein. 2010. “Cross-\\xadLanguage Text Classification \\nUsing Structural Correspondence Learning.” In Proceedings of the 48th \\nAnnual Meeting of the Association for Computational Linguistics, 1118–\\n1127. Association for Computational Linguistics. https://\\u200baclan\\u200btholo\\u200bgy.\\u200b\\norg/\\u200bP10-\\xad\\u200b1114.\\nRodríguez-\\xadSánchez, F., J. de Carrillo-\\xadAlbornoz, L. Plaza, et\\xa0 al. 2021. \\n“Overview of EXIST 2021: sEXism Identification in Social neTworks.” \\nProcesamiento del Lenguaje Natural 67: 195–207. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n26342/\\u200b2021-\\xad\\u200b67-\\xad\\u200b17.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content=\"“Overview of EXIST 2021: sEXism Identification in Social neTworks.” \\nProcesamiento del Lenguaje Natural 67: 195–207. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n26342/\\u200b2021-\\xad\\u200b67-\\xad\\u200b17.\\nRuder, S. 2017. “An Overview of Multi-\\xadTask Learning in Deep Neural \\nNetworks.” CoRR, abs/1706.05098. http://\\u200barxiv.\\u200borg/\\u200babs/\\u200b1706.\\u200b05098\\u200b.\\nRuder, S., M. E. Peters, S. Swayamdipta, and T. Wolf. 2019. “Transfer \\nLearning in Natural Language Processing.” In Proceedings of the \\n2019 Conference of the North American Chapter of the Association \\nfor Computational Linguistics: Tutorials, 15–18. Association for \\nComputational Linguistics. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b18653/\\u200bv1/\\u200bN19-\\xad\\u200b5004.\\nSener, O., and V. Koltun. 2018. “Multi-\\xadTask Learning as Multi-\\xadObjective \\nOptimization.” In Proceedings of the 32nd International Conference \\non Neural Information Processing Systems NIPS'18, 525–536. Curran \\nAssociates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b33269\\u200b43.\\u200b3326992.\\nSinha, A. T., A. Rabinovich, Z. Chen, and V. Badrinarayanan. 2021.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content=\"on Neural Information Processing Systems NIPS'18, 525–536. Curran \\nAssociates Inc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b33269\\u200b43.\\u200b3326992.\\nSinha, A. T., A. Rabinovich, Z. Chen, and V. Badrinarayanan. 2021. \\n“Gradient Adversarial Training of Neural Networks.” US Patent App. \\n17/051, 982.\\nTaulé, M., A. Ariza, M. Nofre, E. Amigó, and P. Rosso. 2021. “Overview \\nof DETOXIS at IberLEF 2021: DEtection of TOxicity in Comments in \\nSpanish.” Procesamiento del Lenguaje Natural 67: 209–221. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b26342/\\u200b2021-\\xad\\u200b67-\\xad\\u200b18.\\nUma, A., T. Fornaciari, A. Dumitrache, et\\xa0 al. 2021. “SemEval-\\xad2021 \\nTask 12: Learning With Disagreements.” In Proceedings of the 15th \\nInternational Workshop on Semantic Evaluation (SemEval-\\xad2021), edited \\nby A. Palmer, N. Schneider, N. Schluter, G. Emerson, A. Herbelot, and \\nX. Zhu, 338–347. Association for Computational Linguistics. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2021.\\u200bsemev\\u200bal-\\xad\\u200b1.\\u200b41.\\nVandenhende, S., S. Georgoulis, L. V. Gool, and B. D. Brabandere. 2020.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content=\"X. Zhu, 338–347. Association for Computational Linguistics. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b18653/\\u200bv1/\\u200b2021.\\u200bsemev\\u200bal-\\xad\\u200b1.\\u200b41.\\nVandenhende, S., S. Georgoulis, L. V. Gool, and B. D. Brabandere. 2020. \\n“Branched Multi-\\xadTask Networks: Deciding What Layers to Share.” In \\nProceedings of the 31st British Machine Vision Conference BMVC '20. BMVA \\nPress. https://\\u200bwww.\\u200bbmvc2\\u200b020-\\xad\\u200bconfe\\u200brence.\\u200bcom/\\u200bassets/\\u200bpapers/\\u200b0213.\\u200bpdf.\\nVandenhende, S., S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. \\nDai, and L. Van Gool. 2022. “Multi-\\xadTask Learning for Dense Prediction \\nTasks: A Survey.” IEEE Transactions on Pattern Analysis and Machine \\nIntelligence \\n44: \\n3614–3633. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bTPAMI.\\u200b2021.\\u200b\\n3054719.\\nVaswani, A., N. Shazeer, N. Parmar, et\\xa0al. 2017. “Attention Is All You \\nNeed.” In Proceedings of the 31st International Conference on Neural \\nInformation Processing Systems NIPS'17, 6000–6010. Curran Associates \\nInc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b32952\\u200b22.\\u200b3295349.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content=\"Need.” In Proceedings of the 31st International Conference on Neural \\nInformation Processing Systems NIPS'17, 6000–6010. Curran Associates \\nInc. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b32952\\u200b22.\\u200b3295349.\\nVoulodimos, A., N. Doulamis, A. Doulamis, E. Protopapadakis, and D. \\nAndina. 2018. “Deep Learning for Computer Vision: A Brief Review.” \\nComputational Intelligence and Neuroscience 2018: 1–13. https://\\u200bdoi.\\u200borg/\\u200b\\n10.\\u200b1155/\\u200b2018/\\u200b7068349.\\nWang, C., and S. Mahadevan. 2011. “Heterogeneous Domain Adaptation \\nUsing Manifold Alignment.” In Proceedings of the Twenty-\\xadSecond \\nInternational Joint Conference on Artificial Intelligence, 1541–1546. \\nAAAI Press. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b5555/\\u200b22835\\u200b16.\\u200b2283652.\\nWang, Y., M. Xu, Y. Yan, T. Zhao, Y. Chen, and J. Yang. 2022. “Exploring \\nTopic Supervision With BERT for Text Matching.” In 2022 International \\nJoint Conference on Neural Networks (IJCNN), 1–7. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1109/\\u200bIJCNN\\u200b55064.\\u200b2022.\\u200b9892023.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content='Topic Supervision With BERT for Text Matching.” In 2022 International \\nJoint Conference on Neural Networks (IJCNN), 1–7. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1109/\\u200bIJCNN\\u200b55064.\\u200b2022.\\u200b9892023.\\nWeiss, K., T. M. Khoshgoftaar, and D. Wang. 2016. “A Survey of Transfer \\nLearning.” Journal of Big Data 3: 1–40. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1186/\\u200bs4053\\u200b\\n7-\\xad\\u200b016-\\xad\\u200b0043-\\xad\\u200b6.\\nWorsham, J., and J. Kalita. 2020. “Multi-\\xadTask Learning for Natural Language \\nProcessing in the 2020s: Where Are We Going?” Pattern Recognition Letters \\n136: 120–126. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1016/j.\\u200bpatrec.\\u200b2020.\\u200b05.\\u200b031.\\nWu, S., H. Fei, and D. Ji. 2020. “Aggressive Language Detection With \\nJoint Text Normalization via Adversarial Multi-\\xadTask Learning.” \\nIn Natural Language Processing and Chinese Computing: 9th CCF \\nInternational Conference, NLPCC 2020, Zhengzhou, China, 683–696. \\nSpringer-\\xadVerlag. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b60450\\u200b-\\xad\\u200b9_\\u200b54.\\nWu, S., H. R. Zhang, and C. Ré. 2020. “Understanding and Improving'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content='Springer-\\xadVerlag. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b60450\\u200b-\\xad\\u200b9_\\u200b54.\\nWu, S., H. R. Zhang, and C. Ré. 2020. “Understanding and Improving \\nInformation Transfer in Multi-\\xadTask Learning.” In 8th International \\nConference on Learning Representations, ICLR 2020, Addis Ababa, \\nEthiopia. \\nOpenReview.net. \\nhttps://\\u200bopenr\\u200beview.\\u200bnet/\\u200bforum?\\u200bid=\\u200bSylzh\\u200b\\nkBtDB\\u200b.\\nXu, D., W. Ouyang, X. Wang, and N. Sebe. 2018. “Pad-\\xadnet: Multi-\\xadAsks \\nGuided Prediction and Distillation Network for Simultaneous Depth \\nEstimation and Scene Parsing.” In 2018 IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, 675–684. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n1109/\\u200bCVPR.\\u200b2018.\\u200b00077\\u200b.\\nZhang, Y., and Q. Yang. 2022. “A Survey on Multi-\\xadTask Learning.” \\nIEEE Transactions on Knowledge and Data Engineering 34: 5586–5609. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bTKDE.\\u200b2021.\\u200b3070203.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 19}, page_content='Zhang, Y., and Q. Yang. 2022. “A Survey on Multi-\\xadTask Learning.” \\nIEEE Transactions on Knowledge and Data Engineering 34: 5586–5609. \\nhttps://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bTKDE.\\u200b2021.\\u200b3070203.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 20}, page_content='21 of 21\\nExpert Systems, 2026\\nZhang, Z., Z. Cui, C. Xu, Z. Jie, X. Li, and J. Yang. 2018. “Joint Task-\\xad\\nRecursive Learning for Semantic Segmentation and Depth Estimation.” \\nIn Computer Vision—ECCV 2018: 15th European Conference, Munich, \\nGermany, September 8–14, 2018, Proceedings, Part X, 238–255. Springer-\\xad\\nVerlag. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01249\\u200b-\\xad\\u200b6_\\u200b15.\\nZhang, Z., Z. Cui, C. Xu, Y. Yan, N. Sebe, and J. Yang. 2019. “Pattern-\\xad\\nAffinitive Propagation Across Depth, Surface Normal and Semantic \\nSegmentation.” In 2019 IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, 4106–4115. IEEE. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1109/\\u200bCVPR.\\u200b\\n2019.\\u200b00423\\u200b.\\nZhao, X., H. Li, X. Shen, X. Liang, and Y. Wu. 2018. “A Modulation \\nModule for Multi-\\xadTask Learning With Applications in Image Retrieval.” \\nIn Proceedings of the European Conference on Computer Vision (ECCV), \\n415–432. Springer International Publishing. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b\\n978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01246\\u200b-\\xad\\u200b5_\\u200b25.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2026-01-03T10:28:47+05:30', 'source': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'file_path': '..\\\\docs\\\\pdfs\\\\Expert Systems - 2026 - Paula - Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': 'Mitigating the Negative Transfer in Multi‐Task Learning for Harmful Language Detection in Spanish and Arabic', 'author': '', 'subject': 'Expert Systems 2026.43:e70182', 'keywords': '', 'moddate': '2026-01-07T23:34:44-08:00', 'trapped': '', 'modDate': \"D:20260107233444-08'00'\", 'creationDate': \"D:20260103102847+05'30'\", 'page': 20}, page_content='In Proceedings of the European Conference on Computer Vision (ECCV), \\n415–432. Springer International Publishing. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b1007/\\u200b\\n978-\\xad\\u200b3-\\xad\\u200b030-\\xad\\u200b01246\\u200b-\\xad\\u200b5_\\u200b25.\\nZhu, Z., K. Lin, A. K. Jain, and J. Zhou. 2023. “Transfer Learning in \\nDeep Reinforcement Learning: A Survey.” IEEE Transactions on \\nPattern Analysis and Machine Intelligence 45: 13344–13362. https://\\u200bdoi.\\u200b\\norg/\\u200b10.\\u200b1109/\\u200bTPAMI.\\u200b2023.\\u200b3292075.\\n 14680394, 2026, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/exsy.70182 by Indian Institute Of Info Tech, Wiley Online Library on [07/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8f5892f",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Tuple,Dict,Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e0cc8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model:all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1042.51it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model successfully,embedding dimensions:384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.embeddingManager at 0x22777202f10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing HuggingFace model(miniLM L6 v2)\n",
    "class embeddingManager:\n",
    "    def __init__(self,model_name: str=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self.load_model()\n",
    "    def load_model(self):\n",
    "        #loading the sentence-transformer model\n",
    "        try:\n",
    "            print(f\"Loading embedding model:{self.model_name}\")\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"Loaded model successfully,embedding dimensions:{self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Loading the model:{e}\")\n",
    "            raise\n",
    "    def generate_embeddings(self,texts:List[str])->np.ndarray:\n",
    "        #generate embeddings for list of models\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not found\")\n",
    "        embeddings=self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated Embeddings with shape {embeddings.shape}\")\n",
    "        return embeddings\n",
    "embedding_manager=embeddingManager()\n",
    "embedding_manager\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02625c1b",
   "metadata": {},
   "source": [
    "Vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2973cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection:pdf_docs\n",
      "Existing docs in collection:1476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.vectorstore at 0x227760c2610>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class vectorstore:\n",
    "    def __init__(self,collection_name: str=\"pdf_docs\",persist_directory=\"../docs/vector_store\"):\n",
    "    #initilization of the vector store\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self.initialize_store()\n",
    "    def initialize_store(self):\n",
    "        #Initilize chroma db\n",
    "        try:\n",
    "            #Creating Persistent Chroma DB\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "            #Get or create collection\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF's documentation embedding for RAGs\"}\n",
    "\n",
    "            )\n",
    "            print(f\"Collection:{self.collection_name}\")\n",
    "            print(f\"Existing docs in collection:{self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "            raise \n",
    "    def add_documents(self,documents:List[Any],embeddings=np.ndarray):\n",
    "        \"Add documents and their embeddings to the vector store\"\n",
    "        if len(documents)!=len(embeddings):\n",
    "            raise ValueError(\"No of documents should match with no of embeddings\")\n",
    "        print(f\"adding {len(documents)} documents to the store\")\n",
    "        #prepare data for chroma db\n",
    "        ids =[]\n",
    "        metadatas=[]\n",
    "        document_texts=[]\n",
    "        embedding_list=[]\n",
    "        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            #generating doc id using uuid(unique)\n",
    "            doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            #meta data\n",
    "            metadata=dict(doc.metadata)\n",
    "            metadata['doc_index']=i\n",
    "            metadata[\"content_length\"]=len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            #document_text\n",
    "            document_texts.append(doc.page_content)\n",
    "            #embedding lists\n",
    "            embedding_list.append(embedding.tolist())\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embedding_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=document_texts\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} in the vector store\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents in the vector store:{e}\")\n",
    "            raise\n",
    "\n",
    "Vectorstore=vectorstore()\n",
    "Vectorstore\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "716391a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:09<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings with shape (492, 384)\n",
      "adding 492 documents to the store\n",
      "Successfully added 492 in the vector store\n"
     ]
    }
   ],
   "source": [
    "#converting texts into chunks\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "#converting chunks into embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "#storing into vector db\n",
    "Vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0cf7a",
   "metadata": {},
   "source": [
    "Retriver pipeline from the vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "575d7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class retriever:\n",
    "    def __init__(self,vector_store:vectorstore,embedding_manager:embeddingManager):\n",
    "        self.vector_store =vector_store\n",
    "        self.embedding_manager=embedding_manager\n",
    "    def retrieve(self,query:str,top_k:int=5,score_threshold : float=0,)-> List[dict[str,any]]:\n",
    "        #retrieve relevant document\n",
    "        print(f\"Retrieving documents for query:{query}\")\n",
    "        print(f\"Top k:{top_k},score_threshold:{score_threshold}\")\n",
    "        #generate query embedding\n",
    "        query_embedding=self.embedding_manager.generate_embeddings([query])[0]\n",
    "        #Search in vector store\n",
    "        try:\n",
    "            results=self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            # Process results\n",
    "            retrived_docs=[]\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents=results['documents'][0]\n",
    "                metadatas=results['metadatas'][0]\n",
    "                distances=results['distances'][0]\n",
    "                ids=results['ids'][0]\n",
    "                for i,(document,metadata,distance,doc_id) in enumerate(zip(documents,metadatas,distances,ids)):\n",
    "                    similarity_score=1/(1+distance)\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrived_docs.append({\n",
    "                            'id':doc_id,\n",
    "                            'content':document,\n",
    "                            'metadata':metadata,\n",
    "                            'similarity_score':similarity_score,\n",
    "                            'distance':distance,\n",
    "                            'rank':i+1\n",
    "                        })\n",
    "                print(f\"retreived {len(retrived_docs)} documents after filtering\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrived_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval {e}\")\n",
    "            return []\n",
    "rag_retriever=retriever(Vectorstore,embedding_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48c6dd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query:Explain about the docs\n",
      "Top k:5,score_threshold:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings with shape (1, 384)\n",
      "retreived 5 documents after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = rag_retriever.retrieve(\"Explain about the docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17b954e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(docs, start=1):\n",
    "    print(f\"\\n========== Document {i} ==========\")\n",
    "    print(d[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86313ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
